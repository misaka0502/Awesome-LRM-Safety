[
  {
    "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2509.09674v1",
    "arxiv_id": "2509.09674v1",
    "authors": [
      "Haozhan Li",
      "Yuxin Zuo",
      "Jiale Yu",
      "Yuhao Zhang",
      "Zhaohui Yang",
      "Kaiyan Zhang",
      "Xuekai Zhu",
      "Yuchen Zhang",
      "Tianxing Chen",
      "Ganqu Cui",
      "Dehui Wang",
      "Dingxiang Luo",
      "Yuchen Fan",
      "Youbang Sun",
      "Jia Zeng",
      "Jiangmiao Pang",
      "Shanghang Zhang",
      "Yu Wang",
      "Yao Mu",
      "Bowen Zhou",
      "Ning Ding"
    ],
    "published": "2025-09-11T17:59:17+00:00",
    "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$ on RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL"
  },
  {
    "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models",
    "url": "http://arxiv.org/abs/2509.09675v1",
    "arxiv_id": "2509.09675v1",
    "authors": [
      "Runpeng Dai",
      "Linfeng Song",
      "Haolin Liu",
      "Zhenwen Liang",
      "Dian Yu",
      "Haitao Mi",
      "Zhaopeng Tu",
      "Rui Liu",
      "Tong Zheng",
      "Hongtu Zhu",
      "Dong Yu"
    ],
    "published": "2025-09-11T17:59:17+00:00",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes."
  },
  {
    "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration",
    "url": "http://arxiv.org/abs/2509.09671v1",
    "arxiv_id": "2509.09671v1",
    "authors": [
      "Sirui Xu",
      "Yu-Wei Chao",
      "Liuyu Bian",
      "Arsalan Mousavian",
      "Yu-Xiong Wang",
      "Liang-Yan Gui",
      "Wei Yang"
    ],
    "published": "2025-09-11T17:59:07+00:00",
    "summary": "Hand-object motion-capture (MoCap) repositories offer large-scale, contact-rich demonstrations and hold promise for scaling dexterous robotic manipulation. Yet demonstration inaccuracies and embodiment gaps between human and robot hands limit the straightforward use of these data. Existing methods adopt a three-stage workflow, including retargeting, tracking, and residual correction, which often leaves demonstrations underused and compound errors across stages. We introduce Dexplore, a unified single-loop optimization that jointly performs retargeting and tracking to learn robot control policies directly from MoCap at scale. Rather than treating demonstrations as ground truth, we use them as soft guidance. From raw trajectories, we derive adaptive spatial scopes, and train with reinforcement learning to keep the policy in-scope while minimizing control effort and accomplishing the task. This unified formulation preserves demonstration intent, enables robot-specific strategies to emerge, improves robustness to noise, and scales to large demonstration corpora. We distill the scaled tracking policy into a vision-based, skill-conditioned generative controller that encodes diverse manipulation skills in a rich latent representation, supporting generalization across objects and real-world deployment. Taken together, these contributions position Dexplore as a principled bridge that transforms imperfect demonstrations into effective training signals for dexterous manipulation."
  },
  {
    "title": "Can Understanding and Generation Truly Benefit Together -- or Just Coexist?",
    "url": "http://arxiv.org/abs/2509.09666v1",
    "arxiv_id": "2509.09666v1",
    "authors": [
      "Zhiyuan Yan",
      "Kaiqing Lin",
      "Zongjian Li",
      "Junyan Ye",
      "Hui Han",
      "Zhendong Wang",
      "Hao Liu",
      "Bin Lin",
      "Hao Li",
      "Xue Xu",
      "Xinyan Xiao",
      "Jingdong Wang",
      "Haifeng Wang",
      "Li Yuan"
    ],
    "published": "2025-09-11T17:57:59+00:00",
    "summary": "In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder's reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising \"aha moment\" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity."
  },
  {
    "title": "Bogoliubov quasi-particles in superconductors are integer-charged particles inapplicable for braiding quantum information",
    "url": "http://arxiv.org/abs/2509.09663v1",
    "arxiv_id": "2509.09663v1",
    "authors": [
      "Zhiyu Fan",
      "Wei Ku"
    ],
    "published": "2025-09-11T17:56:28+00:00",
    "summary": "We present a rigorous proof that under a number-conserving Hamiltonian, one-body quasi-particles generally possess quantized charge and inertial mass identical to the bare particles. It follows that, Bogoliubov zero modes in the vortex (or on the edge) of superconductors $\\textit{cannot}$ be their own anti-particles capable of braiding quantum information. As such, the heavily pursued Majorana zero mode-based route for quantum computation requires a serious re-consideration. This study further reveals the conceptual challenge in preparing and manipulating braid-able quantum states via physical thermalization or slow external fields. These profound results should reignite the long-standing quest for a number-conserving theory of superconductivity and superfluidity without fictitiously breaking global U(1) symmetry."
  },
  {
    "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
    "url": "http://arxiv.org/abs/2509.08827v1",
    "arxiv_id": "2509.08827v1",
    "authors": [
      "Kaiyan Zhang",
      "Yuxin Zuo",
      "Bingxiang He",
      "Youbang Sun",
      "Runze Liu",
      "Che Jiang",
      "Yuchen Fan",
      "Kai Tian",
      "Guoli Jia",
      "Pengfei Li",
      "Yu Fu",
      "Xingtai Lv",
      "Yuchen Zhang",
      "Sihang Zeng",
      "Shang Qu",
      "Haozhan Li",
      "Shijie Wang",
      "Yuru Wang",
      "Xinwei Long",
      "Fangfu Liu",
      "Xiang Xu",
      "Jiaze Ma",
      "Xuekai Zhu",
      "Ermo Hua",
      "Yihao Liu",
      "Zonglin Li",
      "Huayu Chen",
      "Xiaoye Qu",
      "Yafu Li",
      "Weize Chen",
      "Zhenzhao Yuan",
      "Junqi Gao",
      "Dong Li",
      "Zhiyuan Ma",
      "Ganqu Cui",
      "Zhiyuan Liu",
      "Biqing Qi",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "published": "2025-09-10T17:59:43+00:00",
    "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs"
  },
  {
    "title": "RewardDance: Reward Scaling in Visual Generation",
    "url": "http://arxiv.org/abs/2509.08826v1",
    "arxiv_id": "2509.08826v1",
    "authors": [
      "Jie Wu",
      "Yu Gao",
      "Zilyu Ye",
      "Ming Li",
      "Liang Li",
      "Hanzhong Guo",
      "Jie Liu",
      "Zeyue Xue",
      "Xiaoxia Hou",
      "Wei Liu",
      "Yan Zeng",
      "Weilin Huang"
    ],
    "published": "2025-09-10T17:59:31+00:00",
    "summary": "Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models."
  },
  {
    "title": "RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation",
    "url": "http://arxiv.org/abs/2509.08820v1",
    "arxiv_id": "2509.08820v1",
    "authors": [
      "Zongzheng Zhang",
      "Chenghao Yue",
      "Haobo Xu",
      "Minwen Liao",
      "Xianglin Qi",
      "Huan-ang Gao",
      "Ziwei Wang",
      "Hao Zhao"
    ],
    "published": "2025-09-10T17:52:09+00:00",
    "summary": "Robotic chemists promise to both liberate human experts from repetitive tasks and accelerate scientific discovery, yet remain in their infancy. Chemical experiments involve long-horizon procedures over hazardous and deformable substances, where success requires not only task completion but also strict compliance with experimental norms. To address these challenges, we propose \\textit{RoboChemist}, a dual-loop framework that integrates Vision-Language Models (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based systems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with transparent labware, and existing VLA systems (e.g., RDT, pi0) that lack semantic-level feedback for complex tasks, our method leverages a VLM to serve as (1) a planner to decompose tasks into primitive actions, (2) a visual prompt generator to guide VLA models, and (3) a monitor to assess task success and regulatory compliance. Notably, we introduce a VLA interface that accepts image-based visual targets from the VLM, enabling precise, goal-conditioned control. Our system successfully executes both primitive actions and complete multi-step chemistry protocols. Results show 23.57% higher average success rate and a 0.298 average increase in compliance rate over state-of-the-art VLA baselines, while also demonstrating strong generalization to objects and tasks."
  },
  {
    "title": "Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and 3D Metric-Scaled Scene Reconstruction",
    "url": "http://arxiv.org/abs/2509.08813v1",
    "arxiv_id": "2509.08813v1",
    "authors": [
      "Davide Allegro",
      "Matteo Terreran",
      "Stefano Ghidoni"
    ],
    "published": "2025-09-10T17:45:16+00:00",
    "summary": "Robots often rely on RGB images for tasks like manipulation and navigation. However, reliable interaction typically requires a 3D scene representation that is metric-scaled and aligned with the robot reference frame. This depends on accurate camera-to-robot calibration and dense 3D reconstruction, tasks usually treated separately, despite both relying on geometric correspondences from RGB data. Traditional calibration needs patterns, while RGB-based reconstruction yields geometry with an unknown scale in an arbitrary frame. Multi-camera setups add further complexity, as data must be expressed in a shared reference frame. We present Calib3R, a patternless method that jointly performs camera-to-robot calibration and metric-scaled 3D reconstruction via unified optimization. Calib3R handles single- and multi-camera setups on robot arms or mobile robots. It builds on the 3D foundation model MASt3R to extract pointmaps from RGB images, which are combined with robot poses to reconstruct a scaled 3D scene aligned with the robot. Experiments on diverse datasets show that Calib3R achieves accurate calibration with less than 10 images, outperforming target-less and marker-based methods."
  },
  {
    "title": "Isomer- and state-dependent ion--molecule reactions between Coulomb-crystallised Ca$^+$ ions and 1,2-dichloroethene",
    "url": "http://arxiv.org/abs/2509.08791v1",
    "arxiv_id": "2509.08791v1",
    "authors": [
      "Lei Xu",
      "Richard Karl",
      "Jutta Toscano",
      "Stefan Willitsch"
    ],
    "published": "2025-09-10T17:20:09+00:00",
    "summary": "We report a systematic investigation of isomer- and state-dependent reactions between Coulomb-crystallised laser-cooled Ca$^+$ ions and \\emph{cis/trans}-1,2-dichloroethene (DCE) isomers. By manipulating the electronic state populations of Ca$^+$ through tuning of laser cooling parameters, we observed distinct reactivities in its ground and excited states, as well as with the geometric isomers of DCE. Our experiments revealed two primary reaction channels, formation of CaCl$^+$ and C$_2$HCaCl$^+$, followed by secondary reaction pathways. While excited-state reactions proceed at rate coefficients consistent with capture theory predictions, ground-state reactions show a systematically lower reactivity. \\emph{Ab initio} calculations of reaction pathways suggest that this suppression stems from the formation of long-lived reaction complexes. The \\textit{cis} isomer was found to exhibit a higher reactivity with all electronic states of Ca$^+$ than its \\textit{trans} counterpart. The present study provides insights into the combined effects of molecular structure and quantum states influencing ion--molecule reaction dynamics."
  },
  {
    "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2509.07980v1",
    "arxiv_id": "2509.07980v1",
    "authors": [
      "Tong Zheng",
      "Hongming Zhang",
      "Wenhao Yu",
      "Xiaoyang Wang",
      "Xinyu Yang",
      "Runpeng Dai",
      "Rui Liu",
      "Huiwen Bao",
      "Chengsong Huang",
      "Heng Huang",
      "Dong Yu"
    ],
    "published": "2025-09-09T17:59:35+00:00",
    "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \\textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1."
  },
  {
    "title": "One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation",
    "url": "http://arxiv.org/abs/2509.07978v1",
    "arxiv_id": "2509.07978v1",
    "authors": [
      "Zheng Geng",
      "Nan Wang",
      "Shaocong Xu",
      "Chongjie Ye",
      "Bohan Li",
      "Zhaoxi Chen",
      "Sida Peng",
      "Hao Zhao"
    ],
    "published": "2025-09-09T17:59:02+00:00",
    "summary": "Estimating the 6D pose of arbitrary unseen objects from a single reference image is critical for robotics operating in the long-tail of real-world instances. However, this setting is notoriously challenging: 3D models are rarely available, single-view reconstructions lack metric scale, and domain gaps between generated models and real-world images undermine robustness. We propose OnePoseViaGen, a pipeline that tackles these challenges through two key components. First, a coarse-to-fine alignment module jointly refines scale and pose by combining multi-view feature matching with render-and-compare refinement. Second, a text-guided generative domain randomization strategy diversifies textures, enabling effective fine-tuning of pose estimators with synthetic data. Together, these steps allow high-fidelity single-view 3D generation to support reliable one-shot 6D pose estimation. On challenging benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves state-of-the-art performance far surpassing prior approaches. We further demonstrate robust dexterous grasping with a real robot hand, validating the practicality of our method in real-world manipulation. Project page: https://gzwsama.github.io/OnePoseviaGen.github.io/"
  },
  {
    "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search",
    "url": "http://arxiv.org/abs/2509.07969v1",
    "arxiv_id": "2509.07969v1",
    "authors": [
      "Xin Lai",
      "Junyi Li",
      "Wei Li",
      "Tao Liu",
      "Tianjian Li",
      "Hengshuang Zhao"
    ],
    "published": "2025-09-09T17:54:21+00:00",
    "summary": "Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems."
  },
  {
    "title": "TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models",
    "url": "http://arxiv.org/abs/2509.07962v1",
    "arxiv_id": "2509.07962v1",
    "authors": [
      "Zongzheng Zhang",
      "Haobo Xu",
      "Zhuo Yang",
      "Chenghao Yue",
      "Zehao Lin",
      "Huan-ang Gao",
      "Ziwei Wang",
      "Hao Zhao"
    ],
    "published": "2025-09-09T17:50:37+00:00",
    "summary": "Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings."
  },
  {
    "title": "Comparison of the propulsion of helical microrobots based on hard- and soft-magnetic elements under rotating external magnetic fieldsv",
    "url": "http://arxiv.org/abs/2509.07959v1",
    "arxiv_id": "2509.07959v1",
    "authors": [
      "Joost Wijnmalen",
      "Leon Abelmann",
      "Iulian Apachitei"
    ],
    "published": "2025-09-09T17:45:44+00:00",
    "summary": "This study compares the propulsion of helical microrobots based on hard- and soft-magnetic elements under rotating magnetic fields. Results show that hard-magnetic microrobots achieved step-out frequencies and maximum propulsion speeds 4.5 times higher than soft-magnetic microrobots. Below saturation magnetization, soft-magnetic micro-robots demonstrated similar performance irrespective of magnetic susceptibility, high-lighting that torque generation in these materials is purely geometry-dependent. Employing a tapered ribbon design increased propulsion speed by a factor of 3.5 compared to regular helical designs. These results provide a quantitative basis for selecting materials and designs, enabling designers to weigh the propulsion benefits of hard magnets against the biocompatibility of soft-magnetic microrobots."
  },
  {
    "title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments",
    "url": "http://arxiv.org/abs/2509.06953v1",
    "arxiv_id": "2509.06953v1",
    "authors": [
      "Jiahui Yang",
      "Jason Jingzhou Liu",
      "Yulong Li",
      "Youssef Khaky",
      "Kenneth Shaw",
      "Deepak Pathak"
    ],
    "published": "2025-09-08T17:59:35+00:00",
    "summary": "Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize in complex or dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories across diverse simulation scenarios. We further improve IMPACT's static obstacle avoidance through iterative student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time using DCP-RMP, a locally reactive goal-proposal module. We evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior classical and neural methods in success rate across both simulated and real-world settings. Video results and code available at https://deep-reactive-policy.com"
  },
  {
    "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions",
    "url": "http://arxiv.org/abs/2509.06951v1",
    "arxiv_id": "2509.06951v1",
    "authors": [
      "Qi Lv",
      "Weijie Kong",
      "Hao Li",
      "Jia Zeng",
      "Zherui Qiu",
      "Delin Qu",
      "Haoming Song",
      "Qizhi Chen",
      "Xiang Deng",
      "Jiangmiao Pang"
    ],
    "published": "2025-09-08T17:58:30+00:00",
    "summary": "Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability."
  },
  {
    "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions",
    "url": "http://arxiv.org/abs/2509.06951v2",
    "arxiv_id": "2509.06951v2",
    "authors": [
      "Qi Lv",
      "Weijie Kong",
      "Hao Li",
      "Jia Zeng",
      "Zherui Qiu",
      "Delin Qu",
      "Haoming Song",
      "Qizhi Chen",
      "Xiang Deng",
      "Jiangmiao Pang"
    ],
    "published": "2025-09-08T17:58:30+00:00",
    "summary": "Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability."
  },
  {
    "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models",
    "url": "http://arxiv.org/abs/2509.06949v1",
    "arxiv_id": "2509.06949v1",
    "authors": [
      "Yinjie Wang",
      "Ling Yang",
      "Bowen Li",
      "Ye Tian",
      "Ke Shen",
      "Mengdi Wang"
    ],
    "published": "2025-09-08T17:58:06+00:00",
    "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
  },
  {
    "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning",
    "url": "http://arxiv.org/abs/2509.06948v1",
    "arxiv_id": "2509.06948v1",
    "authors": [
      "Liang Chen",
      "Xueting Han",
      "Li Shen",
      "Jing Bai",
      "Kam-Fai Wong"
    ],
    "published": "2025-09-08T17:58:02+00:00",
    "summary": "Reinforcement learning (RL) has proven effective in incentivizing the reasoning abilities of large language models (LLMs), but suffers from severe efficiency challenges due to its trial-and-error nature. While the common practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this decoupled two-stage approach limits interaction between SFT and RL, thereby constraining overall effectiveness. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. By conditioning the SFT objective on the optimal RL policy, our approach enables SFT to meta-learn how to guide RL's optimization process. During training, the lower level performs RL updates while simultaneously receiving SFT supervision, and the upper level explicitly maximizes the cooperative gain-the performance advantage of joint SFT-RL training over RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency."
  },
  {
    "title": "Outcome-based Exploration for LLM Reasoning",
    "url": "http://arxiv.org/abs/2509.06941v1",
    "arxiv_id": "2509.06941v1",
    "authors": [
      "Yuda Song",
      "Julia Kempe",
      "Remi Munos"
    ],
    "published": "2025-09-08T17:52:56+00:00",
    "summary": "Reinforcement learning (RL) has emerged as a powerful method for improving the reasoning abilities of large language models (LLMs). Outcome-based RL, which rewards policies solely for the correctness of the final answer, yields substantial accuracy gains but also induces a systematic loss in generation diversity. This collapse undermines real-world performance, where diversity is critical for test-time scaling. We analyze this phenomenon by viewing RL post-training as a sampling process and show that, strikingly, RL can reduce effective diversity even on the training set relative to the base model. Our study highlights two central findings: (i) a transfer of diversity degradation, where reduced diversity on solved problems propagates to unsolved ones, and (ii) the tractability of the outcome space, since reasoning tasks admit only a limited set of distinct answers. Motivated by these insights, we propose outcome-based exploration, which assigns exploration bonuses according to final outcomes. We introduce two complementary algorithms: historical exploration, which encourages rarely observed answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition to promote test-time diversity. Experiments on standard competition math with Llama and Qwen models demonstrate that both methods improve accuracy while mitigating diversity collapse. On the theoretical side, we formalize the benefit of outcome-based exploration through a new model of outcome-based bandits. Together, these contributions chart a practical path toward RL methods that enhance reasoning without sacrificing the diversity essential for scalable deployment."
  },
  {
    "title": "Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest",
    "url": "http://arxiv.org/abs/2509.05292v1",
    "arxiv_id": "2509.05292v1",
    "authors": [
      "Xiao Yang",
      "Mehdi Ben Ayed",
      "Longyu Zhao",
      "Fan Zhou",
      "Yuchen Shen",
      "Abe Engle",
      "Jinfeng Zhuang",
      "Ling Leng",
      "Jiajing Xu",
      "Charles Rosenberg",
      "Prathibha Deshikachar"
    ],
    "published": "2025-09-05T17:57:45+00:00",
    "summary": "The ranking utility function in an ad recommender system, which linearly combines predictions of various business goals, plays a central role in balancing values across the platform, advertisers, and users. Traditional manual tuning, while offering simplicity and interpretability, often yields suboptimal results due to its unprincipled tuning objectives, the vast amount of parameter combinations, and its lack of personalization and adaptability to seasonality. In this work, we propose a general Deep Reinforcement Learning framework for Personalized Utility Tuning (DRL-PUT) to address the challenges of multi-objective optimization within ad recommender systems. Our key contributions include: 1) Formulating the problem as a reinforcement learning task: given the state of an ad request, we predict the optimal hyperparameters to maximize a pre-defined reward. 2) Developing an approach to directly learn an optimal policy model using online serving logs, avoiding the need to estimate a value function, which is inherently challenging due to the high variance and unbalanced distribution of immediate rewards. We evaluated DRL-PUT through an online A/B experiment in Pinterest's ad recommender system. Compared to the baseline manual utility tuning approach, DRL-PUT improved the click-through rate by 9.7% and the long click-through rate by 7.7% on the treated segment. We conducted a detailed ablation study on the impact of different reward definitions and analyzed the personalization aspect of the learned policy model."
  },
  {
    "title": "Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks",
    "url": "http://arxiv.org/abs/2509.05273v1",
    "arxiv_id": "2509.05273v1",
    "authors": [
      "Jason Gardner",
      "Ayan Dutta",
      "Swapnoneel Roy",
      "O. Patrick Kreidl",
      "Ladislau Boloni"
    ],
    "published": "2025-09-05T17:29:51+00:00",
    "summary": "The growing computational demands of deep reinforcement learning (DRL) have raised concerns about the environmental and economic costs of training large-scale models. While algorithmic efficiency in terms of learning performance has been extensively studied, the energy requirements, greenhouse gas emissions, and monetary costs of DRL algorithms remain largely unexplored. In this work, we present a systematic benchmarking study of the energy consumption of seven state-of-the-art DRL algorithms, namely DQN, TRPO, A2C, ARS, PPO, RecurrentPPO, and QR-DQN, implemented using Stable Baselines. Each algorithm was trained for one million steps each on ten Atari 2600 games, and power consumption was measured in real-time to estimate total energy usage, CO2-Equivalent emissions, and electricity cost based on the U.S. national average electricity price. Our results reveal substantial variation in energy efficiency and training cost across algorithms, with some achieving comparable performance while consuming up to 24% less energy (ARS vs. DQN), emitting nearly 68% less CO2, and incurring almost 68% lower monetary cost (QR-DQN vs. RecurrentPPO) than less efficient counterparts. We further analyze the trade-offs between learning performance, training time, energy use, and financial cost, highlighting cases where algorithmic choices can mitigate environmental and economic impact without sacrificing learning performance. This study provides actionable insights for developing energy-aware and cost-efficient DRL practices and establishes a foundation for incorporating sustainability considerations into future algorithmic design and evaluation."
  },
  {
    "title": "Hunyuan-MT Technical Report",
    "url": "http://arxiv.org/abs/2509.05209v1",
    "arxiv_id": "2509.05209v1",
    "authors": [
      "Mao Zheng",
      "Zheng Li",
      "Bingxin Qu",
      "Mingyang Song",
      "Yang Du",
      "Mingrui Sun",
      "Di Wang"
    ],
    "published": "2025-09-05T16:11:05+00:00",
    "summary": "In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places a special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows a holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across a diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic."
  },
  {
    "title": "Symbolic Graphics Programming with Large Language Models",
    "url": "http://arxiv.org/abs/2509.05208v1",
    "arxiv_id": "2509.05208v1",
    "authors": [
      "Yamei Chen",
      "Haoquan Zhang",
      "Yangyi Huang",
      "Zeju Qiu",
      "Kaipeng Zhang",
      "Yandong Wen",
      "Weiyang Liu"
    ],
    "published": "2025-09-05T16:10:53+00:00",
    "summary": "Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate images rendered from SGPs. Among various SGPs, our paper sticks to scalable vector graphics (SVGs). We begin by examining the extent to which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a comprehensive benchmark covering object fidelity, scene fidelity, and compositionality (attribute binding, spatial relations, numeracy). On SGP-GenBench, we discover that frontier proprietary models substantially outperform open-source models, and performance correlates well with general coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards approach, where a format-validity gate ensures renderable SVG, and a cross-modal reward aligns text and the rendered image via strong vision encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to Qwen-2.5-7B, our method substantially improves SVG generation quality and semantics, achieving performance on par with frontier systems. We further analyze training dynamics, showing that RL induces (i) finer decomposition of objects into controllable primitives and (ii) contextual details that improve scene coherence. Our results demonstrate that symbolic graphics programming offers a precise and interpretable lens on cross-modal grounding."
  },
  {
    "title": "Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers",
    "url": "http://arxiv.org/abs/2509.05201v1",
    "arxiv_id": "2509.05201v1",
    "authors": [
      "Nariman Niknejad",
      "Gokul S. Sankar",
      "Bahare Kiumarsi",
      "Hamidreza Modares"
    ],
    "published": "2025-09-05T16:03:57+00:00",
    "summary": "This paper presents a robust model predictive control (MPC) framework that explicitly addresses the non-Gaussian noise inherent in deep learning-based perception modules used for state estimation. Recognizing that accurate uncertainty quantification of the perception module is essential for safe feedback control, our approach departs from the conventional assumption of zero-mean noise quantification of the perception error. Instead, it employs set-based state estimation with constrained zonotopes to capture biased, heavy-tailed uncertainties while maintaining bounded estimation errors. To improve computational efficiency, the robust MPC is reformulated as a linear program (LP), using a Minkowski-Lyapunov-based cost function with an added slack variable to prevent degenerate solutions. Closed-loop stability is ensured through Minkowski-Lyapunov inequalities and contractive zonotopic invariant sets. The largest stabilizing terminal set and its corresponding feedback gain are then derived via an ellipsoidal approximation of the zonotopes. The proposed framework is validated through both simulations and hardware experiments on an omnidirectional mobile robot along with a camera and a convolutional neural network-based perception module implemented within a ROS2 framework. The results demonstrate that the perception-aware MPC provides stable and accurate control performance under heavy-tailed noise conditions, significantly outperforming traditional Gaussian-noise-based designs in terms of both state estimation error bounding and overall control performance."
  },
  {
    "title": "EMMA: Scaling Mobile Manipulation via Egocentric Human Data",
    "url": "http://arxiv.org/abs/2509.04443v1",
    "arxiv_id": "2509.04443v1",
    "authors": [
      "Lawrence Y. Zhu",
      "Pranav Kuppili",
      "Ryan Punamiya",
      "Patcharapong Aphiwetsa",
      "Dhruv Patel",
      "Simar Kareer",
      "Sehoon Ha",
      "Danfei Xu"
    ],
    "published": "2025-09-04T17:59:10+00:00",
    "summary": "Scaling mobile manipulation imitation learning is bottlenecked by expensive mobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA), an end-to-end framework training mobile manipulation policies from human mobile manipulation data with static robot data, sidestepping mobile teleoperation. To accomplish this, we co-train human full-body motion data with static robot data. In our experiments across three real-world tasks, EMMA demonstrates comparable performance to baselines trained on teleoperated mobile robot data (Mobile ALOHA), achieving higher or equivalent task performance in full task success. We find that EMMA is able to generalize to new spatial configurations and scenes, and we observe positive performance scaling as we increase the hours of human data, opening new avenues for scalable robotic learning in real-world environments. Details of this project can be found at https://ego-moma.github.io/."
  },
  {
    "title": "One Flight Over the Gap: A Survey from Perspective to Panoramic Vision",
    "url": "http://arxiv.org/abs/2509.04444v1",
    "arxiv_id": "2509.04444v1",
    "authors": [
      "Xin Lin",
      "Xian Ge",
      "Dizhe Zhang",
      "Zhaoliang Wan",
      "Xianshun Wang",
      "Xiangtai Li",
      "Wenjie Jiang",
      "Bo Du",
      "Dacheng Tao",
      "Ming-Hsuan Yang",
      "Lu Qi"
    ],
    "published": "2025-09-04T17:59:10+00:00",
    "summary": "Driven by the demand for spatial intelligence and holistic scene perception, omnidirectional images (ODIs), which provide a complete 360\\textdegree{} field of view, are receiving growing attention across diverse applications such as virtual reality, autonomous driving, and embodied robotics. Despite their unique characteristics, ODIs exhibit remarkable differences from perspective images in geometric projection, spatial distribution, and boundary continuity, making it challenging for direct domain adaption from perspective methods. This survey reviews recent panoramic vision techniques with a particular emphasis on the perspective-to-panorama adaptation. We first revisit the panoramic imaging pipeline and projection methods to build the prior knowledge required for analyzing the structural disparities. Then, we summarize three challenges of domain adaptation: severe geometric distortions near the poles, non-uniform sampling in Equirectangular Projection (ERP), and periodic boundary continuity. Building on this, we cover 20+ representative tasks drawn from more than 300 research papers in two dimensions. On one hand, we present a cross-method analysis of representative strategies for addressing panoramic specific challenges across different tasks. On the other hand, we conduct a cross-task comparison and classify panoramic vision into four major categories: visual quality enhancement and assessment, visual understanding, multimodal understanding, and visual generation. In addition, we discuss open challenges and future directions in data, models, and applications that will drive the advancement of panoramic vision research. We hope that our work can provide new insight and forward looking perspectives to advance the development of panoramic vision technologies. Our project page is https://insta360-research-team.github.io/Survey-of-Panorama"
  },
  {
    "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation",
    "url": "http://arxiv.org/abs/2509.04441v1",
    "arxiv_id": "2509.04441v1",
    "authors": [
      "Hao-Shu Fang",
      "Branden Romero",
      "Yichen Xie",
      "Arthur Hu",
      "Bo-Ruei Huang",
      "Juan Alvarez",
      "Matthew Kim",
      "Gabriel Margolis",
      "Kavya Anbarasu",
      "Masayoshi Tomizuka",
      "Edward Adelson",
      "Pulkit Agrawal"
    ],
    "published": "2025-09-04T17:57:13+00:00",
    "summary": "We introduce perioperation, a paradigm for robotic data collection that sensorizes and records human manipulation while maximizing the transferability of the data to real robots. We implement this paradigm in DEXOP, a passive hand exoskeleton designed to maximize human ability to collect rich sensory (vision + tactile) data for diverse dexterous manipulation tasks in natural environments. DEXOP mechanically connects human fingers to robot fingers, providing users with direct contact feedback (via proprioception) and mirrors the human hand pose to the passive robot hand to maximize the transfer of demonstrated skills to the robot. The force feedback and pose mirroring make task demonstrations more natural for humans compared to teleoperation, increasing both speed and accuracy. We evaluate DEXOP across a range of dexterous, contact-rich tasks, demonstrating its ability to collect high-quality demonstration data at scale. Policies learned with DEXOP data significantly improve task performance per unit time of data collection compared to teleoperation, making DEXOP a powerful tool for advancing robot dexterity. Our project page is at https://dex-op.github.io."
  },
  {
    "title": "Towards a Unified View of Large Language Model Post-Training",
    "url": "http://arxiv.org/abs/2509.04419v1",
    "arxiv_id": "2509.04419v1",
    "authors": [
      "Xingtai Lv",
      "Yuxin Zuo",
      "Youbang Sun",
      "Hongyi Liu",
      "Yuntian Wei",
      "Zhekai Chen",
      "Lixuan He",
      "Xuekai Zhu",
      "Kaiyan Zhang",
      "Bingning Wang",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "published": "2025-09-04T17:40:33+00:00",
    "summary": "Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families."
  },
  {
    "title": "SAFE--MA--RRT: Multi-Agent Motion Planning with Data-Driven Safety Certificates",
    "url": "http://arxiv.org/abs/2509.04413v1",
    "arxiv_id": "2509.04413v1",
    "authors": [
      "Babak Esmaeili",
      "Hamidreza Modares"
    ],
    "published": "2025-09-04T17:34:59+00:00",
    "summary": "This paper proposes a fully data-driven motion-planning framework for homogeneous linear multi-agent systems that operate in shared, obstacle-filled workspaces without access to explicit system models. Each agent independently learns its closed-loop behavior from experimental data by solving convex semidefinite programs that generate locally invariant ellipsoids and corresponding state-feedback gains. These ellipsoids, centered along grid-based waypoints, certify the dynamic feasibility of short-range transitions and define safe regions of operation. A sampling-based planner constructs a tree of such waypoints, where transitions are allowed only when adjacent ellipsoids overlap, ensuring invariant-to-invariant transitions and continuous safety. All agents expand their trees simultaneously and are coordinated through a space-time reservation table that guarantees inter-agent safety by preventing simultaneous occupancy and head-on collisions. Each successful edge in the tree is equipped with its own local controller, enabling execution without re-solving optimization problems at runtime. The resulting trajectories are not only dynamically feasible but also provably safe with respect to both environmental constraints and inter-agent collisions. Simulation results demonstrate the effectiveness of the approach in synthesizing synchronized, safe trajectories for multiple agents under shared dynamics and constraints, using only data and convex optimization tools."
  },
  {
    "title": "Can LLMs Lie? Investigation beyond Hallucination",
    "url": "http://arxiv.org/abs/2509.03518v1",
    "arxiv_id": "2509.03518v1",
    "authors": [
      "Haoran Huan",
      "Mihir Prabhudesai",
      "Mengning Wu",
      "Shantanu Jaiswal",
      "Deepak Pathak"
    ],
    "published": "2025-09-03T17:59:45+00:00",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities across a variety of tasks, but their increasing autonomy in real-world applications raises concerns about their trustworthiness. While hallucinations-unintentional falsehoods-have been widely studied, the phenomenon of lying, where an LLM knowingly generates falsehoods to achieve an ulterior objective, remains underexplored. In this work, we systematically investigate the lying behavior of LLMs, differentiating it from hallucinations and testing it in practical scenarios. Through mechanistic interpretability techniques, we uncover the neural mechanisms underlying deception, employing logit lens analysis, causal interventions, and contrastive activation steering to identify and control deceptive behavior. We study real-world lying scenarios and introduce behavioral steering vectors that enable fine-grained manipulation of lying tendencies. Further, we explore the trade-offs between lying and end-task performance, establishing a Pareto frontier where dishonesty can enhance goal optimization. Our findings contribute to the broader discourse on AI ethics, shedding light on the risks and potential safeguards for deploying LLMs in high-stakes environments. Code and more illustrations are available at https://llm-liar.github.io/"
  },
  {
    "title": "Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories",
    "url": "http://arxiv.org/abs/2509.03515v1",
    "arxiv_id": "2509.03515v1",
    "authors": [
      "Yanlin Zhang",
      "Sungyong Chung",
      "Nachuan Li",
      "Dana Monzer",
      "Hani S. Mahmassani",
      "Samer H. Hamdar",
      "Alireza Talebpour"
    ],
    "published": "2025-09-03T17:56:46+00:00",
    "summary": "The Waymo Open Motion Dataset (WOMD) has become a popular resource for data-driven modeling of autonomous vehicles (AVs) behavior. However, its validity for behavioral analysis remains uncertain due to proprietary post-processing, the absence of error quantification, and the segmentation of trajectories into 20-second clips. This study examines whether WOMD accurately captures the dynamics and interactions observed in real-world AV operations. Leveraging an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona (PHX), we perform comparative analyses across three representative urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing behaviors. For the discharging analysis, headways are manually extracted from aerial video to ensure negligible measurement error. For the car-following and lane-changing cases, we apply the Simulation-Extrapolation (SIMEX) method to account for empirically estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to quantify behavioral differences. Results across all scenarios consistently show that behavior in PHX falls outside the behavioral envelope of WOMD. Notably, WOMD underrepresents short headways and abrupt decelerations. These findings suggest that behavioral models calibrated solely on WOMD may systematically underestimate the variability, risk, and complexity of naturalistic driving. Caution is therefore warranted when using WOMD for behavior modeling without proper validation against independently collected data."
  },
  {
    "title": "Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena",
    "url": "http://arxiv.org/abs/2509.03500v1",
    "arxiv_id": "2509.03500v1",
    "authors": [
      "Itai Zilberstein",
      "Alberto Candela",
      "Steve Chien"
    ],
    "published": "2025-09-03T17:32:15+00:00",
    "summary": "Advancements in onboard computing mean remote sensing agents can employ state-of-the-art computer vision and machine learning at the edge. These capabilities can be leveraged to unlock new rare, transient, and pinpoint measurements of dynamic science phenomena. In this paper, we present an automated workflow that synthesizes the detection of these dynamic events in look-ahead satellite imagery with autonomous trajectory planning for a follow-up high-resolution sensor to obtain pinpoint measurements. We apply this workflow to the use case of observing volcanic plumes. We analyze classification approaches including traditional machine learning algorithms and convolutional neural networks. We present several trajectory planning algorithms that track the morphological features of a plume and integrate these algorithms with the classifiers. We show through simulation an order of magnitude increase in the utility return of the high-resolution instrument compared to baselines while maintaining efficient runtimes."
  },
  {
    "title": "On Entropy Control in LLM-RL Algorithms",
    "url": "http://arxiv.org/abs/2509.03493v1",
    "arxiv_id": "2509.03493v1",
    "authors": [
      "Han Shen"
    ],
    "published": "2025-09-03T17:23:19+00:00",
    "summary": "For RL algorithms, appropriate entropy control is crucial to their effectiveness. To control the policy entropy, a commonly used method is entropy regularization, which is adopted in various popular RL algorithms including PPO, SAC and A3C. Although entropy regularization proves effective in robotic and games RL conventionally, studies found that it gives weak to no gains in LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL setting. Specifically, we first argue that the conventional entropy regularization suffers from the LLM's extremely large response space and the sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy control method that utilizes a new clamped entropy bonus with an automatically adjusted coefficient. The clamped entropy is evaluated with the re-normalized policy defined on certain smaller token space, which encourages exploration within a more compact response set. In addition, the algorithm automatically adjusts entropy coefficient according to the clamped entropy value, effectively controlling the entropy-induced bias while leveraging the entropy's benefits. AEnt is tested in math-reasoning tasks under different base models and datasets, and it is observed that AEnt outperforms the baselines consistently across multiple benchmarks."
  },
  {
    "title": "SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models",
    "url": "http://arxiv.org/abs/2509.03487v1",
    "arxiv_id": "2509.03487v1",
    "authors": [
      "Jigang Fan",
      "Zhenghong Zhou",
      "Ruofan Jin",
      "Le Cong",
      "Mengdi Wang",
      "Zaixi Zhang"
    ],
    "published": "2025-09-03T17:13:56+00:00",
    "summary": "Proteins play crucial roles in almost all biological processes. The advancement of deep learning has greatly accelerated the development of protein foundation models, leading to significant successes in protein understanding and design. However, the lack of systematic red-teaming for these models has raised serious concerns about their potential misuse, such as generating proteins with biological safety risks. This paper introduces SafeProtein, the first red-teaming framework designed for protein foundation models to the best of our knowledge. SafeProtein combines multimodal prompt engineering and heuristic beam search to systematically design red-teaming methods and conduct tests on protein foundation models. We also curated SafeProtein-Bench, which includes a manually constructed red-teaming benchmark dataset and a comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks on state-of-the-art protein foundation models (up to 70% attack success rate for ESM3), revealing potential biological safety risks in current protein foundation models and providing insights for the development of robust security protection technologies for frontier models. The codes will be made publicly available at https://github.com/jigang-fan/SafeProtein."
  },
  {
    "title": "Tree-Guided Diffusion Planner",
    "url": "http://arxiv.org/abs/2508.21800v1",
    "arxiv_id": "2508.21800v1",
    "authors": [
      "Hyeonseong Jeon",
      "Cheolhong Min",
      "Jaesik Park"
    ],
    "published": "2025-08-29T17:27:44+00:00",
    "summary": "Planning with pretrained diffusion models has emerged as a promising approach for solving test-time guided control problems. However, standard gradient guidance typically performs optimally under convex and differentiable reward landscapes, showing substantially reduced effectiveness in real-world scenarios involving non-convex objectives, non-differentiable constraints, and multi-reward structures. Furthermore, recent supervised planning approaches require task-specific training or value estimators, which limits test-time flexibility and zero-shot generalization. We propose a Tree-guided Diffusion Planner (TDP), a zero-shot test-time planning framework that balances exploration and exploitation through structured trajectory generation. We frame test-time planning as a tree search problem using a bi-level sampling process: (1) diverse parent trajectories are produced via training-free particle guidance to encourage broad exploration, and (2) sub-trajectories are refined through fast conditional denoising guided by task objectives. TDP addresses the limitations of gradient guidance by exploring diverse trajectory regions and harnessing gradient information across this expanded solution space using only pretrained models and test-time reward signals. We evaluate TDP on three diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze multi-goal exploration. TDP consistently outperforms state-of-the-art approaches on all tasks. The project page can be found at: tree-diffusion-planner.github.io."
  },
  {
    "title": "DynaMark: A Reinforcement Learning Framework for Dynamic Watermarking in Industrial Machine Tool Controllers",
    "url": "http://arxiv.org/abs/2508.21797v1",
    "arxiv_id": "2508.21797v1",
    "authors": [
      "Navid Aftabi",
      "Abhishek Hanchate",
      "Satish Bukkapatnam",
      "Dan Li"
    ],
    "published": "2025-08-29T17:24:00+00:00",
    "summary": "Industry 4.0's highly networked Machine Tool Controllers (MTCs) are prime targets for replay attacks that use outdated sensor data to manipulate actuators. Dynamic watermarking can reveal such tampering, but current schemes assume linear-Gaussian dynamics and use constant watermark statistics, making them vulnerable to the time-varying, partly proprietary behavior of MTCs. We close this gap with DynaMark, a reinforcement learning framework that models dynamic watermarking as a Markov decision process (MDP). It learns an adaptive policy online that dynamically adapts the covariance of a zero-mean Gaussian watermark using available measurements and detector feedback, without needing system knowledge. DynaMark maximizes a unique reward function balancing control performance, energy consumption, and detection confidence dynamically. We develop a Bayesian belief updating mechanism for real-time detection confidence in linear systems. This approach, independent of specific system assumptions, underpins the MDP for systems with linear dynamics. On a Siemens Sinumerik 828D controller digital twin, DynaMark achieves a reduction in watermark energy by 70% while preserving the nominal trajectory, compared to constant variance baselines. It also maintains an average detection delay equivalent to one sampling interval. A physical stepper-motor testbed validates these findings, rapidly triggering alarms with less control performance decline and exceeding existing benchmarks."
  },
  {
    "title": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval",
    "url": "http://arxiv.org/abs/2508.21788v1",
    "arxiv_id": "2508.21788v1",
    "authors": [
      "In\u00e9s Altemir Marinas",
      "Anastasiia Kucherenko",
      "Andrei Kucharavy"
    ],
    "published": "2025-08-29T17:04:20+00:00",
    "summary": "Large language models (LLMs) rely heavily on web-scale datasets like Common Crawl, which provides over 80\\% of training data for some modern models. However, the indiscriminate nature of web crawling raises challenges in data quality, safety, and ethics. Despite the critical importance of training data quality, prior research on harmful content has been limited to small samples due to computational constraints. This project presents a framework for indexing and analyzing LLM training datasets using an ElasticSearch-based pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages), achieving fast query performance--most searches in milliseconds, all under 2 seconds. Our work demonstrates real-time dataset analysis, offering practical tools for safer, more accountable AI systems."
  },
  {
    "title": "UItron: Foundational GUI Agent with Advanced Perception and Planning",
    "url": "http://arxiv.org/abs/2508.21767v1",
    "arxiv_id": "2508.21767v1",
    "authors": [
      "Zhixiong Zeng",
      "Jing Huang",
      "Liming Zheng",
      "Wenkang Han",
      "Yufeng Zhong",
      "Lei Chen",
      "Longrong Yang",
      "Yingjie Chu",
      "Yuzhi He",
      "Lin Ma"
    ],
    "published": "2025-08-29T16:40:57+00:00",
    "summary": "GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application."
  },
  {
    "title": "Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL",
    "url": "http://arxiv.org/abs/2508.21739v1",
    "arxiv_id": "2508.21739v1",
    "authors": [
      "Hamza Ezzaoui Rahali",
      "Abhilasha Dave",
      "Larry Ruckman",
      "Mohammad Mehdi Rahimifar",
      "Audrey C. Therrien",
      "James J. Russel",
      "Ryan T. Herbst"
    ],
    "published": "2025-08-29T16:04:15+00:00",
    "summary": "The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline experiments at rates of up to 1~MHz, with detectors producing data throughputs exceeding 1 TB/s. Managing such massive data streams presents significant challenges, as transmission and storage infrastructures become prohibitively expensive. Machine learning (ML) offers a promising solution for real-time data reduction, but conventional implementations introduce excessive latency, making them unsuitable for high-speed experimental environments. To address these challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized framework designed to deploy real-time ML inference models on Field-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to dynamically update model weights without requiring FPGA resynthesis, enhancing flexibility for adaptive learning applications. To further enhance usability and accessibility, we introduce Auto-SNL, a Python extension that streamlines the process of converting Python-based neural network models into SNL-compatible high-level synthesis code. This paper presents a benchmark comparison against hls4ml, the current state-of-the-art tool, across multiple neural network architectures, fixed-point precisions, and synthesis configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL achieves competitive or superior latency in most tested architectures, while in some cases also offering FPGA resource savings. This adaptation demonstrates SNL's versatility, opening new opportunities for researchers and academics in fields such as high-energy physics, medical imaging, robotics, and many more."
  },
  {
    "title": "OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning",
    "url": "http://arxiv.org/abs/2508.21066v1",
    "arxiv_id": "2508.21066v1",
    "authors": [
      "Yuan Gong",
      "Xionghui Wang",
      "Jie Wu",
      "Shiyin Wang",
      "Yitong Wang",
      "Xinglong Wu"
    ],
    "published": "2025-08-28T17:59:46+00:00",
    "summary": "In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \\textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io"
  },
  {
    "title": "Learning on the Fly: Rapid Policy Adaptation via Differentiable Simulation",
    "url": "http://arxiv.org/abs/2508.21065v1",
    "arxiv_id": "2508.21065v1",
    "authors": [
      "Jiahe Pan",
      "Jiaxu Xing",
      "Rudolf Reiter",
      "Yifan Zhai",
      "Elie Aljalbout",
      "Davide Scaramuzza"
    ],
    "published": "2025-08-28T17:59:34+00:00",
    "summary": "Learning control policies in simulation enables rapid, safe, and cost-effective development of advanced robotic capabilities. However, transferring these policies to the real world remains difficult due to the sim-to-real gap, where unmodeled dynamics and environmental disturbances can degrade policy performance. Existing approaches, such as domain randomization and Real2Sim2Real pipelines, can improve policy robustness, but either struggle under out-of-distribution conditions or require costly offline retraining. In this work, we approach these problems from a different perspective. Instead of relying on diverse training conditions before deployment, we focus on rapidly adapting the learned policy in the real world in an online fashion. To achieve this, we propose a novel online adaptive learning framework that unifies residual dynamics learning with real-time policy adaptation inside a differentiable simulation. Starting from a simple dynamics model, our framework refines the model continuously with real-world data to capture unmodeled effects and disturbances such as payload changes and wind. The refined dynamics model is embedded in a differentiable simulation framework, enabling gradient backpropagation through the dynamics and thus rapid, sample-efficient policy updates beyond the reach of classical RL methods like PPO. All components of our system are designed for rapid adaptation, enabling the policy to adjust to unseen disturbances within 5 seconds of training. We validate the approach on agile quadrotor control under various disturbances in both simulation and the real world. Our framework reduces hovering error by up to 81% compared to L1-MPC and 55% compared to DATT, while also demonstrating robustness in vision-based control without explicit state estimation."
  },
  {
    "title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation",
    "url": "http://arxiv.org/abs/2508.21063v1",
    "arxiv_id": "2508.21063v1",
    "authors": [
      "Ruixuan Liu",
      "Philip Huang",
      "Ava Pun",
      "Kangle Deng",
      "Shobhit Aggarwal",
      "Kevin Tang",
      "Michelle Liu",
      "Deva Ramanan",
      "Jun-Yan Zhu",
      "Jiaoyang Li",
      "Changliu Liu"
    ],
    "published": "2025-08-28T17:59:05+00:00",
    "summary": "Creating assembly products demands significant manual effort and expert knowledge in 1) designing the assembly and 2) constructing the product. This paper introduces Prompt-to-Product, an automated pipeline that generates real-world assembly products from natural language prompts. Specifically, we leverage LEGO bricks as the assembly platform and automate the process of creating brick assembly structures. Given the user design requirements, Prompt-to-Product generates physically buildable brick designs, and then leverages a bimanual robotic system to construct the real assembly products, bringing user imaginations into the real world. We conduct a comprehensive user study, and the results demonstrate that Prompt-to-Product significantly lowers the barrier and reduces manual effort in creating assembly products from imaginative ideas."
  },
  {
    "title": "FakeParts: a New Family of AI-Generated DeepFakes",
    "url": "http://arxiv.org/abs/2508.21052v1",
    "arxiv_id": "2508.21052v1",
    "authors": [
      "Gaetan Brison",
      "Soobash Daiboo",
      "Samy Aimeur",
      "Awais Hussain Sani",
      "Xi Wang",
      "Gianni Franchi",
      "Vicky Kalogeiton"
    ],
    "published": "2025-08-28T17:55:14+00:00",
    "summary": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations."
  },
  {
    "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification",
    "url": "http://arxiv.org/abs/2508.21046v1",
    "arxiv_id": "2508.21046v1",
    "authors": [
      "Wei Li",
      "Renshan Zhang",
      "Rui Shao",
      "Jie He",
      "Liqiang Nie"
    ],
    "published": "2025-08-28T17:50:58+00:00",
    "summary": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA."
  },
  {
    "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning",
    "url": "http://arxiv.org/abs/2508.20096v1",
    "arxiv_id": "2508.20096v1",
    "authors": [
      "Zeyi Sun",
      "Yuhang Cao",
      "Jianze Liang",
      "Qiushi Sun",
      "Ziyu Liu",
      "Zhixiong Zhang",
      "Yuhang Zang",
      "Xiaoyi Dong",
      "Kai Chen",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "published": "2025-08-27T17:59:50+00:00",
    "summary": "Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models."
  },
  {
    "title": "Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning",
    "url": "http://arxiv.org/abs/2508.20095v1",
    "arxiv_id": "2508.20095v1",
    "authors": [
      "Jinhao Liang",
      "Sven Koenig",
      "Ferdinando Fioretto"
    ],
    "published": "2025-08-27T17:59:36+00:00",
    "summary": "Multi-Robot Motion Planning (MRMP) involves generating collision-free trajectories for multiple robots operating in a shared continuous workspace. While discrete multi-agent path finding (MAPF) methods are broadly adopted due to their scalability, their coarse discretization severely limits trajectory quality. In contrast, continuous optimization-based planners offer higher-quality paths but suffer from the curse of dimensionality, resulting in poor scalability with respect to the number of robots. This paper tackles the limitations of these two approaches by introducing a novel framework that integrates discrete MAPF solvers with constrained generative diffusion models. The resulting framework, called Discrete-Guided Diffusion (DGD), has three key characteristics: (1) it decomposes the original nonconvex MRMP problem into tractable subproblems with convex configuration spaces, (2) it combines discrete MAPF solutions with constrained optimization techniques to guide diffusion models capture complex spatiotemporal dependencies among robots, and (3) it incorporates a lightweight constraint repair mechanism to ensure trajectory feasibility. The proposed method sets a new state-of-the-art performance in large-scale, complex environments, scaling to 100 robots while achieving planning efficiency and high success rates."
  },
  {
    "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation",
    "url": "http://arxiv.org/abs/2508.20085v1",
    "arxiv_id": "2508.20085v1",
    "authors": [
      "Zhecheng Yuan",
      "Tianming Wei",
      "Langzhe Gu",
      "Pu Hua",
      "Tianhai Liang",
      "Yuanpei Chen",
      "Huazhe Xu"
    ],
    "published": "2025-08-27T17:53:46+00:00",
    "summary": "Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/."
  },
  {
    "title": "Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning",
    "url": "http://arxiv.org/abs/2508.20083v1",
    "arxiv_id": "2508.20083v1",
    "authors": [
      "Yanbo Dai",
      "Zhenlan Ji",
      "Zongjie Li",
      "Kuan Li",
      "Shuai Wang"
    ],
    "published": "2025-08-27T17:49:28+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) has become a standard approach for improving the reliability of large language models (LLMs). Prior work demonstrates the vulnerability of RAG systems by misleading them into generating attacker-chosen outputs through poisoning the knowledge base. However, this paper uncovers that such attacks could be mitigated by the strong \\textit{self-correction ability (SCA)} of modern LLMs, which can reject false context once properly configured. This SCA poses a significant challenge for attackers aiming to manipulate RAG systems.   In contrast to previous poisoning methods, which primarily target the knowledge base, we introduce \\textsc{DisarmRAG}, a new poisoning paradigm that compromises the retriever itself to suppress the SCA and enforce attacker-chosen outputs. This compromisation enables the attacker to straightforwardly embed anti-SCA instructions into the context provided to the generator, thereby bypassing the SCA. To this end, we present a contrastive-learning-based model editing technique that performs localized and stealthy edits, ensuring the retriever returns a malicious instruction only for specific victim queries while preserving benign retrieval behavior. To further strengthen the attack, we design an iterative co-optimization framework that automatically discovers robust instructions capable of bypassing prompt-based defenses. We extensively evaluate DisarmRAG across six LLMs and three QA benchmarks. Our results show near-perfect retrieval of malicious instructions, which successfully suppress SCA and achieve attack success rates exceeding 90\\% under diverse defensive prompts. Also, the edited retriever remains stealthy under several detection methods, highlighting the urgent need for retriever-centric defenses."
  },
  {
    "title": "Seam360GS: Seamless 360\u00b0 Gaussian Splatting from Real-World Omnidirectional Images",
    "url": "http://arxiv.org/abs/2508.20080v1",
    "arxiv_id": "2508.20080v1",
    "authors": [
      "Changha Shin",
      "Woong Oh Cho",
      "Seon Joo Kim"
    ],
    "published": "2025-08-27T17:46:46+00:00",
    "summary": "360-degree visual content is widely shared on platforms such as YouTube and plays a central role in virtual reality, robotics, and autonomous navigation. However, consumer-grade dual-fisheye systems consistently yield imperfect panoramas due to inherent lens separation and angular distortions. In this work, we introduce a novel calibration framework that incorporates a dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach not only simulates the realistic visual artifacts produced by dual-fisheye cameras but also enables the synthesis of seamlessly rendered 360-degree images. By jointly optimizing 3D Gaussian parameters alongside calibration variables that emulate lens gaps and angular distortions, our framework transforms imperfect omnidirectional inputs into flawless novel view synthesis. Extensive evaluations on real-world datasets confirm that our method produces seamless renderings-even from imperfect images-and outperforms existing 360-degree rendering models."
  },
  {
    "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space",
    "url": "http://arxiv.org/abs/2508.19247v1",
    "arxiv_id": "2508.19247v1",
    "authors": [
      "Lin Li",
      "Zehuan Huang",
      "Haoran Feng",
      "Gengxiong Zhuang",
      "Rui Chen",
      "Chunchao Guo",
      "Lu Sheng"
    ],
    "published": "2025-08-26T17:59:47+00:00",
    "summary": "3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/."
  },
  {
    "title": "Articulate3D: Zero-Shot Text-Driven 3D Object Posing",
    "url": "http://arxiv.org/abs/2508.19244v1",
    "arxiv_id": "2508.19244v1",
    "authors": [
      "Oishi Deb",
      "Anjun Hu",
      "Ashkan Khakzar",
      "Philip Torr",
      "Christian Rupprecht"
    ],
    "published": "2025-08-26T17:59:17+00:00",
    "summary": "We propose a training-free method, Articulate3D, to pose a 3D asset through language control. Despite advances in vision and language models, this task remains surprisingly challenging. To achieve this goal, we decompose the problem into two steps. We modify a powerful image-generator to create target images conditioned on the input image and a text instruction. We then align the mesh to the target images through a multi-view pose optimisation step. In detail, we introduce a self-attention rewiring mechanism (RSActrl) that decouples the source structure from pose within an image generative model, allowing it to maintain a consistent structure across varying poses. We observed that differentiable rendering is an unreliable signal for articulation optimisation; instead, we use keypoints to establish correspondences between input and target images. The effectiveness of Articulate3D is demonstrated across a diverse range of 3D objects and free-form text prompts, successfully manipulating poses while maintaining the original identity of the mesh. Quantitative evaluations and a comparative user study, in which our method was preferred over 85\\% of the time, confirm its superiority over existing approaches. Project page:https://odeb1.github.io/articulate3d_page_deb/"
  },
  {
    "title": "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation",
    "url": "http://arxiv.org/abs/2508.19236v1",
    "arxiv_id": "2508.19236v1",
    "authors": [
      "Hao Shi",
      "Bin Xie",
      "Yingfei Liu",
      "Lin Sun",
      "Fengrong Liu",
      "Tiancai Wang",
      "Erjin Zhou",
      "Haoqiang Fan",
      "Xiangyu Zhang",
      "Gao Huang"
    ],
    "published": "2025-08-26T17:57:16+00:00",
    "summary": "Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline. Project Page: https://shihao1895.github.io/MemoryVLA"
  },
  {
    "title": "Beyond Competitive Gaming: How Casual Players Evaluate and Respond to Teammate Performance",
    "url": "http://arxiv.org/abs/2508.19230v1",
    "arxiv_id": "2508.19230v1",
    "authors": [
      "Kaushall Senthil Nathan",
      "Jieun Lee",
      "Derrick M. Wang",
      "Geneva M. Smith",
      "Eugene Kukshinov",
      "Daniel Harley",
      "Lennart E. Nacke"
    ],
    "published": "2025-08-26T17:45:32+00:00",
    "summary": "Teammate performance evaluation fundamentally shapes intervention design in video games. However, our current understanding stems primarily from competitive E-Sports contexts where individual performance directly impacts outcomes. This research addresses whether performance evaluation mechanisms and behavioural responses identified in competitive games generalize to casual cooperative games. We investigated how casual players evaluate teammate competence and respond behaviourally in a controlled between-subjects experiment (N=23). We manipulated confederate performance in Overcooked 2, combining observations, NASA TLX self-reports, and interviews. We present two key findings. (1) Observations revealed frustration behaviours completely absent in self-report data. Thus, these instruments assess fundamentally distinct constructs. (2) Participants consistently evaluated teammate performance through relative comparison rather than absolute metrics. This contradicts task-performance operationalizations dominant in competitive gaming research. Hence, performance evaluation frameworks from competitive contexts cannot be directly applied to casual cooperative games. We provide empirical evidence that performance evaluation in casual games requires a comparative operationalization."
  },
  {
    "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
    "url": "http://arxiv.org/abs/2508.19229v1",
    "arxiv_id": "2508.19229v1",
    "authors": [
      "Wei Xiong",
      "Wenting Zhao",
      "Weizhe Yuan",
      "Olga Golovneva",
      "Tong Zhang",
      "Jason Weston",
      "Sainbayar Sukhbaatar"
    ],
    "published": "2025-08-26T17:45:05+00:00",
    "summary": "As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search."
  },
  {
    "title": "FlowVLA: Thinking in Motion with a Visual Chain of Thought",
    "url": "http://arxiv.org/abs/2508.18269v1",
    "arxiv_id": "2508.18269v1",
    "authors": [
      "Zhide Zhong",
      "Haodong Yan",
      "Junfeng Li",
      "Xiangchen Liu",
      "Xin Gong",
      "Wenxuan Song",
      "Jiayi Chen",
      "Haoang Li"
    ],
    "published": "2025-08-25T17:59:21+00:00",
    "summary": "Many Vision-Language-Action (VLA) models rely on an internal world model trained via next-frame prediction. This approach, however, struggles with physical reasoning as it entangles static appearance with dynamic motion, often resulting in implausible visual forecasts and inefficient policy learning. To address these limitations, we introduce the Visual Chain of Thought (Visual CoT): a pre-training framework that encourages a model to reason about how a scene evolves before predicting what it will look like. We instantiate this principle in FlowVLA, which predicts a future frame ($v_{t+1}$) only after generating an intermediate optical flow representation ($f_t$) that encodes motion dynamics. This ``$v_t \\rightarrow f_t \\rightarrow v_{t+1}$'' reasoning process is implemented within a single autoregressive Transformer, guiding the model to learn disentangled dynamics. As a result, FlowVLA produces coherent visual predictions and facilitates more efficient policy learning. Experiments on challenging robotics manipulation benchmarks demonstrate state-of-the-art performance with substantially improved sample efficiency, pointing toward a more principled foundation for world modeling. Project page: https://irpn-lab.github.io/FlowVLA/"
  },
  {
    "title": "SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation",
    "url": "http://arxiv.org/abs/2508.18268v1",
    "arxiv_id": "2508.18268v1",
    "authors": [
      "Haoyuan Deng",
      "Wenkai Guo",
      "Qianzhun Wang",
      "Zhenyu Wu",
      "Ziwei Wang"
    ],
    "published": "2025-08-25T17:59:02+00:00",
    "summary": "Bimanual manipulation has been widely applied in household services and manufacturing, which enables the complex task completion with coordination requirements. Recent diffusion-based policy learning approaches have achieved promising performance in modeling action distributions for bimanual manipulation. However, they ignored the physical safety constraints of bimanual manipulation, which leads to the dangerous behaviors with damage to robots and objects. To this end, we propose a test-time trajectory optimization framework named SafeBimanual for any pre-trained diffusion-based bimanual manipulation policies, which imposes the safety constraints on bimanual actions to avoid dangerous robot behaviors with improved success rate. Specifically, we design diverse cost functions for safety constraints in different dual-arm cooperation patterns including avoidance of tearing objects and collision between arms and objects, which optimizes the manipulator trajectories with guided sampling of diffusion denoising process. Moreover, we employ a vision-language model (VLM) to schedule the cost functions by specifying keypoints and corresponding pairwise relationship, so that the optimal safety constraint is dynamically generated in the entire bimanual manipulation process. SafeBimanual demonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase in success rate and a 18.8% reduction in unsafe interactions over state-of-the-art diffusion-based methods. Extensive experiments on 4 real-world tasks further verify its practical value by improving the success rate by 32.5%."
  },
  {
    "title": "Caregiver-in-the-Loop AI: A Simulation-Based Feasibility Study for Dementia Task Verification",
    "url": "http://arxiv.org/abs/2508.18267v1",
    "arxiv_id": "2508.18267v1",
    "authors": [
      "Joy Lai",
      "David Black",
      "Kelly Beaton",
      "Bing Ye",
      "Alex Mihailidis"
    ],
    "published": "2025-08-25T17:58:52+00:00",
    "summary": "Caregivers of people living with dementia (PLwD) experience stress when verifying whether tasks are truly completed, even with digital reminder systems. Generative AI, such as GPT-4, may help by automating task verification through follow-up questioning and decision support.   This feasibility study evaluates an AI-powered task verification system integrated with digital reminders for PLwD. It examines (1) GPT-4's ability to generate effective follow-up questions, (2) the accuracy of an AI-driven response flagging mechanism, and (3) the role of caregiver feedback in refining system adaptability. A simulated pipeline was tested on 64 anonymized reminders. GPT-4 generated follow-up questions with and without contextual information about PLwD routines. Responses were classified into High, Medium, or Low concern, and simulated caregiver feedback was used to refine outputs.   Results show that contextual information and caregiver input improved the clarity and relevance of AI-generated questions. The flagging system accurately identified concerns, particularly for safety-critical tasks, though subjective or non-urgent tasks remained challenging. Findings demonstrate the feasibility of AI-assisted task verification in dementia care. Context-aware AI prompts and caregiver feedback can enhance task monitoring, reduce caregiver stress, and strengthen PLwD support. Future work should focus on real-world validation and scalability."
  },
  {
    "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
    "url": "http://arxiv.org/abs/2508.18265v1",
    "arxiv_id": "2508.18265v1",
    "authors": [
      "Weiyun Wang",
      "Zhangwei Gao",
      "Lixin Gu",
      "Hengjun Pu",
      "Long Cui",
      "Xingguang Wei",
      "Zhaoyang Liu",
      "Linglin Jing",
      "Shenglong Ye",
      "Jie Shao",
      "Zhaokai Wang",
      "Zhe Chen",
      "Hongjie Zhang",
      "Ganlin Yang",
      "Haomin Wang",
      "Qi Wei",
      "Jinhui Yin",
      "Wenhao Li",
      "Erfei Cui",
      "Guanzhou Chen",
      "Zichen Ding",
      "Changyao Tian",
      "Zhenyu Wu",
      "Jingjing Xie",
      "Zehao Li",
      "Bowen Yang",
      "Yuchen Duan",
      "Xuehui Wang",
      "Songze Li",
      "Xiangyu Zhao",
      "Haodong Duan",
      "Nianchen Deng",
      "Bin Fu",
      "Yinan He",
      "Yi Wang",
      "Conghui He",
      "Botian Shi",
      "Junjun He",
      "Yingtong Xiong",
      "Han Lv",
      "Lijun Wu",
      "Wenqi Shao",
      "Kaipeng Zhang",
      "Huipeng Deng",
      "Biqing Qi",
      "Jiaye Ge",
      "Qipeng Guo",
      "Wenwei Zhang",
      "Wanli Ouyang",
      "Limin Wang",
      "Min Dou",
      "Xizhou Zhu",
      "Tong Lu",
      "Dahua Lin",
      "Jifeng Dai",
      "Bowen Zhou",
      "Weijie Su",
      "Kai Chen",
      "Yu Qiao",
      "Wenhai Wang",
      "Gen Luo"
    ],
    "published": "2025-08-25T17:58:17+00:00",
    "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05$\\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released."
  },
  {
    "title": "Addressing Dipole Tension via Clustering in $\u039b$CDM and beyond",
    "url": "http://arxiv.org/abs/2508.18259v1",
    "arxiv_id": "2508.18259v1",
    "authors": [
      "Arefeh Daei Rasouli",
      "Haniyeh S. Tadayyoni",
      "Shant Baghram",
      "Sohrab Rahvar"
    ],
    "published": "2025-08-25T17:51:57+00:00",
    "summary": "The dipole in the angular distribution of the cosmic microwave background (CMB) is typically attributed to the Doppler effect and our motion relative to the CMB rest frame. It is expected that observations of large-scale structures would also exhibit a related kinematic dipole, helping to confirm the kinematic origin of the CMB dipole. However, numerous studies of the large-scale structure dipole have shown significant discrepancies with predictions based on the CMB. In this work, we investigate how considering the clustering dipole affects the cosmic large-scale structure distribution dipole using the National Radio Astronomy Observatory (NRAO) Very Large Array (VLA) Sky Survey (NVSS) and examine the nonlinear regime to calculate the correlation between the clustering dipole and the kinematic dipole. We also determine whether these outcomes help reconcile previous measurements of the NVSS dipole with predictions based on the CMB. Additionally, we explore a model in which the distribution of matter on large scales might be intrinsically anisotropic. Using the remnant discrepancy between the observed and predicted dipole, we derive an upper limit for the amplitude of intrinsic anisotropy and calculate the clustering for this model. Furthermore, we investigate these results within the framework of modified gravity theories, specifically the $f(R)$ gravity model. By examining this model, it is possible to gain a better understanding of how the cosmic dark energy component affects the dipole. Finally, by comparing these two models, we can see that the $f(R)$ model predicts a higher clustering dipole compared to the standard cosmological model, which explains the discrepancy between the kinetic and clustering dipoles and leads to a higher dipole amplitude."
  },
  {
    "title": "Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems",
    "url": "http://arxiv.org/abs/2508.16574v1",
    "arxiv_id": "2508.16574v1",
    "authors": [
      "Yizhi Wang",
      "Degang Xu",
      "Yongfang Xie",
      "Shuzhong Tan",
      "Xianan Zhou",
      "Peng Chen"
    ],
    "published": "2025-08-22T17:57:56+00:00",
    "summary": "This paper presents a hierarchical decision-making framework for autonomous navigation in four-wheel independent steering and driving (4WISD) systems. The proposed approach integrates deep reinforcement learning (DRL) for high-level navigation with fuzzy logic for low-level control to ensure both task performance and physical feasibility. The DRL agent generates global motion commands, while the fuzzy logic controller enforces kinematic constraints to prevent mechanical strain and wheel slippage. Simulation experiments demonstrate that the proposed framework outperforms traditional navigation methods, offering enhanced training efficiency and stability and mitigating erratic behaviors compared to purely DRL-based solutions. Real-world validations further confirm the framework's ability to navigate safely and effectively in dynamic industrial settings. Overall, this work provides a scalable and reliable solution for deploying 4WISD mobile robots in complex, real-world scenarios."
  },
  {
    "title": "RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs",
    "url": "http://arxiv.org/abs/2508.16546v1",
    "arxiv_id": "2508.16546v1",
    "authors": [
      "Hangzhan Jin",
      "Sicheng Lv",
      "Sifan Wu",
      "Mohammad Hamdaqa"
    ],
    "published": "2025-08-22T17:10:37+00:00",
    "summary": "Training large language models (LLMs) from scratch is increasingly impractical, making post-training methods such as supervised fine-tuning (SFT) and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern practice. Using an out-of-distribution (OOD) variant of the 24-point card game and new spectrum-based diagnostics, we revisit how these two stages reshape model representation and OOD performance. Our key findings are- (1) RL-FT can restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to 15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and a clear distribution shift, RL-FT cannot fully recover OOD performance. (2) Direction shifts of singular vectors matter more than singular value magnitudes. These shifts concentrate on directions linked to the largest and smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and shallow recovery is effective: restoring singular vector directions for the top 20% of values or first 25% of layers recovers 70-80% of OOD performance. (4) Stronger SFT checkpoints enable better recovery by RL, while overfitted ones resist restoration. These results reconcile prior reports of RL superior OOD performance: RL primarily counteracts SFT-induced directional drift rather than finding new solutions. Our spectrum-aware analysis highlights inexpensive recovery knobs low-rank UV merging and shallow-layer resets that practitioners can use before costly RL fine-tuning."
  },
  {
    "title": "Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning",
    "url": "http://arxiv.org/abs/2508.16524v1",
    "arxiv_id": "2508.16524v1",
    "authors": [
      "Xuan Zhang",
      "Zhijian Zhou",
      "Weidi Xu",
      "Yanting Miao",
      "Chao Qu",
      "Yuan Qi"
    ],
    "published": "2025-08-22T16:47:08+00:00",
    "summary": "Enabling neural networks to learn complex logical constraints and fulfill symbolic reasoning is a critical challenge. Bridging this gap often requires guiding the neural network's output distribution to move closer to the symbolic constraints. While diffusion models have shown remarkable generative capability across various domains, we employ the powerful architecture to perform neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline adopts a two-stage training strategy: the first stage focuses on cultivating basic reasoning abilities, while the second emphasizes systematic learning of logical constraints. To impose hard constraints on neural outputs in the second stage, we formulate the diffusion reasoner as a Markov decision process and innovatively fine-tune it with an improved proximal policy optimization algorithm. We utilize a rule-based reward signal derived from the logical consistency of neural outputs and adopt a flexible strategy to optimize the diffusion reasoner's policy. We evaluate our methodology on some classical symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and preference learning. Experimental results demonstrate that our approach achieves outstanding accuracy and logical consistency among neural networks."
  },
  {
    "title": "Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation",
    "url": "http://arxiv.org/abs/2508.16521v1",
    "arxiv_id": "2508.16521v1",
    "authors": [
      "Zhijian Zhou",
      "Junyi An",
      "Zongkai Liu",
      "Yunfei Shi",
      "Xuan Zhang",
      "Fenglei Cao",
      "Chao Qu",
      "Yuan Qi"
    ],
    "published": "2025-08-22T16:44:55+00:00",
    "summary": "Generating physically realistic 3D molecular structures remains a core challenge in molecular generative modeling. While diffusion models equipped with equivariant neural networks have made progress in capturing molecular geometries, they often struggle to produce equilibrium structures that adhere to physical principles such as force field consistency. To bridge this gap, we propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework that extends Denoising Diffusion Policy Optimization to 3D molecular generation. RLPF formulates the task as a Markov decision process and applies proximal policy optimization to fine-tune equivariant diffusion models. Crucially, RLPF introduces reward functions derived from force-field evaluations, providing direct physical feedback to guide the generation toward energetically stable and physically meaningful structures. Experiments on the QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves molecular stability compared to existing methods. These results highlight the value of incorporating physics-based feedback into generative modeling. The code is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion."
  },
  {
    "title": "Comparative Analysis of UAV Path Planning Algorithms for Efficient Navigation in Urban 3D Environments",
    "url": "http://arxiv.org/abs/2508.16515v1",
    "arxiv_id": "2508.16515v1",
    "authors": [
      "Hichem Cheriet",
      "Khellat Kihel Badra",
      "Chouraqui Samira"
    ],
    "published": "2025-08-22T16:37:59+00:00",
    "summary": "The most crucial challenges for UAVs are planning paths and avoiding obstacles in their way. In recent years, a wide variety of path-planning algorithms have been developed. These algorithms have successfully solved path-planning problems; however, they suffer from multiple challenges and limitations. To test the effectiveness and efficiency of three widely used algorithms, namely A*, RRT*, and Particle Swarm Optimization (PSO), this paper conducts extensive experiments in 3D urban city environments cluttered with obstacles. Three experiments were designed with two scenarios each to test the aforementioned algorithms. These experiments consider different city map sizes, different altitudes, and varying obstacle densities and sizes in the environment. According to the experimental results, the A* algorithm outperforms the others in both computation efficiency and path quality. PSO is especially suitable for tight turns and dense environments, and RRT* offers a balance and works well across all experiments due to its randomized approach to finding solutions."
  },
  {
    "title": "Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO",
    "url": "http://arxiv.org/abs/2508.15766v1",
    "arxiv_id": "2508.15766v1",
    "authors": [
      "Jaeha Lee",
      "Gio Huh",
      "Ning Su",
      "Tony Yue YU"
    ],
    "published": "2025-08-21T17:58:50+00:00",
    "summary": "Recent efforts have extended the capabilities of transformers in logical reasoning and symbolic computations. In this work, we investigate their capacity for non-linear latent pattern discovery in the context of functional decomposition, focusing on the challenging algebraic task of multivariate polynomial decomposition. This problem, with widespread applications in science and engineering, is proved to be NP-hard, and demands both precision and insight. Our contributions are threefold: First, we develop a synthetic data generation pipeline providing fine-grained control over problem complexity. Second, we train transformer models via supervised learning and evaluate them across four key dimensions involving scaling behavior and generalizability. Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a rank-aware reinforcement learning method suitable for hard algebraic problems. Finetuning with BGRPO improves accuracy while reducing beam width by up to half, resulting in approximately 75% lower inference compute. Additionally, our model demonstrates competitive performance in polynomial simplification, outperforming Mathematica in various cases."
  },
  {
    "title": "Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space",
    "url": "http://arxiv.org/abs/2508.15764v1",
    "arxiv_id": "2508.15764v1",
    "authors": [
      "Kiarash Kazari",
      "Ezzeldin Shereen",
      "Gy\u00f6rgy D\u00e1n"
    ],
    "published": "2025-08-21T17:58:36+00:00",
    "summary": "We address the problem of detecting adversarial attacks against cooperative multi-agent reinforcement learning with continuous action space. We propose a decentralized detector that relies solely on the local observations of the agents and makes use of a statistical characterization of the normal behavior of observable agents. The proposed detector utilizes deep neural networks to approximate the normal behavior of agents as parametric multivariate Gaussian distributions. Based on the predicted density functions, we define a normality score and provide a characterization of its mean and variance. This characterization allows us to employ a two-sided CUSUM procedure for detecting deviations of the normality score from its mean, serving as a detector of anomalous behavior in real-time. We evaluate our scheme on various multi-agent PettingZoo benchmarks against different state-of-the-art attack methods, and our results demonstrate the effectiveness of our method in detecting impactful adversarial attacks. Particularly, it outperforms the discrete counterpart by achieving AUC-ROC scores of over 0.95 against the most impactful attacks in all evaluated environments."
  },
  {
    "title": "Intern-S1: A Scientific Multimodal Foundation Model",
    "url": "http://arxiv.org/abs/2508.15763v1",
    "arxiv_id": "2508.15763v1",
    "authors": [
      "Lei Bai",
      "Zhongrui Cai",
      "Maosong Cao",
      "Weihan Cao",
      "Chiyu Chen",
      "Haojiong Chen",
      "Kai Chen",
      "Pengcheng Chen",
      "Ying Chen",
      "Yongkang Chen",
      "Yu Cheng",
      "Yu Cheng",
      "Pei Chu",
      "Tao Chu",
      "Erfei Cui",
      "Ganqu Cui",
      "Long Cui",
      "Ziyun Cui",
      "Nianchen Deng",
      "Ning Ding",
      "Nanqin Dong",
      "Peijie Dong",
      "Shihan Dou",
      "Sinan Du",
      "Haodong Duan",
      "Caihua Fan",
      "Ben Gao",
      "Changjiang Gao",
      "Jianfei Gao",
      "Songyang Gao",
      "Yang Gao",
      "Zhangwei Gao",
      "Jiaye Ge",
      "Qiming Ge",
      "Lixin Gu",
      "Yuzhe Gu",
      "Aijia Guo",
      "Qipeng Guo",
      "Xu Guo",
      "Conghui He",
      "Junjun He",
      "Yili Hong",
      "Siyuan Hou",
      "Caiyu Hu",
      "Hanglei Hu",
      "Jucheng Hu",
      "Ming Hu",
      "Zhouqi Hua",
      "Haian Huang",
      "Junhao Huang",
      "Xu Huang",
      "Zixian Huang",
      "Zhe Jiang",
      "Lingkai Kong",
      "Linyang Li",
      "Peiji Li",
      "Pengze Li",
      "Shuaibin Li",
      "Tianbin Li",
      "Wei Li",
      "Yuqiang Li",
      "Dahua Lin",
      "Junyao Lin",
      "Tianyi Lin",
      "Zhishan Lin",
      "Hongwei Liu",
      "Jiangning Liu",
      "Jiyao Liu",
      "Junnan Liu",
      "Kai Liu",
      "Kaiwen Liu",
      "Kuikun Liu",
      "Shichun Liu",
      "Shudong Liu",
      "Wei Liu",
      "Xinyao Liu",
      "Yuhong Liu",
      "Zhan Liu",
      "Yinquan Lu",
      "Haijun Lv",
      "Hongxia Lv",
      "Huijie Lv",
      "Qidang Lv",
      "Ying Lv",
      "Chengqi Lyu",
      "Chenglong Ma",
      "Jianpeng Ma",
      "Ren Ma",
      "Runmin Ma",
      "Runyuan Ma",
      "Xinzhu Ma",
      "Yichuan Ma",
      "Zihan Ma",
      "Sixuan Mi",
      "Junzhi Ning",
      "Wenchang Ning",
      "Xinle Pang",
      "Jiahui Peng",
      "Runyu Peng",
      "Yu Qiao",
      "Jiantao Qiu",
      "Xiaoye Qu",
      "Yuan Qu",
      "Yuchen Ren",
      "Fukai Shang",
      "Wenqi Shao",
      "Junhao Shen",
      "Shuaike Shen",
      "Chunfeng Song",
      "Demin Song",
      "Diping Song",
      "Chenlin Su",
      "Weijie Su",
      "Weigao Sun",
      "Yu Sun",
      "Qian Tan",
      "Cheng Tang",
      "Huanze Tang",
      "Kexian Tang",
      "Shixiang Tang",
      "Jian Tong",
      "Aoran Wang",
      "Bin Wang",
      "Dong Wang",
      "Lintao Wang",
      "Rui Wang",
      "Weiyun Wang",
      "Wenhai Wang",
      "Yi Wang",
      "Ziyi Wang",
      "Ling-I Wu",
      "Wen Wu",
      "Yue Wu",
      "Zijian Wu",
      "Linchen Xiao",
      "Shuhao Xing",
      "Chao Xu",
      "Huihui Xu",
      "Jun Xu",
      "Ruiliang Xu",
      "Wanghan Xu",
      "GanLin Yang",
      "Yuming Yang",
      "Haochen Ye",
      "Jin Ye",
      "Shenglong Ye",
      "Jia Yu",
      "Jiashuo Yu",
      "Jing Yu",
      "Fei Yuan",
      "Bo Zhang",
      "Chao Zhang",
      "Chen Zhang",
      "Hongjie Zhang",
      "Jin Zhang",
      "Qiaosheng Zhang",
      "Qiuyinzhe Zhang",
      "Songyang Zhang",
      "Taolin Zhang",
      "Wenlong Zhang",
      "Wenwei Zhang",
      "Yechen Zhang",
      "Ziyang Zhang",
      "Haiteng Zhao",
      "Qian Zhao",
      "Xiangyu Zhao",
      "Xiangyu Zhao",
      "Bowen Zhou",
      "Dongzhan Zhou",
      "Peiheng Zhou",
      "Yuhao Zhou",
      "Yunhua Zhou",
      "Dongsheng Zhu",
      "Lin Zhu",
      "Yicheng Zou"
    ],
    "published": "2025-08-21T17:58:00+00:00",
    "summary": "In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1."
  },
  {
    "title": "Neural Robot Dynamics",
    "url": "http://arxiv.org/abs/2508.15755v1",
    "arxiv_id": "2508.15755v1",
    "authors": [
      "Jie Xu",
      "Eric Heiden",
      "Iretiayo Akinola",
      "Dieter Fox",
      "Miles Macklin",
      "Yashraj Narang"
    ],
    "published": "2025-08-21T17:54:41+00:00",
    "summary": "Accurate and efficient simulation of modern robots remains challenging due to their high degrees of freedom and intricate mechanisms. Neural simulators have emerged as a promising alternative to traditional analytical simulators, capable of efficiently predicting complex dynamics and adapting to real-world data; however, existing neural simulators typically require application-specific training and fail to generalize to novel tasks and/or environments, primarily due to inadequate representations of the global state. In this work, we address the problem of learning generalizable neural simulators for robots that are structured as articulated rigid bodies. We propose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models for predicting future states for articulated rigid bodies under contact constraints. NeRD uniquely replaces the low-level dynamics and contact solvers in an analytical simulator and employs a robot-centric and spatially-invariant simulation state representation. We integrate the learned NeRD models as an interchangeable backend solver within a state-of-the-art robotics simulator. We conduct extensive experiments to show that the NeRD simulators are stable and accurate over a thousand simulation steps; generalize across tasks and environment configurations; enable policy learning exclusively in a neural engine; and, unlike most classical simulators, can be fine-tuned from real-world data to bridge the gap between simulation and reality."
  },
  {
    "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning",
    "url": "http://arxiv.org/abs/2508.15746v1",
    "arxiv_id": "2508.15746v1",
    "authors": [
      "Qiaoyu Zheng",
      "Yuze Sun",
      "Chaoyi Wu",
      "Weike Zhao",
      "Pengcheng Qiu",
      "Yongguo Yu",
      "Kun Sun",
      "Yanfeng Wang",
      "Ya Zhang",
      "Weidi Xie"
    ],
    "published": "2025-08-21T17:42:47+00:00",
    "summary": "Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.   Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch's diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See https://github.com/MAGIC-AI4Med/Deep-DxSearch."
  },
  {
    "title": "Virtual Community: An Open World for Humans, Robots, and Society",
    "url": "http://arxiv.org/abs/2508.14893v1",
    "arxiv_id": "2508.14893v1",
    "authors": [
      "Qinhong Zhou",
      "Hongxin Zhang",
      "Xiangye Lin",
      "Zheyuan Zhang",
      "Yutian Chen",
      "Wenjun Liu",
      "Zunzhe Zhang",
      "Sunli Chen",
      "Lixing Fang",
      "Qiushi Lyu",
      "Xinyu Sun",
      "Jincheng Yang",
      "Zeyuan Wang",
      "Bao Chi Dang",
      "Zhehuan Chen",
      "Daksha Ladia",
      "Jiageng Liu",
      "Chuang Gan"
    ],
    "published": "2025-08-20T17:59:32+00:00",
    "summary": "The rapid progress in AI and Robotics may lead to a profound societal transformation, as humans and robots begin to coexist within shared communities, introducing both opportunities and challenges. To explore this future, we present Virtual Community-an open-world platform for humans, robots, and society-built on a universal physics engine and grounded in real-world 3D scenes. With Virtual Community, we aim to study embodied social intelligence at scale: 1) How robots can intelligently cooperate or compete; 2) How humans develop social relations and build community; 3) More importantly, how intelligent robots and humans can co-exist in an open world. To support these, Virtual Community features: 1) An open-source multi-agent physics simulator that supports robots, humans, and their interactions within a society; 2) A large-scale, real-world aligned community generation pipeline, including vast outdoor space, diverse indoor scenes, and a community of grounded agents with rich characters and appearances. Leveraging Virtual Community, we propose two novel challenges. The Community Planning Challenge evaluates multi-agent reasoning and planning ability in open-world settings, such as cooperating to help agents with daily activities and efficiently connecting other agents. The Community Robot Challenge requires multiple heterogeneous robots to collaborate in solving complex open-world tasks. We evaluate various baselines on these tasks and demonstrate the challenges in both high-level open-world task planning and low-level cooperation controls. We hope that Virtual Community will unlock further study of human-robot coexistence within open-world environments."
  },
  {
    "title": "GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects",
    "url": "http://arxiv.org/abs/2508.14891v1",
    "arxiv_id": "2508.14891v1",
    "authors": [
      "Licheng Shen",
      "Saining Zhang",
      "Honghan Li",
      "Peilin Yang",
      "Zihao Huang",
      "Zongzheng Zhang",
      "Hao Zhao"
    ],
    "published": "2025-08-20T17:59:08+00:00",
    "summary": "Reconstructing articulated objects is essential for building digital twins of interactive environments. However, prior methods typically decouple geometry and motion by first reconstructing object shape in distinct states and then estimating articulation through post-hoc alignment. This separation complicates the reconstruction pipeline and restricts scalability, especially for objects with complex, multi-part articulation. We introduce a unified representation that jointly models geometry and motion using articulated 3D Gaussians. This formulation improves robustness in motion decomposition and supports articulated objects with up to 20 parts, significantly outperforming prior approaches that often struggle beyond 2--3 parts due to brittle initialization. To systematically assess scalability and generalization, we propose MPArt-90, a new benchmark consisting of 90 articulated objects across 20 categories, each with diverse part counts and motion configurations. Extensive experiments show that our method consistently achieves superior accuracy in part-level geometry reconstruction and motion estimation across a broad range of object types. We further demonstrate applicability to downstream tasks such as robotic simulation and human-scene interaction modeling, highlighting the potential of unified articulated representations in scalable physical modeling."
  },
  {
    "title": "Novel Limits on Dark Photon Mixing from Radiation Safety",
    "url": "http://arxiv.org/abs/2508.14885v1",
    "arxiv_id": "2508.14885v1",
    "authors": [
      "Wen Yin"
    ],
    "published": "2025-08-20T17:55:34+00:00",
    "summary": "I propose a novel laboratory search for dark photons based on radiation-safety monitoring at synchrotron radiation facilities, including NanoTerasu, SPring-8, KEK-PF, and ESRF. Dark photons can be produced parasitically in undulators or via photon-mirror interactions, and subsequently traverse optical systems and shielding. Taking into account quantum effects and the internal structure of undulators, mirrors, and detectors, I show that even a simple Geiger-M\\\"uller counter, routinely used for radiation-safety monitoring, can detect such dark photons outside the shielding and set competitive limits on the kinetic mixing parameter down to $\\chi \\lesssim 5\\times 10^{-6}$ in the eV mass range, providing some of the strongest bounds among laboratory searches. Because radiation safety is strictly regulated, the resulting limits can be regarded as robust and realistic constraints."
  },
  {
    "title": "Deep Reinforcement Learning Based Routing for Heterogeneous Multi-Hop Wireless Networks",
    "url": "http://arxiv.org/abs/2508.14884v1",
    "arxiv_id": "2508.14884v1",
    "authors": [
      "Brian Kim",
      "Justin H. Kong",
      "Terrence J. Moore",
      "Fikadu T. Dagefu"
    ],
    "published": "2025-08-20T17:54:43+00:00",
    "summary": "Routing in multi-hop wireless networks is a complex problem, especially in heterogeneous networks where multiple wireless communication technologies coexist. Reinforcement learning (RL) methods, such as Q-learning, have been introduced for decentralized routing by allowing nodes to make decisions based on local observations. However, Q-learning suffers from scalability issues and poor generalization due to the difficulty in managing the Q-table in large or dynamic network topologies, especially in heterogeneous networks (HetNets) with diverse channel characteristics. Thus, in this paper, we propose a novel deep Q-network (DQN)-based routing framework for heterogeneous multi-hop wireless networks to maximize the end-to-end rate of the route by improving scalability and adaptability, where each node uses a deep neural network (DNN) to estimate the Q-values and jointly select the next-hop relay and a communication technology for transmission. To achieve better performance with the DNN, selecting which nodes to exchange information is critical, as it not only defines the state and action spaces but also determines the input to the DNN. To this end, we propose neighbor node selection strategies based on channel gain and rate between nodes rather than a simple distance-based approach for an improved set of states and actions for DQN-based routing. During training, the model experiences diverse network topologies to ensure generalization and robustness, and simulation results show that the proposed neighbor node selection outperforms simple distance-based selection. Further, we observe that the DQN-based approach outperforms various benchmark schemes and performs comparably to the optimal approach."
  },
  {
    "title": "Compute-Optimal Scaling for Value-Based Deep RL",
    "url": "http://arxiv.org/abs/2508.14881v1",
    "arxiv_id": "2508.14881v1",
    "authors": [
      "Preston Fu",
      "Oleh Rybkin",
      "Zhiyuan Zhou",
      "Michal Nauman",
      "Pieter Abbeel",
      "Sergey Levine",
      "Aviral Kumar"
    ],
    "published": "2025-08-20T17:54:21+00:00",
    "summary": "As models grow larger and training them becomes expensive, it becomes increasingly important to scale training recipes not just to larger models and more data, but to do so in a compute-optimal manner that extracts maximal performance per unit of compute. While such scaling has been well studied for language modeling, reinforcement learning (RL) has received less attention in this regard. In this paper, we investigate compute scaling for online, value-based deep RL. These methods present two primary axes for compute allocation: model capacity and the update-to-data (UTD) ratio. Given a fixed compute budget, we ask: how should resources be partitioned across these axes to maximize sample efficiency? Our analysis reveals a nuanced interplay between model size, batch size, and UTD. In particular, we identify a phenomenon we call TD-overfitting: increasing the batch quickly harms Q-function accuracy for small models, but this effect is absent in large models, enabling effective use of large batch size at scale. We provide a mental model for understanding this phenomenon and build guidelines for choosing batch size and UTD to optimize compute usage. Our findings provide a grounded starting point for compute-optimal scaling in deep RL, mirroring studies in supervised learning but adapted to TD learning."
  },
  {
    "title": "Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation",
    "url": "http://arxiv.org/abs/2508.14042v1",
    "arxiv_id": "2508.14042v1",
    "authors": [
      "Zhuoling Li",
      "Xiaoyang Wu",
      "Zhenhua Xu",
      "Hengshuang Zhao"
    ],
    "published": "2025-08-19T17:59:59+00:00",
    "summary": "Realizing generalizable dynamic object manipulation is important for enhancing manufacturing efficiency, as it eliminates specialized engineering for various scenarios. To this end, imitation learning emerges as a promising paradigm, leveraging expert demonstrations to teach a policy manipulation skills. Although the generalization of an imitation learning policy can be improved by increasing demonstrations, demonstration collection is labor-intensive. To address this problem, this paper investigates whether strong generalization in dynamic object manipulation is achievable with only a few demonstrations. Specifically, we develop an entropy-based theoretical framework to quantify the optimization of imitation learning. Based on this framework, we propose a system named Generalizable Entropy-based Manipulation (GEM). Extensive experiments in simulated and real tasks demonstrate that GEM can generalize across diverse environment backgrounds, robot embodiments, motion dynamics, and object geometries. Notably, GEM has been deployed in a real canteen for tableware collection. Without any in-scene demonstration, it achieves a success rate of over 97% across more than 10,000 operations."
  },
  {
    "title": "ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents",
    "url": "http://arxiv.org/abs/2508.14040v1",
    "arxiv_id": "2508.14040v1",
    "authors": [
      "Hanyu Lai",
      "Xiao Liu",
      "Yanxiao Zhao",
      "Han Xu",
      "Hanchen Zhang",
      "Bohao Jing",
      "Yanyu Ren",
      "Shuntian Yao",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "published": "2025-08-19T17:59:45+00:00",
    "summary": "We introduce ComputerRL, a framework for autonomous desktop intelligence that enables agents to operate complex digital workspaces skillfully. ComputerRL features the API-GUI paradigm, which unifies programmatic API calls and direct GUI interaction to address the inherent mismatch between machine agents and human-centric desktop environments. Scaling end-to-end RL training is crucial for improvement and generalization across diverse desktop tasks, yet remains challenging due to environmental inefficiency and instability in extended training. To support scalable and robust training, we develop a distributed RL infrastructure capable of orchestrating thousands of parallel virtual desktop environments to accelerate large-scale online RL. Furthermore, we propose Entropulse, a training strategy that alternates reinforcement learning with supervised fine-tuning, effectively mitigating entropy collapse during extended training runs. We employ ComputerRL on open models GLM-4-9B-0414 and Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%, demonstrating significant improvements for general agents in desktop automation. The algorithm and framework are adopted in building AutoGLM (Liu et al., 2024a)"
  },
  {
    "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation",
    "url": "http://arxiv.org/abs/2508.14031v1",
    "arxiv_id": "2508.14031v1",
    "authors": [
      "Dongyoon Hahm",
      "Taywon Min",
      "Woogyeol Jin",
      "Kimin Lee"
    ],
    "published": "2025-08-19T17:53:35+00:00",
    "summary": "Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature."
  },
  {
    "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR",
    "url": "http://arxiv.org/abs/2508.14029v1",
    "arxiv_id": "2508.14029v1",
    "authors": [
      "Xiao Liang",
      "Zhongzhi Li",
      "Yeyun Gong",
      "Yelong Shen",
      "Ying Nian Wu",
      "Zhijiang Guo",
      "Weizhu Chen"
    ],
    "published": "2025-08-19T17:42:45+00:00",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy's generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy's correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS."
  },
  {
    "title": "Learning from Preferences and Mixed Demonstrations in General Settings",
    "url": "http://arxiv.org/abs/2508.14027v1",
    "arxiv_id": "2508.14027v1",
    "authors": [
      "Jason R Brown",
      "Carl Henrik Ek",
      "Robert D Mullins"
    ],
    "published": "2025-08-19T17:37:35+00:00",
    "summary": "Reinforcement learning is a general method for learning in sequential settings, but it can often be difficult to specify a good reward function when the task is complex. In these cases, preference feedback or expert demonstrations can be used instead. However, existing approaches utilising both together are often ad-hoc, rely on domain-specific properties, or won't scale. We develop a new framing for learning from human data, \\emph{reward-rational partial orderings over observations}, designed to be flexible and scalable. Based on this we introduce a practical algorithm, LEOPARD: Learning Estimated Objectives from Preferences And Ranked Demonstrations. LEOPARD can learn from a broad range of data, including negative demonstrations, to efficiently learn reward functions across a wide range of domains. We find that when a limited amount of preference and demonstration feedback is available, LEOPARD outperforms existing baselines by a significant margin. Furthermore, we use LEOPARD to investigate learning from many types of feedback compared to just a single one, and find that combining feedback types is often beneficial."
  },
  {
    "title": "IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion",
    "url": "http://arxiv.org/abs/2508.13153v1",
    "arxiv_id": "2508.13153v1",
    "authors": [
      "Wenhao Hu",
      "Zesheng Li",
      "Haonan Zhou",
      "Liu Liu",
      "Xuexiang Wen",
      "Zhizhong Su",
      "Xi Li",
      "Gaoang Wang"
    ],
    "published": "2025-08-18T17:59:47+00:00",
    "summary": "Reconstructing complete and interactive 3D scenes remains a fundamental challenge in computer vision and robotics, particularly due to persistent object occlusions and limited sensor coverage. Multiview observations from a single scene scan often fail to capture the full structural details. Existing approaches typically rely on multi stage pipelines, such as segmentation, background completion, and inpainting or require per-object dense scanning, both of which are error-prone, and not easily scalable. We propose IGFuse, a novel framework that reconstructs interactive Gaussian scene by fusing observations from multiple scans, where natural object rearrangement between captures reveal previously occluded regions. Our method constructs segmentation aware Gaussian fields and enforces bi-directional photometric and semantic consistency across scans. To handle spatial misalignments, we introduce a pseudo-intermediate scene state for unified alignment, alongside collaborative co-pruning strategies to refine geometry. IGFuse enables high fidelity rendering and object level scene manipulation without dense observations or complex pipelines. Extensive experiments validate the framework's strong generalization to novel scene configurations, demonstrating its effectiveness for real world 3D reconstruction and real-to-simulation transfer. Our project page is available online."
  },
  {
    "title": "Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors",
    "url": "http://arxiv.org/abs/2508.13151v1",
    "arxiv_id": "2508.13151v1",
    "authors": [
      "Yuying Zhang",
      "Joni Pajarinen"
    ],
    "published": "2025-08-18T17:58:57+00:00",
    "summary": "Mobile manipulation in dynamic environments is challenging due to movable obstacles blocking the robot's path. Traditional methods, which treat navigation and manipulation as separate tasks, often fail in such 'manipulate-to-navigate' scenarios, as obstacles must be removed before navigation. In these cases, active interaction with the environment is required to clear obstacles while ensuring sufficient space for movement. To address the manipulate-to-navigate problem, we propose a reinforcement learning-based approach for learning manipulation actions that facilitate subsequent navigation. Our method combines manipulability priors to focus the robot on high manipulability body positions with affordance maps for selecting high-quality manipulation actions. By focusing on feasible and meaningful actions, our approach reduces unnecessary exploration and allows the robot to learn manipulation strategies more effectively. We present two new manipulate-to-navigate simulation tasks called Reach and Door with the Boston Dynamics Spot robot. The first task tests whether the robot can select a good hand position in the target area such that the robot base can move effectively forward while keeping the end effector position fixed. The second task requires the robot to move a door aside in order to clear the navigation path. Both of these tasks need first manipulation and then navigating the base forward. Results show that our method allows a robot to effectively interact with and traverse dynamic environments. Finally, we transfer the learned policy to a real Boston Dynamics Spot robot, which successfully performs the Reach task."
  },
  {
    "title": "MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models",
    "url": "http://arxiv.org/abs/2508.13148v1",
    "arxiv_id": "2508.13148v1",
    "authors": [
      "Haoyu He",
      "Katrin Renz",
      "Yong Cao",
      "Andreas Geiger"
    ],
    "published": "2025-08-18T17:58:13+00:00",
    "summary": "Diffusion language models, as a promising alternative to traditional autoregressive (AR) models, enable faster generation and richer conditioning on bidirectional context. However, they suffer from a key discrepancy between training and inference: during inference, MDLMs progressively reveal the structure of the generated sequence by producing fewer and fewer masked tokens, whereas this structure is ignored in training as tokens are masked at random. Although this discrepancy between training and inference can lead to suboptimal performance, it has been largely overlooked by previous works, leaving closing this gap between the two stages an open problem. To address this, we frame the problem of learning effective denoising trajectories as a sequential decision-making problem and use the resulting framework to apply reinforcement learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to exploit the Markov property diffusion possesses and explicitly train the model under the same progressive refining schedule used at inference. MDPO matches the performance of the previous state-of-the-art (SOTA) method with 60x fewer gradient updates, while achieving average improvements of 9.6% on MATH500 and 54.2% on Countdown over SOTA when trained within the same number of weight updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in inference replacement to overcome the limitation that the model cannot refine tokens flexibly. This simple yet effective training-free strategy, what we refer to as RCR, consistently improves performance and yields additional gains when combined with MDPO. Our findings establish great potential for investigating the discrepancy between pre-training and inference of MDLMs. Code: https://github.com/autonomousvision/mdpo. Project Page: https://cli212.github.io/MDPO/."
  },
  {
    "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
    "url": "http://arxiv.org/abs/2508.13142v1",
    "arxiv_id": "2508.13142v1",
    "authors": [
      "Zhongang Cai",
      "Yubo Wang",
      "Qingping Sun",
      "Ruisi Wang",
      "Chenyang Gu",
      "Wanqi Yin",
      "Zhiqian Lin",
      "Zhitao Yang",
      "Chen Wei",
      "Xuanke Shi",
      "Kewang Deng",
      "Xiaoyang Han",
      "Zukai Chen",
      "Jiaqi Li",
      "Xiangyu Fan",
      "Hanming Deng",
      "Lewei Lu",
      "Bo Li",
      "Ziwei Liu",
      "Quan Wang",
      "Dahua Lin",
      "Lei Yang"
    ],
    "published": "2025-08-18T17:55:17+00:00",
    "summary": "Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path toward spatial intelligence. First, we propose a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and discuss the challenges in ensuring fair evaluation. We then evaluate state-of-the-art proprietary and open-source models on eight key benchmarks, at a cost exceeding one billion total tokens. Our empirical study reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2) still falls short of human performance across a broad spectrum of tasks. Moreover, we (3) identify the more challenging spatial intelligence problems for multi-modal models, and (4) proprietary models do not exhibit a decisive advantage when facing the most difficult problems. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans yet fail even the most advanced multi-modal models."
  },
  {
    "title": "Improving Detection of Watermarked Language Models",
    "url": "http://arxiv.org/abs/2508.13131v1",
    "arxiv_id": "2508.13131v1",
    "authors": [
      "Dara Bahri",
      "John Wieting"
    ],
    "published": "2025-08-18T17:43:06+00:00",
    "summary": "Watermarking has recently emerged as an effective strategy for detecting the generations of large language models (LLMs). The strength of a watermark typically depends strongly on the entropy afforded by the language model and the set of input prompts. However, entropy can be quite limited in practice, especially for models that are post-trained, for example via instruction tuning or reinforcement learning from human feedback (RLHF), which makes detection based on watermarking alone challenging. In this work, we investigate whether detection can be improved by combining watermark detectors with non-watermark ones. We explore a number of hybrid schemes that combine the two, observing performance gains over either class of detector under a wide range of experimental conditions."
  },
  {
    "title": "Thyme: Think Beyond Images",
    "url": "http://arxiv.org/abs/2508.11630v1",
    "arxiv_id": "2508.11630v1",
    "authors": [
      "Yi-Fan Zhang",
      "Xingyu Lu",
      "Shukang Yin",
      "Chaoyou Fu",
      "Wei Chen",
      "Xiao Hu",
      "Bin Wen",
      "Kaiyu Jiang",
      "Changyi Liu",
      "Tianke Zhang",
      "Haonan Fan",
      "Kaibing Chen",
      "Jiankang Chen",
      "Haojie Ding",
      "Kaiyu Tang",
      "Zhang Zhang",
      "Liang Wang",
      "Fan Yang",
      "Tingting Gao",
      "Guorui Zhou"
    ],
    "published": "2025-08-15T17:59:49+00:00",
    "summary": "Following OpenAI's introduction of the ``thinking with images'' concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in perception and reasoning tasks. However, to the best of our knowledge, no open-source work currently offers a feature set as rich as proprietary models (O3), which can perform diverse image manipulations and simultaneously enhance logical reasoning capabilities through code. In this paper, we make a preliminary attempt in this direction by introducing Thyme (Think Beyond Images), a novel paradigm for enabling MLLMs to transcend existing ``think with images'' approaches by autonomously generating and executing diverse image processing and computational operations via executable code. This approach not only facilitates a rich, on-the-fly set of image manipulations (e.g., cropping, rotation, contrast enhancement) but also allows for mathematical computations, all while maintaining high autonomy in deciding when and how to apply these operations. We activate this capability through a two-stage training strategy: an initial SFT on a curated dataset of 500K samples to teach code generation, followed by a RL phase to refine decision-making. For the RL stage, we manually collect and design high-resolution question-answer pairs to increase the learning difficulty, and we propose GRPO-ATS (Group Relative Policy Optimization with Adaptive Temperature Sampling), an algorithm that applies distinct temperatures to text and code generation to balance reasoning exploration with code execution precision. We conduct extensive experimental analysis and ablation studies. Comprehensive evaluations on nearly 20 benchmarks show that Thyme yields significant and consistent performance gains, particularly in challenging high-resolution perception and complex reasoning tasks."
  },
  {
    "title": "Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective",
    "url": "http://arxiv.org/abs/2508.11618v1",
    "arxiv_id": "2508.11618v1",
    "authors": [
      "Jungang Chen",
      "Seyyed A. Hosseini"
    ],
    "published": "2025-08-15T17:36:25+00:00",
    "summary": "Carbon capture and storage (CCS) projects typically involve a diverse array of stakeholders or players from public, private, and regulatory sectors, each with different objectives and responsibilities. Given the complexity, scale, and long-term nature of CCS operations, determining whether individual stakeholders can independently maximize their interests or whether collaborative coalition agreements are needed remains a central question for effective CCS project planning and management. CCS projects are often implemented in geologically connected sites, where shared geological features such as pressure space and reservoir pore capacity can lead to competitive behavior among stakeholders. Furthermore, CO2 storage sites are often located in geologically mature basins that previously served as sites for hydrocarbon extraction or wastewater disposal in order to leverage existing infrastructures, which makes unilateral optimization even more complicated and unrealistic.   In this work, we propose a paradigm based on Markov games to quantitatively investigate how different coalition structures affect the goals of stakeholders. We frame this multi-stakeholder multi-site problem as a multi-agent reinforcement learning problem with safety constraints. Our approach enables agents to learn optimal strategies while compliant with safety regulations. We present an example where multiple operators are injecting CO2 into their respective project areas in a geologically connected basin. To address the high computational cost of repeated simulations of high-fidelity models, a previously developed surrogate model based on the Embed-to-Control (E2C) framework is employed. Our results demonstrate the effectiveness of the proposed framework in addressing optimal management of CO2 storage when multiple stakeholders with various objectives and goals are involved."
  },
  {
    "title": "Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation",
    "url": "http://arxiv.org/abs/2508.11588v1",
    "arxiv_id": "2508.11588v1",
    "authors": [
      "Benjamin Walt",
      "Jordan Westphal",
      "Girish Krishnan"
    ],
    "published": "2025-08-15T16:47:42+00:00",
    "summary": "Effective and efficient agricultural manipulation and harvesting depend on accurately understanding the current state of the grasp. The agricultural environment presents unique challenges due to its complexity, clutter, and occlusion. Additionally, fruit is physically attached to the plant, requiring precise separation during harvesting. Selecting appropriate sensors and modeling techniques is critical for obtaining reliable feedback and correctly identifying grasp states. This work investigates a set of key sensors, namely inertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile sensors, and RGB cameras, integrated into a compliant gripper to classify grasp states. We evaluate the individual contribution of each sensor and compare the performance of two widely used classification models: Random Forest and Long Short-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest classifier, trained in a controlled lab environment and tested on real cherry tomato plants, achieved 100% accuracy in identifying slip, grasp failure, and successful picks, marking a substantial improvement over baseline performance. Furthermore, we identify a minimal viable sensor combination, namely IMU and tension sensors that effectively classifies grasp states. This classifier enables the planning of corrective actions based on real-time feedback, thereby enhancing the efficiency and reliability of fruit harvesting operations."
  },
  {
    "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks",
    "url": "http://arxiv.org/abs/2508.11584v1",
    "arxiv_id": "2508.11584v1",
    "authors": [
      "Jakub \u0141ucki",
      "Jonathan Becktor",
      "Georgios Georgakis",
      "Robert Royce",
      "Shehryar Khattak"
    ],
    "published": "2025-08-15T16:42:23+00:00",
    "summary": "Deploying multiple machine learning models on resource-constrained robotic platforms for different perception tasks often results in redundant computations, large memory footprints, and complex integration challenges. In response, this work presents Visual Perception Engine (VPEngine), a modular framework designed to enable efficient GPU usage for visual multitasking while maintaining extensibility and developer accessibility. Our framework architecture leverages a shared foundation model backbone that extracts image representations, which are efficiently shared, without any unnecessary GPU-CPU memory transfers, across multiple specialized task-specific model heads running in parallel. This design eliminates the computational redundancy inherent in feature extraction component when deploying traditional sequential models while enabling dynamic task prioritization based on application demands. We demonstrate our framework's capabilities through an example implementation using DINOv2 as the foundation model with multiple task (depth, object detection and semantic segmentation) heads, achieving up to 3x speedup compared to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine offers efficient GPU utilization and maintains a constant memory footprint while allowing per-task inference frequencies to be adjusted dynamically during runtime. The framework is written in Python and is open source with ROS2 C++ (Humble) bindings for ease of use by the robotics community across diverse robotic platforms. Our example implementation demonstrates end-to-end real-time performance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized models."
  },
  {
    "title": "Intergenerational Support for Deepfake Scams Targeting Older Adults",
    "url": "http://arxiv.org/abs/2508.11579v1",
    "arxiv_id": "2508.11579v1",
    "authors": [
      "Karina LaRubbio",
      "Alyssa Lanter",
      "Seihyun Lee",
      "Mahima Ramesh",
      "Diana Freed"
    ],
    "published": "2025-08-15T16:37:59+00:00",
    "summary": "AI-enhanced scams now employ deepfake technology to produce convincing audio and visual impersonations of trusted family members, often grandchildren, in real time. These attacks fabricate urgent scenarios, such as legal or medical emergencies, to socially engineer older adults into transferring money. The realism of these AI-generated impersonations undermines traditional cues used to detect fraud, making them a powerful tool for financial exploitation. In this study, we explore older adults' perceptions of these emerging threats and their responses, with a particular focus on the role of youth, who may also be impacted by having their identities exploited, in supporting older family members' online safety. We conducted focus groups with 37 older adults (ages 65+) to examine their understanding of deepfake impersonation scams and the value of intergenerational technology support. Findings suggest that older adults frequently rely on trusted relationships to detect scams and develop protective practices. Based on this, we identify opportunities to engage youth as active partners in enhancing resilience across generations."
  },
  {
    "title": "SSRL: Self-Search Reinforcement Learning",
    "url": "http://arxiv.org/abs/2508.10874v1",
    "arxiv_id": "2508.10874v1",
    "authors": [
      "Yuchen Fan",
      "Kaiyan Zhang",
      "Heng Zhou",
      "Yuxin Zuo",
      "Yanxu Chen",
      "Yu Fu",
      "Xinwei Long",
      "Xuekai Zhu",
      "Che Jiang",
      "Yuchen Zhang",
      "Li Kang",
      "Gang Chen",
      "Cheng Huang",
      "Zhizhou He",
      "Bingning Wang",
      "Lei Bai",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "published": "2025-08-14T17:46:01+00:00",
    "summary": "We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training."
  },
  {
    "title": "TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning",
    "url": "http://arxiv.org/abs/2508.10872v1",
    "arxiv_id": "2508.10872v1",
    "authors": [
      "Anantha Narayanan",
      "Battu Bhanu Teja",
      "Pruthwik Mishra"
    ],
    "published": "2025-08-14T17:44:51+00:00",
    "summary": "The increasing congestion of Low Earth Orbit (LEO) poses persistent challenges to the efficient deployment and safe operation of Earth observation satellites. Mission planners must now account not only for mission-specific requirements but also for the increasing collision risk with active satellites and space debris. This work presents a reinforcement learning framework using the Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital parameters for precise terrestrial coverage within predefined surface radii. By formulating the problem as a Markov Decision Process (MDP) within a custom OpenAI Gymnasium environment, our method simulates orbital dynamics using classical Keplerian elements. The agent progressively learns to adjust five of the orbital parameters - semi-major axis, eccentricity, inclination, right ascension of ascending node, and the argument of perigee-to achieve targeted terrestrial coverage. Comparative evaluation against Proximal Policy Optimization (PPO) demonstrates A2C's superior performance, achieving 5.8x higher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer timesteps (2,000 vs 63,000). The A2C agent consistently meets mission objectives across diverse target coordinates while maintaining computational efficiency suitable for real-time mission planning applications. Key contributions include: (1) a TLE-based orbital simulation environment incorporating physics constraints, (2) validation of actor-critic methods' superiority over trust region approaches in continuous orbital control, and (3) demonstration of rapid convergence enabling adaptive satellite deployment. This approach establishes reinforcement learning as a computationally efficient alternative for scalable and intelligent LEO mission planning."
  },
  {
    "title": "CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups",
    "url": "http://arxiv.org/abs/2508.10867v1",
    "arxiv_id": "2508.10867v1",
    "authors": [
      "Yizhi Zhou",
      "Ziwei Kang",
      "Jiawei Xia",
      "Xuan Wang"
    ],
    "published": "2025-08-14T17:43:00+00:00",
    "summary": "Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial odometry (VIO) systems. Consistency is crucial for ensuring the estimation accuracy of a UWBaided VIO system. An inconsistent estimator can degrade localization performance, where the inconsistency primarily arises from two main factors: (1) the estimator fails to preserve the correct system observability, and (2) UWB anchor positions are assumed to be known, leading to improper neglect of calibration uncertainty. In this paper, we propose a consistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system based on the Lie group. Our method incorporates the UWB anchor state into the system state, explicitly accounting for UWB calibration uncertainty and enabling the joint and consistent estimation of both robot and anchor states. Furthermore, observability consistency is ensured by leveraging the invariant error properties of the Lie group. We analytically prove that the CVIRO algorithm naturally maintains the system's correct unobservable subspace, thereby preserving estimation consistency. Extensive simulations and experiments demonstrate that CVIRO achieves superior localization accuracy and consistency compared to existing methods."
  },
  {
    "title": "Phased-Array Laser Power Beaming from Cislunar Space to the Lunar Surface",
    "url": "http://arxiv.org/abs/2508.10855v1",
    "arxiv_id": "2508.10855v1",
    "authors": [
      "Slava G. Turyshev"
    ],
    "published": "2025-08-14T17:26:08+00:00",
    "summary": "This paper presents a rigorous analytical framework for quantitatively evaluating space-based laser power beaming from lunar-orbiting spacecraft to surface receivers, addressing the critical need for continuous, high-density energy to sustain lunar exploration and habitation. The framework integrates physics-based models of spacecraft photovoltaic generation, precise orbital geometries, time-dependent link availability and slant-range variations, coherent beam propagation (including transmitter aperture diameter, beam quality factor, path losses, and pointing jitter), and photonic-to-electrical conversion at the lunar surface. Particular emphasis is placed on phased-array transmitter systems, whose large effective apertures significantly reduce beam divergence relative to single-aperture designs, resulting in orders-of-magnitude increases in delivered surface power under equivalent orbital and power conditions. Parametric sensitivity analyses and illustrative numerical simulations demonstrate how phased-array architectures improve power density and end-to-end efficiency at operational lunar distances. The study also examines advanced orbital configurations (e.g., Near-Rectilinear Halo Orbits, Earth-Moon Lagrange points), real-time adaptive beam steering and wavefront control, optimized receiver geometries, and thermal/dust mitigation strategies. The results establish a clear pathway toward scalable, efficient laser power beaming infrastructures capable of overcoming lunar-specific challenges - including prolonged darkness and permanently shadowed regions - and enabling sustained robotic and crewed surface operations."
  },
  {
    "title": "A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots",
    "url": "http://arxiv.org/abs/2508.10828v1",
    "arxiv_id": "2508.10828v1",
    "authors": [
      "Henry Powell",
      "Guy Laban",
      "Emily S. Cross"
    ],
    "published": "2025-08-14T16:50:51+00:00",
    "summary": "Subjective self-disclosure is an important feature of human social interaction. While much has been done in the social and behavioural literature to characterise the features and consequences of subjective self-disclosure, little work has been done thus far to develop computational systems that are able to accurately model it. Even less work has been done that attempts to model specifically how human interactants self-disclose with robotic partners. It is becoming more pressing as we require social robots to work in conjunction with and establish relationships with humans in various social settings. In this paper, our aim is to develop a custom multimodal attention network based on models from the emotion recognition literature, training this model on a large self-collected self-disclosure video corpus, and constructing a new loss function, the scale preserving cross entropy loss, that improves upon both classification and regression versions of this problem. Our results show that the best performing model, trained with our novel loss function, achieves an F1 score of 0.83, an improvement of 0.48 from the best baseline model. This result makes significant headway in the aim of allowing social robots to pick up on an interaction partner's self-disclosures, an ability that will be essential in social robots with social cognition."
  },
  {
    "title": "Masquerade: Learning from In-the-wild Human Videos using Data-Editing",
    "url": "http://arxiv.org/abs/2508.09976v1",
    "arxiv_id": "2508.09976v1",
    "authors": [
      "Marion Lepert",
      "Jiaying Fang",
      "Jeannette Bohg"
    ],
    "published": "2025-08-13T17:43:34+00:00",
    "summary": "Robot manipulation research still suffers from significant data scarcity: even the largest robot datasets are orders of magnitude smaller and less diverse than those that fueled recent breakthroughs in language and vision. We introduce Masquerade, a method that edits in-the-wild egocentric human videos to bridge the visual embodiment gap between humans and robots and then learns a robot policy with these edited videos. Our pipeline turns each human video into robotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the human arms, and (iii) overlaying a rendered bimanual robot that tracks the recovered end-effector trajectories. Pre-training a visual encoder to predict future 2-D robot keypoints on 675K frames of these edited clips, and continuing that auxiliary loss while fine-tuning a diffusion policy head on only 50 robot demonstrations per task, yields policies that generalize significantly better than prior work. On three long-horizon, bimanual kitchen tasks evaluated in three unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations show that both the robot overlay and co-training are indispensable, and performance scales logarithmically with the amount of edited human video. These results demonstrate that explicitly closing the visual embodiment gap unlocks a vast, readily available source of data from human videos that can be used to improve robot policies."
  },
  {
    "title": "Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model",
    "url": "http://arxiv.org/abs/2508.09971v1",
    "arxiv_id": "2508.09971v1",
    "authors": [
      "Zihan Wang",
      "Nina Mahmoudian"
    ],
    "published": "2025-08-13T17:39:09+00:00",
    "summary": "Vision-driven autonomous river following by Unmanned Aerial Vehicles is critical for applications such as rescue, surveillance, and environmental monitoring, particularly in dense riverine environments where GPS signals are unreliable. We formalize river following as a coverage control problem in which the reward function is submodular, yielding diminishing returns as more unique river segments are visited, thereby framing the task as a Submodular Markov Decision Process. First, we introduce Marginal Gain Advantage Estimation, which refines the reward advantage function by using a sliding window baseline computed from historical episodic returns, thus aligning the advantage estimation with the agent's evolving recognition of action value in non-Markovian settings. Second, we develop a Semantic Dynamics Model based on patchified water semantic masks that provides more interpretable and data-efficient short-term prediction of future observations compared to latent vision dynamics models. Third, we present the Constrained Actor Dynamics Estimator architecture, which integrates the actor, the cost estimator, and SDM for cost advantage estimation to form a model-based SafeRL framework capable of solving partially observable Constrained Submodular Markov Decision Processes. Simulation results demonstrate that MGAE achieves faster convergence and superior performance over traditional critic-based methods like Generalized Advantage Estimation. SDM provides more accurate short-term state predictions that enable the cost estimator to better predict potential violations. Overall, CADE effectively integrates safety regulation into model-based RL, with the Lagrangian approach achieving the soft balance of reward and safety during training, while the safety layer enhances performance during inference by hard action overlay."
  },
  {
    "title": "Online Safety under Multiple Constraints and Input Bounds using gatekeeper: Theory and Applications",
    "url": "http://arxiv.org/abs/2508.09963v1",
    "arxiv_id": "2508.09963v1",
    "authors": [
      "Devansh R. Agrawal",
      "Dimitra Panagou"
    ],
    "published": "2025-08-13T17:31:29+00:00",
    "summary": "This letter presents an approach to guarantee online safety of a cyber-physical system under multiple state and input constraints. Our proposed framework, called gatekeeper, recursively guarantees the existence of an infinite-horizon trajectory that satisfies all constraints and system dynamics. Such trajectory is constructed using a backup controller, which we define formally in this paper. gatekeeper relies on a small number of verifiable assumptions, and is computationally efficient since it requires optimization over a single scalar variable. We make two primary contributions in this letter. (A) First, we develop the theory of gatekeeper: we derive a sub-optimality bound relative to a full nonlinear trajectory optimization problem, and show how this can be used in runtime to validate performance. This also informs the design of the backup controllers and sets. (B) Second, we demonstrate in detail an application of gatekeeper for multi-agent formation flight, where each Dubins agent must avoid multiple obstacles and weapons engagement zones, both of which are nonlinear, nonconvex constraints."
  },
  {
    "title": "GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation",
    "url": "http://arxiv.org/abs/2508.09960v1",
    "arxiv_id": "2508.09960v1",
    "authors": [
      "Yifei Yao",
      "Chengyuan Luo",
      "Jiaheng Du",
      "Wentao He",
      "Jun-Guo Lu"
    ],
    "published": "2025-08-13T17:28:39+00:00",
    "summary": "The creation of human-like humanoid robots is hindered by a fundamental fragmentation: data processing and learning algorithms are rarely universal across different robot morphologies. This paper introduces the Generalized Behavior Cloning (GBC) framework, a comprehensive and unified solution designed to solve this end-to-end challenge. GBC establishes a complete pathway from human motion to robot action through three synergistic innovations. First, an adaptive data pipeline leverages a differentiable IK network to automatically retarget any human MoCap data to any humanoid. Building on this foundation, our novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust, high-fidelity imitation policies. To complete the ecosystem, the entire framework is delivered as an efficient, open-source platform based on Isaac Lab, empowering the community to deploy the full workflow via simple configuration scripts. We validate the power and generality of GBC by training policies on multiple heterogeneous humanoids, demonstrating excellent performance and transfer to novel motions. This work establishes the first practical and unified pathway for creating truly generalized humanoid controllers."
  },
  {
    "title": "LIA-X: Interpretable Latent Portrait Animator",
    "url": "http://arxiv.org/abs/2508.09959v1",
    "arxiv_id": "2508.09959v1",
    "authors": [
      "Yaohui Wang",
      "Di Yang",
      "Xinyuan Chen",
      "Francois Bremond",
      "Yu Qiao",
      "Antitza Dantcheva"
    ],
    "published": "2025-08-13T17:22:05+00:00",
    "summary": "We introduce LIA-X, a novel interpretable portrait animator designed to transfer facial dynamics from a driving video to a source portrait with fine-grained control. LIA-X is an autoencoder that models motion transfer as a linear navigation of motion codes in latent space. Crucially, it incorporates a novel Sparse Motion Dictionary that enables the model to disentangle facial dynamics into interpretable factors. Deviating from previous 'warp-render' approaches, the interpretability of the Sparse Motion Dictionary allows LIA-X to support a highly controllable 'edit-warp-render' strategy, enabling precise manipulation of fine-grained facial semantics in the source portrait. This helps to narrow initial differences with the driving video in terms of pose and expression. Moreover, we demonstrate the scalability of LIA-X by successfully training a large-scale model with approximately 1 billion parameters on extensive datasets. Experimental results show that our proposed method outperforms previous approaches in both self-reenactment and cross-reenactment tasks across several benchmarks. Additionally, the interpretable and controllable nature of LIA-X supports practical applications such as fine-grained, user-guided image and video editing, as well as 3D-aware portrait video manipulation."
  },
  {
    "title": "Nonlinear Symmetry Breaking to Enhance the Sagnac Effect in a Microresonator Gyroscope",
    "url": "http://arxiv.org/abs/2508.09132v1",
    "arxiv_id": "2508.09132v1",
    "authors": [
      "Thariq Shanavas",
      "Gregory Krueper",
      "Jiangang Zhu",
      "Wounjhang Park",
      "Juliet T. Gopinath"
    ],
    "published": "2025-08-12T17:57:24+00:00",
    "summary": "Optical gyroscopes based on the Sagnac effect have been widely used for inertial navigation in aircrafts, submarines, satellites and unmanned robotics. With the rapid progress in the field of ultrahigh-quality whispering gallery mode and ring resonators in recent years, these devices offer the promise of a compact alternative to ring-laser gyroscopes (RLGs) and fiber-optic gyroscopes (FOGs). Yet, successful commercialization of a microresonator gyroscope has been hindered by the scaling of the Sagnac effect with resonator area. While several techniques have been proposed to enhance the Sagnac effect in microresonators, these enhancements also amplify the thermal noise in the microresonator. Here, we present a novel approach to measuring the Sagnac signal in chip-scale devices that overcomes this fundamental noise limitation to achieve unprecedented performance in a 200 {\\mu}m optical resonator - the smallest reported to date. Our proof-of-concept design shows a 10^4 enhancement of the Sagnac signal while simultaneously suppressing thermal noise by 27 dB and environmental contributions to noise by 22 dB. We believe this approach offers a pathway for integrated photonic gyroscopes with sensitivities that match or exceed RLGs and FOGs."
  },
  {
    "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer",
    "url": "http://arxiv.org/abs/2508.09131v1",
    "arxiv_id": "2508.09131v1",
    "authors": [
      "Zixin Yin",
      "Xili Dai",
      "Ling-Hao Chen",
      "Deyu Zhou",
      "Jianan Wang",
      "Duomin Wang",
      "Gang Yu",
      "Lionel M. Ni",
      "Heung-Yeung Shum"
    ],
    "published": "2025-08-12T17:57:04+00:00",
    "summary": "Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility."
  },
  {
    "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer",
    "url": "http://arxiv.org/abs/2508.09131v2",
    "arxiv_id": "2508.09131v2",
    "authors": [
      "Zixin Yin",
      "Xili Dai",
      "Ling-Hao Chen",
      "Deyu Zhou",
      "Jianan Wang",
      "Duomin Wang",
      "Gang Yu",
      "Lionel M. Ni",
      "Lei Zhang",
      "Heung-Yeung Shum"
    ],
    "published": "2025-08-12T17:57:04+00:00",
    "summary": "Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility."
  },
  {
    "title": "A Review On Safe Reinforcement Learning Using Lyapunov and Barrier Functions",
    "url": "http://arxiv.org/abs/2508.09128v1",
    "arxiv_id": "2508.09128v1",
    "authors": [
      "Dhruv S. Kushwaha",
      "Zoleikha A. Biron"
    ],
    "published": "2025-08-12T17:55:36+00:00",
    "summary": "Reinforcement learning (RL) has proven to be particularly effective in solving complex decision-making problems for a wide range of applications. From a control theory perspective, RL can be considered as an adaptive optimal control scheme. Lyapunov and barrier functions are the most commonly used certificates to guarantee system stability for a proposed/derived controller and constraint satisfaction guarantees, respectively, in control theoretic approaches. However, compared to theoretical guarantees available in control theoretic methods, RL lacks closed-loop stability of a computed policy and constraint satisfaction guarantees. Safe reinforcement learning refers to a class of constrained problems where the constraint violations lead to partial or complete system failure. The goal of this review is to provide an overview of safe RL techniques using Lyapunov and barrier functions to guarantee this notion of safety discussed (stability of the system in terms of a computed policy and constraint satisfaction during training and deployment). The different approaches employed are discussed in detail along with their shortcomings and benefits to provide critique and possible future research directions. Key motivation for this review is to discuss current theoretical approaches for safety and stability guarantees in RL similar to control theoretic approaches using Lyapunov and barrier functions. The review provides proven potential and promising scope of providing safety guarantees for complex dynamical systems with operational constraints using model-based and model-free RL."
  },
  {
    "title": "Deep Neural Network Calibration by Reducing Classifier Shift with Stochastic Masking",
    "url": "http://arxiv.org/abs/2508.09116v1",
    "arxiv_id": "2508.09116v1",
    "authors": [
      "Jiani Ni",
      "He Zhao",
      "Yibo Yang",
      "Dandan Guo"
    ],
    "published": "2025-08-12T17:50:23+00:00",
    "summary": "In recent years, deep neural networks (DNNs) have shown competitive results in many fields. Despite this success, they often suffer from poor calibration, especially in safety-critical scenarios such as autonomous driving and healthcare, where unreliable confidence estimates can lead to serious consequences. Recent studies have focused on improving calibration by modifying the classifier, yet such efforts remain limited. Moreover, most existing approaches overlook calibration errors caused by underconfidence, which can be equally detrimental. To address these challenges, we propose MaC-Cal, a novel mask-based classifier calibration method that leverages stochastic sparsity to enhance the alignment between confidence and accuracy. MaC-Cal adopts a two-stage training scheme with adaptive sparsity, dynamically adjusting mask retention rates based on the deviation between confidence and accuracy. Extensive experiments show that MaC-Cal achieves superior calibration performance and robustness under data corruption, offering a practical and effective solution for reliable confidence estimation in DNNs."
  },
  {
    "title": "Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving",
    "url": "http://arxiv.org/abs/2508.09099v1",
    "arxiv_id": "2508.09099v1",
    "authors": [
      "Tianyun Yang",
      "Yunwen Li",
      "Ziniu Li",
      "Zhihang Lin",
      "Ruoyu Sun",
      "Tian Ding"
    ],
    "published": "2025-08-12T17:26:23+00:00",
    "summary": "Large vision language models exhibit notable limitations on Geometry Problem Solving (GPS) because of their unreliable diagram interpretation and pure natural-language reasoning. A recent line of work mitigates this by using symbolic solvers: the model directly generates a formal program that a geometry solver can execute. However, this direct program generation lacks intermediate reasoning, making the decision process opaque and prone to errors. In this work, we explore a new approach that integrates Chain-of-Thought (CoT) with formal language. The model interleaves natural language reasoning with incremental emission of solver-executable code, producing a hybrid reasoning trace in which critical derivations are expressed in formal language. To teach this behavior at scale, we combine (1) supervised fine-tuning on an 11K newly developed synthetic dataset with interleaved natural language reasoning and automatic formalization, and (2) solver-in-the-loop reinforcement learning that jointly optimizes both the CoT narrative and the resulting program through outcome-based rewards. Built on Qwen2.5-VL-7B, our new model, named GF-Reasoner, achieves up to 15% accuracy improvements on standard GPS benchmarks, surpassing both 7B-scale peers and the much larger model Qwen2.5-VL-72B. By exploiting high-order geometric knowledge and offloading symbolic computation to the solver, the generated reasoning traces are noticeably shorter and cleaner. Furthermore, we present a comprehensive analysis of method design choices (e.g., reasoning paradigms, data synthesis, training epochs, etc.), providing actionable insights for future research."
  },
  {
    "title": "Jinx: Unlimited LLMs for Probing Alignment Failures",
    "url": "http://arxiv.org/abs/2508.08243v1",
    "arxiv_id": "2508.08243v1",
    "authors": [
      "Jiahao Zhao",
      "Liwei Dong"
    ],
    "published": "2025-08-11T17:56:06+00:00",
    "summary": "Unlimited, or so-called helpful-only language models are trained without safety alignment constraints and never refuse user queries. They are widely used by leading AI companies as internal tools for red teaming and alignment evaluation. For example, if a safety-aligned model produces harmful outputs similar to an unlimited model, this indicates alignment failures that require further attention. Despite their essential role in assessing alignment, such models are not available to the research community.   We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx responds to all queries without refusals or safety filtering, while preserving the base model's capabilities in reasoning and instruction following. It provides researchers with an accessible tool for probing alignment failures, evaluating safety boundaries, and systematically studying failure modes in language model safety."
  },
  {
    "title": "BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion",
    "url": "http://arxiv.org/abs/2508.08241v1",
    "arxiv_id": "2508.08241v1",
    "authors": [
      "Takara E. Truong",
      "Qiayuan Liao",
      "Xiaoyu Huang",
      "Guy Tevet",
      "C. Karen Liu",
      "Koushil Sreenath"
    ],
    "published": "2025-08-11T17:55:26+00:00",
    "summary": "Learning skills from human motions offers a promising path toward generalizable policies for whole-body humanoid control, yet two key cornerstones are missing: (1) a high-quality motion tracking framework that faithfully transforms large-scale kinematic references into robust and extremely dynamic motions on real hardware, and (2) a distillation approach that can effectively learn these motion primitives and compose them to solve downstream tasks. We address these gaps with BeyondMimic, the first real-world framework to learn from human motions for versatile and naturalistic humanoid control via guided diffusion. Our framework provides a motion tracking pipeline capable of challenging skills such as jumping spins, sprinting, and cartwheels with state-of-the-art motion quality. Moving beyond mimicking existing motions and synthesize novel ones, we further introduce a unified diffusion policy that enables zero-shot task-specific control at test time using simple cost functions. Deployed on hardware, BeyondMimic performs diverse tasks at test time, including waypoint navigation, joystick teleoperation, and obstacle avoidance, bridging sim-to-real motion tracking and flexible synthesis of human motion primitives for whole-body control. https://beyondmimic.github.io/."
  },
  {
    "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks",
    "url": "http://arxiv.org/abs/2508.08240v1",
    "arxiv_id": "2508.08240v1",
    "authors": [
      "Kaijun Wang",
      "Liqin Lu",
      "Mingyu Liu",
      "Jianuo Jiang",
      "Zeju Li",
      "Bolin Zhang",
      "Wancai Zheng",
      "Xinyi Yu",
      "Hao Chen",
      "Chunhua Shen"
    ],
    "published": "2025-08-11T17:54:31+00:00",
    "summary": "Language-guided long-horizon mobile manipulation has long been a grand challenge in embodied semantic reasoning, generalizable manipulation, and adaptive locomotion. Three fundamental limitations hinder progress: First, although large language models have improved spatial reasoning and task planning through semantic priors, existing implementations remain confined to tabletop scenarios, failing to address the constrained perception and limited actuation ranges of mobile platforms. Second, current manipulation strategies exhibit insufficient generalization when confronted with the diverse object configurations encountered in open-world environments. Third, while crucial for practical deployment, the dual requirement of maintaining high platform maneuverability alongside precise end-effector control in unstructured settings remains understudied.   In this work, we present ODYSSEY, a unified mobile manipulation framework for agile quadruped robots equipped with manipulators, which seamlessly integrates high-level task planning with low-level whole-body control. To address the challenge of egocentric perception in language-conditioned tasks, we introduce a hierarchical planner powered by a vision-language model, enabling long-horizon instruction decomposition and precise action execution. At the control level, our novel whole-body policy achieves robust coordination across challenging terrains. We further present the first benchmark for long-horizon mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through successful sim-to-real transfer, we demonstrate the system's generalization and robustness in real-world deployments, underscoring the practicality of legged manipulators in unstructured environments. Our work advances the feasibility of generalized robotic assistants capable of complex, dynamic tasks. Our project page: https://kaijwang.github.io/odyssey.github.io/"
  },
  {
    "title": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge",
    "url": "http://arxiv.org/abs/2508.08236v1",
    "arxiv_id": "2508.08236v1",
    "authors": [
      "Yunna Cai",
      "Fan Wang",
      "Haowei Wang",
      "Kun Wang",
      "Kailai Yang",
      "Sophia Ananiadou",
      "Moyan Li",
      "Mingming Fan"
    ],
    "published": "2025-08-11T17:52:07+00:00",
    "summary": "Evaluating the safety alignment of LLM responses in high-risk mental health dialogues is particularly difficult due to missing gold-standard answers and the ethically sensitive nature of these interactions. To address this challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark based on real-world Chinese mental health dialogues. It evaluates whether the model responses align with the safety principles defined by experts. Specifically designed for settings without standard references, our method adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation using expert-defined reasoning chains grounded in psychological intervention principles. We employ binary point-wise scoring across multiple safety dimensions to enhance the explainability and traceability of the evaluation. Additionally, we present a manually curated, high-quality Chinese-language dataset covering self-harm, suicidal ideation, and existential distress, derived from real-world online discourse. Experiments on 3600 judgments show that our method achieves the highest agreement with expert assessments and produces more interpretable evaluation rationales compared to existing approaches. Our dataset and evaluation tool are publicly available to facilitate further research."
  },
  {
    "title": "Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy",
    "url": "http://arxiv.org/abs/2508.08226v1",
    "arxiv_id": "2508.08226v1",
    "authors": [
      "Haiyue Chen",
      "Aniket Datar",
      "Tong Xu",
      "Francesco Cancelliere",
      "Harsh Rangwala",
      "Madhan Balaji Rao",
      "Daeun Song",
      "David Eichinger",
      "Xuesu Xiao"
    ],
    "published": "2025-08-11T17:44:27+00:00",
    "summary": "Off-road navigation is an important capability for mobile robots deployed in environments that are inaccessible or dangerous to humans, such as disaster response or planetary exploration. Progress is limited due to the lack of a controllable and standardized real-world testbed for systematic data collection and validation. To fill this gap, we introduce Verti-Arena, a reconfigurable indoor facility designed specifically for off-road autonomy. By providing a repeatable benchmark environment, Verti-Arena supports reproducible experiments across a variety of vertically challenging terrains and provides precise ground truth measurements through onboard sensors and a motion capture system. Verti-Arena also supports consistent data collection and comparative evaluation of algorithms in off-road autonomy research. We also develop a web-based interface that enables research groups worldwide to remotely conduct standardized off-road autonomy experiments on Verti-Arena."
  },
  {
    "title": "Voting-Based Semi-Parallel Proof-of-Work Protocol",
    "url": "http://arxiv.org/abs/2508.06489v1",
    "arxiv_id": "2508.06489v1",
    "authors": [
      "Mustafa Doger",
      "Sennur Ulukus"
    ],
    "published": "2025-08-08T17:57:35+00:00",
    "summary": "Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety guarantees, transaction throughput and confirmation latencies of Nakamoto consensus. In this work, we first consider the existing parallel PoW protocols and develop hard-coded incentive attack structures. Our theoretical results and simulations show that the existing parallel PoW protocols are more vulnerable to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller profitability threshold and they result in higher relative rewards. Next, we introduce a voting-based semi-parallel PoW protocol that outperforms both Nakamoto consensus and the existing parallel PoW protocols from most practical perspectives such as communication overheads, throughput, transaction conflicts, incentive compatibility of the protocol as well as a fair distribution of transaction fees among the voters and the leaders. We use state-of-the-art analysis to evaluate the consistency of the protocol and consider Markov decision process (MDP) models to substantiate our claims about the resilience of our protocol against incentive attacks."
  },
  {
    "title": "The Problem of Atypicality in LLM-Powered Psychiatry",
    "url": "http://arxiv.org/abs/2508.06479v1",
    "arxiv_id": "2508.06479v1",
    "authors": [
      "Bosco Garcia",
      "Eugene Y. S. Chua",
      "Harman Singh Brah"
    ],
    "published": "2025-08-08T17:36:42+00:00",
    "summary": "Large language models (LLMs) are increasingly proposed as scalable solutions to the global mental health crisis. But their deployment in psychiatric contexts raises a distinctive ethical concern: the problem of atypicality. Because LLMs generate outputs based on population-level statistical regularities, their responses -- while typically appropriate for general users -- may be dangerously inappropriate when interpreted by psychiatric patients, who often exhibit atypical cognitive or interpretive patterns. We argue that standard mitigation strategies, such as prompt engineering or fine-tuning, are insufficient to resolve this structural risk. Instead, we propose dynamic contextual certification (DCC): a staged, reversible and context-sensitive framework for deploying LLMs in psychiatry, inspired by clinical translation and dynamic safety models from artificial intelligence governance. DCC reframes chatbot deployment as an ongoing epistemic and ethical process that prioritises interpretive safety over static performance benchmarks. Atypicality, we argue, cannot be eliminated -- but it can, and must, be proactively managed."
  },
  {
    "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning",
    "url": "http://arxiv.org/abs/2508.06475v1",
    "arxiv_id": "2508.06475v1",
    "authors": [
      "Guimin Hu",
      "Daniel Hershcovich",
      "Hasti Seifi"
    ],
    "published": "2025-08-08T17:25:37+00:00",
    "summary": "Haptic captioning is the task of generating natural language descriptions from haptic signals, such as vibrations, for use in virtual reality, accessibility, and rehabilitation applications. While previous multimodal research has focused primarily on vision and audio, haptic signals for the sense of touch remain underexplored. To address this gap, we formalize the haptic captioning task and propose HapticLLaMA, a multimodal sensory language model that interprets vibration signals into descriptions in a given sensory, emotional, or associative category. We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units, enabling their integration with the LLaMA model. HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation, and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We assess HapticLLaMA's captioning performance using both automated n-gram metrics and human evaluation. HapticLLaMA demonstrates strong capability in interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated captions received human ratings above 3.5 on a 7-point scale, with RLHF yielding a 10% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception. These findings highlight the potential of large language models to process and adapt to sensory data."
  },
  {
    "title": "Revisiting the Gas Dynamics of Henize 2-10: Possible Drivers of the Starburst",
    "url": "http://arxiv.org/abs/2508.06472v1",
    "arxiv_id": "2508.06472v1",
    "authors": [
      "Josephine M. Dalsin",
      "Allison H. Costa",
      "Remy Indebetouw",
      "Kelsey E. Johnson",
      "Natalie O. Johnson",
      "Sabrina Stierwalt"
    ],
    "published": "2025-08-08T17:21:26+00:00",
    "summary": "The triggers of starburst episodes are a key component to our understanding of the baryon cycle in galaxies. Galaxy mergers are a commonly suggested catalyst for starbursts, but once the galaxies coalesce into a single kinematically disturbed system, their merger history can be difficult to assess. This is particularly true for dwarf galaxies, which are expected to dominate the merger rate at all redshifts due to their large numbers. One such dwarf galaxy undergoing an enigmatic starburst episode is Henize 2-10, which appears to be isolated. Possible scenarios that might have caused the starburst episode include a previous merger or stochastic processes within the galaxy itself, such as self-regulation via feedback processes. We present new VLA 21-cm observations and unpublished archival CARMA CO data to investigate the dynamical state and star formation activity in the galaxy. We do not detect an HI tail consistent with the structure reported by Kobulnicky et al. (1995), which was suggested as evidence for a merger or interaction, but rather these new observations indicate an extended HI distribution. We also find that the HI appears dynamically decoupled from an extended CO feature (inferred to be a tidal tail in previous work), suggesting large-scale dynamical processes of some type are affecting the gas in this system. We provide a meta-analysis of available results to enhance our understanding of what might be triggering the starburst episode in Henize 2-10, and speculate that the large CO feature could be falling into the galaxy and potentially trigger starburst activity."
  },
  {
    "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
    "url": "http://arxiv.org/abs/2508.06471v1",
    "arxiv_id": "2508.06471v1",
    "authors": [
      "GLM-4. 5 Team",
      ":",
      "Aohan Zeng",
      "Xin Lv",
      "Qinkai Zheng",
      "Zhenyu Hou",
      "Bin Chen",
      "Chengxing Xie",
      "Cunxiang Wang",
      "Da Yin",
      "Hao Zeng",
      "Jiajie Zhang",
      "Kedong Wang",
      "Lucen Zhong",
      "Mingdao Liu",
      "Rui Lu",
      "Shulin Cao",
      "Xiaohan Zhang",
      "Xuancheng Huang",
      "Yao Wei",
      "Yean Cheng",
      "Yifan An",
      "Yilin Niu",
      "Yuanhao Wen",
      "Yushi Bai",
      "Zhengxiao Du",
      "Zihan Wang",
      "Zilin Zhu",
      "Bohan Zhang",
      "Bosi Wen",
      "Bowen Wu",
      "Bowen Xu",
      "Can Huang",
      "Casey Zhao",
      "Changpeng Cai",
      "Chao Yu",
      "Chen Li",
      "Chendi Ge",
      "Chenghua Huang",
      "Chenhui Zhang",
      "Chenxi Xu",
      "Chenzheng Zhu",
      "Chuang Li",
      "Congfeng Yin",
      "Daoyan Lin",
      "Dayong Yang",
      "Dazhi Jiang",
      "Ding Ai",
      "Erle Zhu",
      "Fei Wang",
      "Gengzheng Pan",
      "Guo Wang",
      "Hailong Sun",
      "Haitao Li",
      "Haiyang Li",
      "Haiyi Hu",
      "Hanyu Zhang",
      "Hao Peng",
      "Hao Tai",
      "Haoke Zhang",
      "Haoran Wang",
      "Haoyu Yang",
      "He Liu",
      "He Zhao",
      "Hongwei Liu",
      "Hongxi Yan",
      "Huan Liu",
      "Huilong Chen",
      "Ji Li",
      "Jiajing Zhao",
      "Jiamin Ren",
      "Jian Jiao",
      "Jiani Zhao",
      "Jianyang Yan",
      "Jiaqi Wang",
      "Jiayi Gui",
      "Jiayue Zhao",
      "Jie Liu",
      "Jijie Li",
      "Jing Li",
      "Jing Lu",
      "Jingsen Wang",
      "Jingwei Yuan",
      "Jingxuan Li",
      "Jingzhao Du",
      "Jinhua Du",
      "Jinxin Liu",
      "Junkai Zhi",
      "Junli Gao",
      "Ke Wang",
      "Lekang Yang",
      "Liang Xu",
      "Lin Fan",
      "Lindong Wu",
      "Lintao Ding",
      "Lu Wang",
      "Man Zhang",
      "Minghao Li",
      "Minghuan Xu",
      "Mingming Zhao",
      "Mingshu Zhai",
      "Pengfan Du",
      "Qian Dong",
      "Shangde Lei",
      "Shangqing Tu",
      "Shangtong Yang",
      "Shaoyou Lu",
      "Shijie Li",
      "Shuang Li",
      "Shuang-Li",
      "Shuxun Yang",
      "Sibo Yi",
      "Tianshu Yu",
      "Wei Tian",
      "Weihan Wang",
      "Wenbo Yu",
      "Weng Lam Tam",
      "Wenjie Liang",
      "Wentao Liu",
      "Xiao Wang",
      "Xiaohan Jia",
      "Xiaotao Gu",
      "Xiaoying Ling",
      "Xin Wang",
      "Xing Fan",
      "Xingru Pan",
      "Xinyuan Zhang",
      "Xinze Zhang",
      "Xiuqing Fu",
      "Xunkai Zhang",
      "Yabo Xu",
      "Yandong Wu",
      "Yida Lu",
      "Yidong Wang",
      "Yilin Zhou",
      "Yiming Pan",
      "Ying Zhang",
      "Yingli Wang",
      "Yingru Li",
      "Yinpei Su",
      "Yipeng Geng",
      "Yitong Zhu",
      "Yongkun Yang",
      "Yuhang Li",
      "Yuhao Wu",
      "Yujiang Li",
      "Yunan Liu",
      "Yunqing Wang",
      "Yuntao Li",
      "Yuxuan Zhang",
      "Zezhen Liu",
      "Zhen Yang",
      "Zhengda Zhou",
      "Zhongpei Qiao",
      "Zhuoer Feng",
      "Zhuorui Liu",
      "Zichen Zhang",
      "Zihan Wang",
      "Zijun Yao",
      "Zikang Wang",
      "Ziqiang Liu",
      "Ziwei Chai",
      "Zixuan Li",
      "Zuodong Zhao",
      "Wenguang Chen",
      "Jidong Zhai",
      "Bin Xu",
      "Minlie Huang",
      "Hongning Wang",
      "Juanzi Li",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "published": "2025-08-08T17:21:06+00:00",
    "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5."
  },
  {
    "title": "Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation",
    "url": "http://arxiv.org/abs/2508.05635v1",
    "arxiv_id": "2508.05635v1",
    "authors": [
      "Yue Liao",
      "Pengfei Zhou",
      "Siyuan Huang",
      "Donglin Yang",
      "Shengcong Chen",
      "Yuxin Jiang",
      "Yue Hu",
      "Jingbin Cai",
      "Si Liu",
      "Jianlan Luo",
      "Liliang Chen",
      "Shuicheng Yan",
      "Maoqing Yao",
      "Guanghui Ren"
    ],
    "published": "2025-08-07T17:59:44+00:00",
    "summary": "We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly."
  },
  {
    "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling",
    "url": "http://arxiv.org/abs/2508.05634v1",
    "arxiv_id": "2508.05634v1",
    "authors": [
      "Jianpeng Yao",
      "Xiaopan Zhang",
      "Yu Xia",
      "Zejin Wang",
      "Amit K. Roy-Chowdhury",
      "Jiachen Li"
    ],
    "published": "2025-08-07T17:59:43+00:00",
    "summary": "Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/."
  },
  {
    "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification",
    "url": "http://arxiv.org/abs/2508.05629v1",
    "arxiv_id": "2508.05629v1",
    "authors": [
      "Yongliang Wu",
      "Yizhou Zhou",
      "Zhou Ziheng",
      "Yingzhe Peng",
      "Xinyu Ye",
      "Xinting Hu",
      "Wenbo Zhu",
      "Lu Qi",
      "Ming-Hsuan Yang",
      "Xu Yang"
    ],
    "published": "2025-08-07T17:59:04+00:00",
    "summary": "We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT."
  },
  {
    "title": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations",
    "url": "http://arxiv.org/abs/2508.05625v1",
    "arxiv_id": "2508.05625v1",
    "authors": [
      "Brandon Jaipersaud",
      "David Krueger",
      "Ekdeep Singh Lubana"
    ],
    "published": "2025-08-07T17:58:41+00:00",
    "summary": "Large Language Models (LLMs) have started to demonstrate the ability to persuade humans, yet our understanding of how this dynamic transpires is limited. Recent work has used linear probes, lightweight tools for analyzing model representations, to study various LLM skills such as the ability to model user sentiment and political perspective. Motivated by this, we apply probes to study persuasion dynamics in natural, multi-turn conversations. We leverage insights from cognitive science to train probes on distinct aspects of persuasion: persuasion success, persuadee personality, and persuasion strategy. Despite their simplicity, we show that they capture various aspects of persuasion at both the sample and dataset levels. For instance, probes can identify the point in a conversation where the persuadee was persuaded or where persuasive success generally occurs across the entire dataset. We also show that in addition to being faster than expensive prompting-based approaches, probes can do just as well and even outperform prompting in some settings, such as when uncovering persuasion strategy. This suggests probes as a plausible avenue for studying other complex behaviours such as deception and manipulation, especially in multi-turn settings and large-scale dataset analysis where prompting-based methods would be computationally inefficient."
  },
  {
    "title": "Back to Bits: Extending Shannon's communication performance framework to computing",
    "url": "http://arxiv.org/abs/2508.05621v1",
    "arxiv_id": "2508.05621v1",
    "authors": [
      "Max Hawkins",
      "Richard Vuduc"
    ],
    "published": "2025-08-07T17:57:45+00:00",
    "summary": "This work proposes a novel computing performance unit grounded in information theory. Modern computing systems are increasingly diverse, supporting low-precision formats, hardware specialization, and emerging paradigms such as analog, quantum, and reversible logic. Traditional metrics like floating-point operations (flops) no longer accurately capture this complexity. We frame computing as the transformation of information through a channel and define performance in terms of the mutual information between a system's inputs and outputs. This approach measures not just the quantity of data processed, but the amount of meaningful information encoded, manipulated, and retained through computation. Our framework provides a principled, implementation-agnostic foundation for evaluating performance."
  },
  {
    "title": "BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning",
    "url": "http://arxiv.org/abs/2508.04702v1",
    "arxiv_id": "2508.04702v1",
    "authors": [
      "Ziyang Leng",
      "Jiawei Yang",
      "Zhicheng Ren",
      "Bolei Zhou"
    ],
    "published": "2025-08-06T17:59:37+00:00",
    "summary": "We present BEVCon, a simple yet effective contrastive learning framework designed to improve Bird's Eye View (BEV) perception in autonomous driving. BEV perception offers a top-down-view representation of the surrounding environment, making it crucial for 3D object detection, segmentation, and trajectory prediction tasks. While prior work has primarily focused on enhancing BEV encoders and task-specific heads, we address the underexplored potential of representation learning in BEV models. BEVCon introduces two contrastive learning modules: an instance feature contrast module for refining BEV features and a perspective view contrast module that enhances the image backbone. The dense contrastive learning designed on top of detection losses leads to improved feature representations across both the BEV encoder and the backbone. Extensive experiments on the nuScenes dataset demonstrate that BEVCon achieves consistent performance gains, achieving up to +2.4% mAP improvement over state-of-the-art baselines. Our results highlight the critical role of representation learning in BEV perception and offer a complementary avenue to conventional task-specific optimizations."
  },
  {
    "title": "Achieving Precise and Reliable Locomotion with Differentiable Simulation-Based System Identification",
    "url": "http://arxiv.org/abs/2508.04696v1",
    "arxiv_id": "2508.04696v1",
    "authors": [
      "Vyacheslav Kovalev",
      "Ekaterina Chaikovskaia",
      "Egor Davydenko",
      "Roman Gorbachev"
    ],
    "published": "2025-08-06T17:57:58+00:00",
    "summary": "Accurate system identification is crucial for reducing trajectory drift in bipedal locomotion, particularly in reinforcement learning and model-based control. In this paper, we present a novel control framework that integrates system identification into the reinforcement learning training loop using differentiable simulation. Unlike traditional approaches that rely on direct torque measurements, our method estimates system parameters using only trajectory data (positions, velocities) and control inputs. We leverage the differentiable simulator MuJoCo-XLA to optimize system parameters, ensuring that simulated robot behavior closely aligns with real-world motion. This framework enables scalable and flexible parameter optimization. Accurate system identification is crucial for reducing trajectory drift in bipedal locomotion, particularly in reinforcement learning and model-based control. In this paper, we present a novel control framework that integrates system identification into the reinforcement learning training loop using differentiable simulation. Unlike traditional approaches that rely on direct torque measurements, our method estimates system parameters using only trajectory data (positions, velocities) and control inputs. We leverage the differentiable simulator MuJoCo-XLA to optimize system parameters, ensuring that simulated robot behavior closely aligns with real-world motion. This framework enables scalable and flexible parameter optimization. It supports fundamental physical properties such as mass and inertia. Additionally, it handles complex system nonlinear behaviors, including advanced friction models, through neural network approximations. Experimental results show that our framework significantly improves trajectory following."
  },
  {
    "title": "From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario",
    "url": "http://arxiv.org/abs/2508.04691v1",
    "arxiv_id": "2508.04691v1",
    "authors": [
      "Yuanchen Bai",
      "Zijian Ding",
      "Shaoyue Wen",
      "Xiang Chang",
      "Angelique Taylor"
    ],
    "published": "2025-08-06T17:54:10+00:00",
    "summary": "Multi-agent robotic systems (MARS) build upon multi-agent systems by integrating physical and task-related constraints, increasing the complexity of action execution and agent coordination. However, despite the availability of advanced multi-agent frameworks, their real-world deployment on robots remains limited, hindering the advancement of MARS research in practice. To bridge this gap, we conducted two studies to investigate performance trade-offs of hierarchical multi-agent frameworks in a simulated real-world multi-robot healthcare scenario. In Study 1, using CrewAI, we iteratively refine the system's knowledge base, to systematically identify and categorize coordination failures (e.g., tool access violations, lack of timely handling of failure reports) not resolvable by providing contextual knowledge alone. In Study 2, using AutoGen, we evaluate a redesigned bidirectional communication structure and further measure the trade-offs between reasoning and non-reasoning models operating within the same robotic team setting. Drawing from our empirical findings, we emphasize the tension between autonomy and stability and the importance of edge-case testing to improve system reliability and safety for future real-world deployment. Supplementary materials, including codes, task agent setup, trace outputs, and annotated examples of coordination failures and reasoning behaviors, are available at: https://byc-sophie.github.io/mas-to-mars/."
  },
  {
    "title": "Open Scene Graphs for Open-World Object-Goal Navigation",
    "url": "http://arxiv.org/abs/2508.04678v1",
    "arxiv_id": "2508.04678v1",
    "authors": [
      "Joel Loo",
      "Zhanxin Wu",
      "David Hsu"
    ],
    "published": "2025-08-06T17:43:29+00:00",
    "summary": "How can we build general-purpose robot systems for open-world semantic navigation, e.g., searching a novel environment for a target object specified in natural language? To tackle this challenge, we introduce OSG Navigator, a modular system composed of foundation models, for open-world Object-Goal Navigation (ObjectNav). Foundation models provide enormous semantic knowledge about the world, but struggle to organise and maintain spatial information effectively at scale. Key to OSG Navigator is the Open Scene Graph representation, which acts as spatial memory for OSG Navigator. It organises spatial information hierarchically using OSG schemas, which are templates, each describing the common structure of a class of environments. OSG schemas can be automatically generated from simple semantic labels of a given environment, e.g., \"home\" or \"supermarket\". They enable OSG Navigator to adapt zero-shot to new environment types. We conducted experiments using both Fetch and Spot robots in simulation and in the real world, showing that OSG Navigator achieves state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot over diverse goals, environments, and robot embodiments."
  },
  {
    "title": "Inequality in the Age of Pseudonymity",
    "url": "http://arxiv.org/abs/2508.04668v1",
    "arxiv_id": "2508.04668v1",
    "authors": [
      "Aviv Yaish",
      "Nir Chemaya",
      "Lin William Cong",
      "Dahlia Malkhi"
    ],
    "published": "2025-08-06T17:36:01+00:00",
    "summary": "Inequality measures such as the Gini coefficient are used to inform and motivate policymaking, and are increasingly applied to digital platforms. We analyze how measures fare in pseudonymous settings, as common to internet-based or blockchain-based platforms. One key challenge that arises is the ability of actors to create multiple fake identities under fictitious false names, also known as ``Sybils.'' While some actors may do so to preserve their privacy, we show that this can inadvertently distort inequality metrics. As we show, when using inequality measures that satisfy literature's canonical set of desired properties, the presence of Sybils in an economy implies that it is impossible to properly measure the economy's inequality. Then, we present several classes of Sybil-proof measures that satisfy relaxed versions of the aforementioned desired properties, and, by fully characterizing them, we prove that the structure imposed restricts their ability to assess inequality at a fine-grained level. In addition, we prove that popular inequality metrics, including the famous Gini coefficient, are vulnerable to Sybil manipulations, and examine the dynamics that result in the creation of Sybils, whether in pseudonymous settings or traditional ones."
  },
  {
    "title": "LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences",
    "url": "http://arxiv.org/abs/2508.03692v1",
    "arxiv_id": "2508.03692v1",
    "authors": [
      "Ao Liang",
      "Youquan Liu",
      "Yu Yang",
      "Dongyue Lu",
      "Linfeng Li",
      "Lingdong Kong",
      "Huaici Zhao",
      "Wei Tsang Ooi"
    ],
    "published": "2025-08-05T17:59:56+00:00",
    "summary": "Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community."
  },
  {
    "title": "PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning",
    "url": "http://arxiv.org/abs/2508.03693v1",
    "arxiv_id": "2508.03693v1",
    "authors": [
      "Ondrej Bajgar",
      "Dewi S. W. Gould",
      "Jonathon Liu",
      "Alessandro Abate",
      "Konstantinos Gatsis",
      "Michael A. Osborne"
    ],
    "published": "2025-08-05T17:59:56+00:00",
    "summary": "As AI systems become increasingly autonomous, reliably aligning their decision-making to human preferences is essential. Inverse reinforcement learning (IRL) offers a promising approach to infer preferences from demonstrations. These preferences can then be used to produce an apprentice policy that performs well on the demonstrated task. However, in domains like autonomous driving or robotics, where errors can have serious consequences, we need not just good average performance but reliable policies with formal guarantees -- yet obtaining sufficient human demonstrations for reliability guarantees can be costly. Active IRL addresses this challenge by strategically selecting the most informative scenarios for human demonstration. We introduce PAC-EIG, an information-theoretic acquisition function that directly targets probably-approximately-correct (PAC) guarantees for the learned policy -- providing the first such theoretical guarantee for active IRL with noisy expert demonstrations. Our method maximises information gain about the regret of the apprentice policy, efficiently identifying states requiring further demonstration. We also present Reward-EIG as an alternative when learning the reward itself is the primary objective. Focusing on finite state-action spaces, we prove convergence bounds, illustrate failure modes of prior heuristic methods, and demonstrate our method's advantages experimentally."
  },
  {
    "title": "La La LiDAR: Large-Scale Layout Generation from LiDAR Data",
    "url": "http://arxiv.org/abs/2508.03691v1",
    "arxiv_id": "2508.03691v1",
    "authors": [
      "Youquan Liu",
      "Lingdong Kong",
      "Weidong Yang",
      "Xin Li",
      "Ao Liang",
      "Runnan Chen",
      "Ben Fei",
      "Tongliang Liu"
    ],
    "published": "2025-08-05T17:59:55+00:00",
    "summary": "Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model (\"La La LiDAR\"), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation."
  },
  {
    "title": "Veila: Panoramic LiDAR Generation from a Monocular RGB Image",
    "url": "http://arxiv.org/abs/2508.03690v1",
    "arxiv_id": "2508.03690v1",
    "authors": [
      "Youquan Liu",
      "Lingdong Kong",
      "Weidong Yang",
      "Ao Liang",
      "Jianxiong Gao",
      "Yang Wu",
      "Xiang Xu",
      "Xin Li",
      "Linfeng Li",
      "Runnan Chen",
      "Ben Fei"
    ],
    "published": "2025-08-05T17:59:53+00:00",
    "summary": "Realistic and controllable panoramic LiDAR data generation is critical for scalable 3D perception in autonomous driving and robotics. Existing methods either perform unconditional generation with poor controllability or adopt text-guided synthesis, which lacks fine-grained spatial control. Leveraging a monocular RGB image as a spatial control signal offers a scalable and low-cost alternative, which remains an open problem. However, it faces three core challenges: (i) semantic and depth cues from RGB are vary spatially, complicating reliable conditioning generation; (ii) modality gaps between RGB appearance and LiDAR geometry amplify alignment errors under noisy diffusion; and (iii) maintaining structural coherence between monocular RGB and panoramic LiDAR is challenging, particularly in non-overlap regions between images and LiDAR. To address these challenges, we propose Veila, a novel conditional diffusion framework that integrates: a Confidence-Aware Conditioning Mechanism (CACM) that strengthens RGB conditioning by adaptively balancing semantic and depth cues according to their local reliability; a Geometric Cross-Modal Alignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a Panoramic Feature Coherence (PFC) for enforcing global structural consistency across monocular RGB and panoramic LiDAR. Additionally, we introduce two metrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to evaluate alignment quality across modalities. Experiments on nuScenes, SemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila achieves state-of-the-art generation fidelity and cross-modal consistency, while enabling generative data augmentation that improves downstream LiDAR semantic segmentation."
  },
  {
    "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward",
    "url": "http://arxiv.org/abs/2508.03686v1",
    "arxiv_id": "2508.03686v1",
    "authors": [
      "Shudong Liu",
      "Hongwei Liu",
      "Junnan Liu",
      "Linchen Xiao",
      "Songyang Gao",
      "Chengqi Lyu",
      "Yuzhe Gu",
      "Wenwei Zhang",
      "Derek F. Wong",
      "Songyang Zhang",
      "Kai Chen"
    ],
    "published": "2025-08-05T17:55:24+00:00",
    "summary": "Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier."
  },
  {
    "title": "MedVLThinker: Simple Baselines for Multimodal Medical Reasoning",
    "url": "http://arxiv.org/abs/2508.02669v1",
    "arxiv_id": "2508.02669v1",
    "authors": [
      "Xiaoke Huang",
      "Juncheng Wu",
      "Hui Liu",
      "Xianfeng Tang",
      "Yuyin Zhou"
    ],
    "published": "2025-08-04T17:59:38+00:00",
    "summary": "Large Reasoning Models (LRMs) have introduced a new paradigm in AI by enabling models to ``think before responding\" via chain-of-thought reasoning. However, the absence of open and reproducible recipes for building reasoning-centric medical LMMs hinders community-wide research, analysis, and comparison. In this paper, we present MedVLThinker, a suite of simple yet strong baselines. Our fully open recipe consists of: (1) systematic data curation for both text-only and image-text medical data, filtered according to varying levels of reasoning difficulty, and (2) two training paradigms: Supervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement Learning with Verifiable Rewards (RLVR) based on final answer correctness. Across extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six medical QA benchmarks, we find that RLVR consistently and significantly outperforms SFT. Additionally, under the RLVR framework, a key, counter-intuitive finding is that training on our curated text-only reasoning data provides a more substantial performance boost than training on multimodal image-text data. Our best open 7B model, trained using the RLVR recipe on text-only data, establishes a new state-of-the-art on existing public VQA benchmarks, surpassing all previous open-source medical LMMs. Furthermore, scaling our model to 32B achieves performance on par with the proprietary GPT-4o. We release all curated data, models, and code to provide the community with a strong, open foundation for future research in multimodal medical reasoning."
  },
  {
    "title": "Manip4Care: Robotic Manipulation of Human Limbs for Solving Assistive Tasks",
    "url": "http://arxiv.org/abs/2508.02649v1",
    "arxiv_id": "2508.02649v1",
    "authors": [
      "Yubin Koh",
      "Ahmed H. Qureshi"
    ],
    "published": "2025-08-04T17:41:58+00:00",
    "summary": "Enabling robots to grasp and reposition human limbs can significantly enhance their ability to provide assistive care to individuals with severe mobility impairments, particularly in tasks such as robot-assisted bed bathing and dressing. However, existing assistive robotics solutions often assume that the human remains static or quasi-static, limiting their effectiveness. To address this issue, we present Manip4Care, a modular simulation pipeline that enables robotic manipulators to grasp and reposition human limbs effectively. Our approach features a physics simulator equipped with built-in techniques for grasping and repositioning while considering biomechanical and collision avoidance constraints. Our grasping method employs antipodal sampling with force closure to grasp limbs, and our repositioning system utilizes the Model Predictive Path Integral (MPPI) and vector-field-based control method to generate motion trajectories under collision avoidance and biomechanical constraints. We evaluate this approach across various limb manipulation tasks in both supine and sitting positions and compare outcomes for different age groups with differing shoulder joint limits. Additionally, we demonstrate our approach for limb manipulation using a real-world mannequin and further showcase its effectiveness in bed bathing tasks."
  },
  {
    "title": "D2PPO: Diffusion Policy Policy Optimization with Dispersive Loss",
    "url": "http://arxiv.org/abs/2508.02644v1",
    "arxiv_id": "2508.02644v1",
    "authors": [
      "Guowei Zou",
      "Weibing Li",
      "Hejun Wu",
      "Yukun Qian",
      "Yuhang Wang",
      "Haitao Wang"
    ],
    "published": "2025-08-04T17:33:41+00:00",
    "summary": "Diffusion policies excel at robotic manipulation by naturally modeling multimodal action distributions in high-dimensional spaces. Nevertheless, diffusion policies suffer from diffusion representation collapse: semantically similar observations are mapped to indistinguishable features, ultimately impairing their ability to handle subtle but critical variations required for complex robotic manipulation. To address this problem, we propose D2PPO (Diffusion Policy Policy Optimization with Dispersive Loss). D2PPO introduces dispersive loss regularization that combats representation collapse by treating all hidden representations within each batch as negative pairs. D2PPO compels the network to learn discriminative representations of similar observations, thereby enabling the policy to identify subtle yet crucial differences necessary for precise manipulation. In evaluation, we find that early-layer regularization benefits simple tasks, while late-layer regularization sharply enhances performance on complex manipulation tasks. On RoboMimic benchmarks, D2PPO achieves an average improvement of 22.7% in pre-training and 26.1% after fine-tuning, setting new SOTA results. In comparison with SOTA, results of real-world experiments on a Franka Emika Panda robot show the excitingly high success rate of our method. The superiority of our method is especially evident in complex tasks. Project page: https://guowei-zou.github.io/d2ppo/"
  },
  {
    "title": "Dam Management in the Era of Climate Change",
    "url": "http://arxiv.org/abs/2508.02636v1",
    "arxiv_id": "2508.02636v1",
    "authors": [
      "Cristina Di Girolami",
      "M'hamed Mrad",
      "Ga\u00efgi",
      "Vathana Ly Vath",
      "Simone Scotti"
    ],
    "published": "2025-08-04T17:22:24+00:00",
    "summary": "Climate change has a dramatic impact, particularly by concentrating rainfall into a few short periods, interspersed by long dry spells. In this context, the role of dams is crucial. We consider the optimal control of a dam, where the water level must not exceed a designated safety threshold, nor fall below a minimum level to ensure functionality and sustainability for for the outgoing river. To model dry spells and intense rainfall events, commonly referred to as water bombs, we introduce a Hawkes process, a well-known example of a self-exciting process characterised by time-correlated intensity, which endogenously reproduces the concentration of events. The problem is formulated as an optimal switching problem with constraints. We establish existence results and propose numerical methods for approximating the solution. Finally, we illustrate the main achievements of this approach through numerical examples. The main and counterintuitive result of our numerical analysis is that the optimal water level inside the dam increases with the self-exciting parameter. This result shows that, when facing the dilemma of managing the opposing risks of dam overtopping and dry spells, the former ultimately dominates the latter. In conclusion, dams will increasingly lose their role as water reserves and take on a greater role in flood protection."
  },
  {
    "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents",
    "url": "http://arxiv.org/abs/2508.02629v1",
    "arxiv_id": "2508.02629v1",
    "authors": [
      "Yibin Liu",
      "Zhixuan Liang",
      "Zanxin Chen",
      "Tianxing Chen",
      "Mengkang Hu",
      "Wanxi Dong",
      "Congsheng Xu",
      "Zhaoming Han",
      "Yusen Qin",
      "Yao Mu"
    ],
    "published": "2025-08-04T17:18:14+00:00",
    "summary": "Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines."
  },
  {
    "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
    "url": "http://arxiv.org/abs/2508.00823v1",
    "arxiv_id": "2508.00823v1",
    "authors": [
      "Wenxuan Guo",
      "Xiuwei Xu",
      "Hang Yin",
      "Ziwei Wang",
      "Jianjiang Feng",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "published": "2025-08-01T17:59:56+00:00",
    "summary": "Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/."
  },
  {
    "title": "Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning",
    "url": "http://arxiv.org/abs/2508.00822v1",
    "arxiv_id": "2508.00822v1",
    "authors": [
      "Alexander Nikitas Dimopoulos",
      "Joseph Grasso"
    ],
    "published": "2025-08-01T17:59:02+00:00",
    "summary": "This study analyzes semantic segmentation performance across heterogeneously labeled point-cloud datasets relevant to public safety applications, including pre-incident planning systems derived from lidar scans. Using NIST's Point Cloud City dataset (Enfield and Memphis collections), we investigate challenges in unifying differently labeled 3D data. Our methodology employs a graded schema with the KPConv architecture, evaluating performance through IoU metrics on safety-relevant features. Results indicate performance variability: geometrically large objects (e.g. stairs, windows) achieve higher segmentation performance, suggesting potential for navigational context, while smaller safety-critical features exhibit lower recognition rates. Performance is impacted by class imbalance and the limited geometric distinction of smaller objects in typical lidar scans, indicating limitations in detecting certain safety-relevant features using current point-cloud methods. Key identified challenges include insufficient labeled data, difficulties in unifying class labels across datasets, and the need for standardization. Potential directions include automated labeling and multi-dataset learning strategies. We conclude that reliable point-cloud semantic segmentation for public safety necessitates standardized annotation protocols and improved labeling techniques to address data heterogeneity and the detection of small, safety-critical elements."
  },
  {
    "title": "Close encounters between periodic light and periodic arrays of quantum emitters",
    "url": "http://arxiv.org/abs/2508.00797v1",
    "arxiv_id": "2508.00797v1",
    "authors": [
      "Frieder Lindel",
      "Carlos J. S\u00e1nchez Mart\u00ednez",
      "Johannes Feist",
      "Francisco J. Garc\u00eda-Vidal"
    ],
    "published": "2025-08-01T17:24:07+00:00",
    "summary": "Periodically structured surfaces (metasurfaces), i.e., periodic light, have evolved as a powerful tool for manipulating electromagnetic fields both in classical and quantum regimes. However, no general approach for quantizing the electromagnetic fields and treating quantum light-matter interactions in such structures exists. Here, we construct an ab initio few-mode quantization scheme for metasurface resonances based on macroscopic quantum electrodynamics. We use our approach to propose a framework for strong light-matter coupling in which collective excitations of periodic arrays of quantum emitters are strongly coupled to the light modes supported by the metasurface, leading to the formation of crystal polaritons. As a proof-of-principle example of their potential, we show that interactions between crystal polaritons can lead to an efficient and directional generation of entangled photon pairs."
  },
  {
    "title": "Video Generators are Robot Policies",
    "url": "http://arxiv.org/abs/2508.00795v1",
    "arxiv_id": "2508.00795v1",
    "authors": [
      "Junbang Liang",
      "Pavel Tokmakov",
      "Ruoshi Liu",
      "Sruthi Sudhakar",
      "Paarth Shah",
      "Rares Ambrus",
      "Carl Vondrick"
    ],
    "published": "2025-08-01T17:23:49+00:00",
    "summary": "Despite tremendous progress in dexterous manipulation, current visuomotor policies remain fundamentally limited by two challenges: they struggle to generalize under perceptual or behavioral distribution shifts, and their performance is constrained by the size of human demonstration data. In this paper, we use video generation as a proxy for robot policy learning to address both limitations simultaneously. We propose Video Policy, a modular framework that combines video and action generation that can be trained end-to-end. Our results demonstrate that learning to generate videos of robot behavior allows for the extraction of policies with minimal demonstration data, significantly improving robustness and sample efficiency. Our method shows strong generalization to unseen objects, backgrounds, and tasks, both in simulation and the real world. We further highlight that task success is closely tied to the generated video, with action-free video data providing critical benefits for generalizing to novel tasks. By leveraging large-scale video generative models, we achieve superior performance compared to traditional behavior cloning, paving the way for more scalable and data-efficient robot policy learning."
  },
  {
    "title": "Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms",
    "url": "http://arxiv.org/abs/2508.00775v1",
    "arxiv_id": "2508.00775v1",
    "authors": [
      "Andrea Martin",
      "Ian R. Manchester",
      "Luca Furieri"
    ],
    "published": "2025-08-01T16:56:42+00:00",
    "summary": "In high-stakes engineering applications, optimization algorithms must come with provable worst-case guarantees over a mathematically defined class of problems. Designing for the worst case, however, inevitably sacrifices performance on the specific problem instances that often occur in practice. We address the problem of augmenting a given linearly convergent algorithm to improve its average-case performance on a restricted set of target problems - for example, tailoring an off-the-shelf solver for model predictive control (MPC) for an application to a specific dynamical system - while preserving its worst-case guarantees across the entire problem class. Toward this goal, we characterize the class of algorithms that achieve linear convergence for classes of nonsmooth composite optimization problems. In particular, starting from a baseline linearly convergent algorithm, we derive all - and only - the modifications to its update rule that maintain its convergence properties. Our results apply to augmenting legacy algorithms such as gradient descent for nonconvex, gradient-dominated functions; Nesterov's accelerated method for strongly convex functions; and projected methods for optimization over polyhedral feasibility sets. We showcase effectiveness of the approach on solving optimization problems with tight iteration budgets in application to ill-conditioned systems of linear equations and MPC for linear systems."
  },
  {
    "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding",
    "url": "http://arxiv.org/abs/2507.23779v1",
    "arxiv_id": "2507.23779v1",
    "authors": [
      "Miaosen Zhang",
      "Ziqiang Xu",
      "Jialiang Zhu",
      "Qi Dai",
      "Kai Qiu",
      "Yifan Yang",
      "Chong Luo",
      "Tianyi Chen",
      "Justin Wagle",
      "Tim Franklin",
      "Baining Guo"
    ],
    "published": "2025-07-31T17:59:09+00:00",
    "summary": "With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65\\% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. % , as a single misclick can result in unacceptable consequences. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the \\textbf{Phi-Ground} model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under $10B$ parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on ScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: \\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}"
  },
  {
    "title": "Half-Physics: Enabling Kinematic 3D Human Model with Physical Interactions",
    "url": "http://arxiv.org/abs/2507.23778v1",
    "arxiv_id": "2507.23778v1",
    "authors": [
      "Li Siyao",
      "Yao Feng",
      "Omid Tehari",
      "Chen Change Loy",
      "Michael J. Black"
    ],
    "published": "2025-07-31T17:58:33+00:00",
    "summary": "While current general-purpose 3D human models (e.g., SMPL-X) efficiently represent accurate human shape and pose, they lacks the ability to physically interact with the environment due to the kinematic nature. As a result, kinematic-based interaction models often suffer from issues such as interpenetration and unrealistic object dynamics. To address this limitation, we introduce a novel approach that embeds SMPL-X into a tangible entity capable of dynamic physical interactions with its surroundings. Specifically, we propose a \"half-physics\" mechanism that transforms 3D kinematic motion into a physics simulation. Our approach maintains kinematic control over inherent SMPL-X poses while ensuring physically plausible interactions with scenes and objects, effectively eliminating penetration and unrealistic object dynamics. Unlike reinforcement learning-based methods, which demand extensive and complex training, our half-physics method is learning-free and generalizes to any body shape and motion; meanwhile, it operates in real time. Moreover, it preserves the fidelity of the original kinematic motion while seamlessly integrating physical interactions"
  },
  {
    "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model",
    "url": "http://arxiv.org/abs/2507.23773v1",
    "arxiv_id": "2507.23773v1",
    "authors": [
      "Mingkai Deng",
      "Jinyu Hou",
      "Yilin Shen",
      "Hongxia Jin",
      "Graham Neubig",
      "Zhiting Hu",
      "Eric Xing"
    ],
    "published": "2025-07-31T17:57:20+00:00",
    "summary": "AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, \\modelname overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural language. Experiments on difficult web browsing tasks show that \\modelname improves the success of flight search from 0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent advantage of up to 124\\% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make SimuRA, a web-browsing agent built on \\modelname with pretrained LLMs, available as a research demo for public testing."
  },
  {
    "title": "Intrinsic Heralding and Optimal Decoders for Non-Abelian Topological Order",
    "url": "http://arxiv.org/abs/2507.23765v1",
    "arxiv_id": "2507.23765v1",
    "authors": [
      "Dian Jing",
      "Pablo Sala",
      "Liang Jiang",
      "Ruben Verresen"
    ],
    "published": "2025-07-31T17:52:03+00:00",
    "summary": "Topological order (TO) provides a natural platform for storing and manipulating quantum information. However, its stability to noise has only been systematically understood for Abelian TOs. In this work, we exploit the non-deterministic fusion of non-Abelian anyons to inform active error correction and design decoders where the fusion products, instead of flag qubits, herald the noise. This intrinsic heralding enhances thresholds over those of Abelian counterparts when noise is dominated by a single non-Abelian anyon type. Furthermore, we present an approach for determining the optimal threshold for non-Abelian TOs with perfect anyon syndromes for any noise model, formulated as a statistical mechanics model using Bayesian inference. We numerically illustrate these results for $D_4 \\cong \\mathbb Z_4 \\rtimes \\mathbb Z_2$ TO. In particular, for non-Abelian charge noise and perfect syndrome measurement, we find an optimal threshold $p_c=0.218(1)$, whereas an intrinsically heralded minimal-weight perfect-matching (MWPM) decoder already gives $p_c=0.20842(2)$, outperforming standard MWPM with $p_c = 0.15860(1)$. Our work highlights how non-Abelian properties can enhance stability, rather than reduce it, and discusses potential generalizations for achieving fault tolerance."
  },
  {
    "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy",
    "url": "http://arxiv.org/abs/2507.23735v1",
    "arxiv_id": "2507.23735v1",
    "authors": [
      "Markus Buchholz",
      "Ignacio Carlucho",
      "Michele Grimaldi",
      "Yvan R. Petillot"
    ],
    "published": "2025-07-31T17:18:55+00:00",
    "summary": "Achieving robust cognitive autonomy in robots navigating complex, unpredictable environments remains a fundamental challenge in robotics. This paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a groundbreaking architecture leveraging distributed Large Language Model AI agents integrated within the Robot Operating System 2 (ROS 2) framework to enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA decentralises cognition into specialised AI agents responsible for multimodal perception, adaptive reasoning, dynamic mission planning, and real-time decision-making. Central innovations include flexible agents dynamically adapting their roles, retrieval-augmented generation utilising vector databases for efficient knowledge management, reinforcement learning-driven behavioural optimisation, and autonomous on-the-fly ROS 2 node generation for runtime functional extensibility. Extensive empirical validation demonstrates UROSA's promising adaptability and reliability through realistic underwater missions in simulation and real-world deployments, showing significant advantages over traditional rule-based architectures in handling unforeseen scenarios, environmental uncertainties, and novel mission objectives. This work not only advances underwater autonomy but also establishes a scalable, safe, and versatile cognitive robotics framework capable of generalising to a diverse array of real-world applications."
  },
  {
    "title": "Viser: Imperative, Web-based 3D Visualization in Python",
    "url": "http://arxiv.org/abs/2507.22885v1",
    "arxiv_id": "2507.22885v1",
    "authors": [
      "Brent Yi",
      "Chung Min Kim",
      "Justin Kerr",
      "Gina Wu",
      "Rebecca Feng",
      "Anthony Zhang",
      "Jonas Kulhanek",
      "Hongsuk Choi",
      "Yi Ma",
      "Matthew Tancik",
      "Angjoo Kanazawa"
    ],
    "published": "2025-07-30T17:59:31+00:00",
    "summary": "We present Viser, a 3D visualization library for computer vision and robotics. Viser aims to bring easy and extensible 3D visualization to Python: we provide a comprehensive set of 3D scene and 2D GUI primitives, which can be used independently with minimal setup or composed to build specialized interfaces. This technical report describes Viser's features, interface, and implementation. Key design choices include an imperative-style API and a web-based viewer, which improve compatibility with modern programming patterns and workflows."
  },
  {
    "title": "Floquet Spin Splitting and Spin Generation in Antiferromagnets",
    "url": "http://arxiv.org/abs/2507.22884v1",
    "arxiv_id": "2507.22884v1",
    "authors": [
      "Bo Li",
      "Ding-Fu Shao",
      "Alexey A. Kovalev"
    ],
    "published": "2025-07-30T17:59:07+00:00",
    "summary": "In antiferromagnetic spintronics, accessing the spin degree of freedom is essential for generating spin currents and manipulating magnetic order, which generally requires lifting spin degeneracy. This is typically achieved through relativistic spin-orbit coupling or non-relativistic spin splitting in altermagnets. Here, we propose an alternative approach: a dynamical spin splitting induced by an optical field in antiferromagnets. By coupling the driven system to a thermal bath, we demonstrate the emergence of steady-state pure spin currents, as well as linear-response longitudinal and transverse spin currents. Crucially, thermal bath engineering allows the generation of a net spin accumulation without relying on spin-orbit coupling. Our results provide a broadly applicable and experimentally tunable route to control spins in antiferromagnets, offering new opportunities for spin generation and manipulation in antiferromagnetic spintronics."
  },
  {
    "title": "A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model",
    "url": "http://arxiv.org/abs/2507.22854v1",
    "arxiv_id": "2507.22854v1",
    "authors": [
      "Andris Ambainis",
      "Joao F. Doriguello",
      "Debbie Lim"
    ],
    "published": "2025-07-30T17:24:23+00:00",
    "summary": "We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a \"simulator\". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like \"optimism in the face of uncertainty\" and \"posterior sampling\" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\\operatorname{poly}\\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces."
  },
  {
    "title": "Repair-R1: Better Test Before Repair",
    "url": "http://arxiv.org/abs/2507.22853v1",
    "arxiv_id": "2507.22853v1",
    "authors": [
      "Haichuan Hu",
      "Xiaochen Xie",
      "Quanjun Zhang"
    ],
    "published": "2025-07-30T17:24:05+00:00",
    "summary": "APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to 48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage by 0.78\\% to 53.96\\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step."
  },
  {
    "title": "Morph: ChirpTransformer-based Encoder-decoder Co-design for Reliable LoRa Communication",
    "url": "http://arxiv.org/abs/2507.22851v1",
    "arxiv_id": "2507.22851v1",
    "authors": [
      "Yidong Ren",
      "Maolin Gan",
      "Chenning Li",
      "Shakhrul Iman Siam",
      "Mi Zhang",
      "Shigang Chen",
      "Zhichao Cao"
    ],
    "published": "2025-07-30T17:22:27+00:00",
    "summary": "In this paper, we propose Morph, a LoRa encoder-decoder co-design to enhance communication reliability while improving its computation efficiency in extremely-low signal-to-noise ratio (SNR) situations. The standard LoRa encoder controls 6 Spreading Factors (SFs) to tradeoff SNR tolerance with data rate. SF-12 is the maximum SF providing the lowest SNR tolerance on commercial off-the-shelf (COTS) LoRa nodes. In Morph, we develop an SF-configuration based encoder to mimic the larger SFs beyond SF-12 while it is compatible with COTS LoRa nodes. Specifically, we manipulate four SF configurations of a Morph symbol to encode 2-bit data. Accordingly, we recognize the used SF configuration of the symbol for data decoding. We leverage a Deep Neural Network (DNN) decoder to fully capture multi-dimensional features among diverse SF configurations to maximize the SNR gain. Moreover, we customize the input size, neural network structure, and training method of the DNN decoder to improve its efficiency, reliability, and generalizability. We implement Morph with COTS LoRa nodes and a USRP N210, then evaluate its performance on indoor and campus-scale testbeds. Results show that we can reliably decode data at -28.8~dB SNR, which is 6.4~dB lower than the standard LoRa with SF-12 chirps. In addition, the computation efficiency of our DNN decoder is about 3x higher than state-of-the-art."
  },
  {
    "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again",
    "url": "http://arxiv.org/abs/2507.22058v1",
    "arxiv_id": "2507.22058v1",
    "authors": [
      "Zigang Geng",
      "Yibing Wang",
      "Yeyao Ma",
      "Chen Li",
      "Yongming Rao",
      "Shuyang Gu",
      "Zhao Zhong",
      "Qinglin Lu",
      "Han Hu",
      "Xiaosong Zhang",
      "Linus",
      "Di Wang",
      "Jie Jiang"
    ],
    "published": "2025-07-29T17:59:04+00:00",
    "summary": "Numerous efforts have been made to extend the ``next token prediction'' paradigm to visual contents, aiming to create a unified approach for both image generation and understanding. Nevertheless, attempts to generate images through autoregressive modeling with discrete tokens have been plagued by issues such as low visual fidelity, distorted outputs, and failure to adhere to complex instructions when rendering intricate details. These shortcomings are likely attributed to cumulative errors during autoregressive inference or information loss incurred during the discretization process. Probably due to this challenge, recent research has increasingly shifted toward jointly training image generation with diffusion objectives and language generation with autoregressive objectives, moving away from unified modeling approaches. In this work, we demonstrate that reinforcement learning can effectively mitigate artifacts and largely enhance the generation quality of a discrete autoregressive modeling method, thereby enabling seamless integration of image and language generation. Our framework comprises a semantic image tokenizer, a unified autoregressive model for both language and images, and an offline diffusion decoder for image generation, termed X-Omni. X-Omni achieves state-of-the-art performance in image generation tasks using a 7B language model, producing images with high aesthetic quality while exhibiting strong capabilities in following instructions and rendering long texts."
  },
  {
    "title": "A Nonlinear MPC Framework for Loco-Manipulation of Quadrupedal Robots with Non-Negligible Manipulator Dynamics",
    "url": "http://arxiv.org/abs/2507.22042v1",
    "arxiv_id": "2507.22042v1",
    "authors": [
      "Ruturaj Sambhus",
      "Kapi Ketan Mehta",
      "Ali MirMohammad Sadeghi",
      "Basit Muhammad Imran",
      "Jeeseop Kim",
      "Taizoon Chunawala",
      "Vittorio Pastore",
      "Sujith Vijayan",
      "Kaveh Akbari Hamed"
    ],
    "published": "2025-07-29T17:47:34+00:00",
    "summary": "Model predictive control (MPC) combined with reduced-order template models has emerged as a powerful tool for trajectory optimization in dynamic legged locomotion. However, loco-manipulation tasks performed by legged robots introduce additional complexity, necessitating computationally efficient MPC algorithms capable of handling high-degree-of-freedom (DoF) models. This letter presents a computationally efficient nonlinear MPC (NMPC) framework tailored for loco-manipulation tasks of quadrupedal robots equipped with robotic manipulators whose dynamics are non-negligible relative to those of the quadruped. The proposed framework adopts a decomposition strategy that couples locomotion template models -- such as the single rigid body (SRB) model -- with a full-order dynamic model of the robotic manipulator for torque-level control. This decomposition enables efficient real-time solution of the NMPC problem in a receding horizon fashion at 60 Hz. The optimal state and input trajectories generated by the NMPC for locomotion are tracked by a low-level nonlinear whole-body controller (WBC) running at 500 Hz, while the optimal torque commands for the manipulator are directly applied. The layered control architecture is validated through extensive numerical simulations and hardware experiments on a 15-kg Unitree Go2 quadrupedal robot augmented with a 4.4-kg 4-DoF Kinova arm. Given that the Kinova arm dynamics are non-negligible relative to the Go2 base, the proposed NMPC framework demonstrates robust stability in performing diverse loco-manipulation tasks, effectively handling external disturbances, payload variations, and uneven terrain."
  },
  {
    "title": "Structure-Informed Deep Reinforcement Learning for Inventory Management",
    "url": "http://arxiv.org/abs/2507.22040v1",
    "arxiv_id": "2507.22040v1",
    "authors": [
      "Alvaro Maggiar",
      "Sohrab Andaz",
      "Akhil Bagaria",
      "Carson Eisenach",
      "Dean Foster",
      "Omer Gottesman",
      "Dominique Perrault-Joncas"
    ],
    "published": "2025-07-29T17:41:45+00:00",
    "summary": "This paper investigates the application of Deep Reinforcement Learning (DRL) to classical inventory management problems, with a focus on practical implementation considerations. We apply a DRL algorithm based on DirectBackprop to several fundamental inventory management scenarios including multi-period systems with lost sales (with and without lead times), perishable inventory management, dual sourcing, and joint inventory procurement and removal. The DRL approach learns policies across products using only historical information that would be available in practice, avoiding unrealistic assumptions about demand distributions or access to distribution parameters. We demonstrate that our generic DRL implementation performs competitively against or outperforms established benchmarks and heuristics across these diverse settings, while requiring minimal parameter tuning. Through examination of the learned policies, we show that the DRL approach naturally captures many known structural properties of optimal policies derived from traditional operations research methods. To further improve policy performance and interpretability, we propose a Structure-Informed Policy Network technique that explicitly incorporates analytically-derived characteristics of optimal policies into the learning process. This approach can help interpretability and add robustness to the policy in out-of-sample performance, as we demonstrate in an example with realistic demand data. Finally, we provide an illustrative application of DRL in a non-stationary setting. Our work bridges the gap between data-driven learning and analytical insights in inventory management while maintaining practical applicability."
  },
  {
    "title": "Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security",
    "url": "http://arxiv.org/abs/2507.22037v1",
    "arxiv_id": "2507.22037v1",
    "authors": [
      "Muzhi Dai",
      "Shixuan Liu",
      "Zhiyuan Zhao",
      "Junyu Gao",
      "Hao Sun",
      "Xuelong Li"
    ],
    "published": "2025-07-29T17:39:48+00:00",
    "summary": "The rapid advancement of multimodal large language models (MLLMs) has led to breakthroughs in various applications, yet their security remains a critical challenge. One pressing issue involves unsafe image-query pairs--jailbreak inputs specifically designed to bypass security constraints and elicit unintended responses from MLLMs. Compared to general multimodal data, such unsafe inputs are relatively sparse, which limits the diversity and richness of training samples available for developing robust defense models. Meanwhile, existing guardrail-type methods rely on external modules to enforce security constraints but fail to address intrinsic vulnerabilities within MLLMs. Traditional supervised fine-tuning (SFT), on the other hand, often over-refuses harmless inputs, compromising general performance. Given these challenges, we propose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack training method to enhance the security of MLLMs. SecTOW consists of two modules: a defender and an auxiliary attacker, both trained iteratively using reinforcement learning (GRPO). During the iterative process, the attacker identifies security vulnerabilities in the defense model and expands jailbreak data. The expanded data are then used to train the defender, enabling it to address identified security vulnerabilities. We also design reward mechanisms used for GRPO to simplify the use of response labels, reducing dependence on complex generative labels and enabling the efficient use of synthetic data. Additionally, a quality monitoring mechanism is used to mitigate the defender's over-refusal of harmless inputs and ensure the diversity of the jailbreak data generated by the attacker. Experimental results on safety-specific and general benchmarks demonstrate that SecTOW significantly improves security while preserving general performance."
  },
  {
    "title": "From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2507.22028v1",
    "arxiv_id": "2507.22028v1",
    "authors": [
      "Honglin He",
      "Yukai Ma",
      "Wayne Wu",
      "Bolei Zhou"
    ],
    "published": "2025-07-29T17:26:10+00:00",
    "summary": "Navigation foundation models trained on massive webscale data enable agents to generalize across diverse environments and embodiments. However, these models trained solely on offline data, often lack the capacity to reason about the consequences of their actions or adapt through counterfactual understanding. They thus face significant limitations in the real-world urban navigation where interactive and safe behaviors, such as avoiding obstacles and moving pedestrians, are critical. To tackle these challenges, we introduce the Seeing-to-Experiencing framework to scale the capability of navigation foundation models with reinforcement learning. S2E combines the strengths of pre-training on videos and post-training through RL. It maintains the generalizability acquired from large-scale real-world videos while enhancing its interactivity through RL in simulation environments. Specifically, we introduce two innovations: an Anchor-Guided Distribution Matching strategy, which stabilizes learning and models diverse motion patterns through anchor-based supervision; and a Residual-Attention Module, which obtains reactive behaviors from simulation environments without erasing the model's pretrained knowledge. Moreover, we establish a comprehensive end-to-end evaluation benchmark, NavBench-GS, built on photorealistic 3DGS reconstructions of real-world scenes that incorporate physical interactions. It can systematically assess the generalizability and safety of navigation foundation models. Extensive experiments show that S2E mitigates the diminishing returns often seen when scaling with offline data alone. We perform a thorough analysis of the benefits of Reinforcement Learning compared to Supervised Fine-Tuning in the context of post-training for robot learning. Our findings emphasize the crucial role of integrating interactive online experiences to effectively scale foundation models in Robotics."
  },
  {
    "title": "Flow Matching Policy Gradients",
    "url": "http://arxiv.org/abs/2507.21053v1",
    "arxiv_id": "2507.21053v1",
    "authors": [
      "David McAllister",
      "Songwei Ge",
      "Brent Yi",
      "Chung Min Kim",
      "Ethan Weber",
      "Hongsuk Choi",
      "Haiwen Feng",
      "Angjoo Kanazawa"
    ],
    "published": "2025-07-28T17:59:57+00:00",
    "summary": "Flow-based generative models, including diffusion models, excel at modeling continuous distributions in high-dimensional spaces. In this work, we introduce Flow Policy Optimization (FPO), a simple on-policy reinforcement learning algorithm that brings flow matching into the policy gradient framework. FPO casts policy optimization as maximizing an advantage-weighted ratio computed from the conditional flow matching loss, in a manner compatible with the popular PPO-clip framework. It sidesteps the need for exact likelihood computation while preserving the generative capabilities of flow-based models. Unlike prior approaches for diffusion-based reinforcement learning that bind training to a specific sampling method, FPO is agnostic to the choice of diffusion or flow integration at both training and inference time. We show that FPO can train diffusion-style policies from scratch in a variety of continuous control tasks. We find that flow-based models can capture multimodal action distributions and achieve higher performance than Gaussian policies, particularly in under-conditioned settings."
  },
  {
    "title": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning",
    "url": "http://arxiv.org/abs/2507.21049v1",
    "arxiv_id": "2507.21049v1",
    "authors": [
      "Zedong Wang",
      "Siyuan Li",
      "Dan Xu"
    ],
    "published": "2025-07-28T17:59:28+00:00",
    "summary": "Despite the promise of Multi-Task Learning in leveraging complementary knowledge across tasks, existing multi-task optimization (MTO) techniques remain fixated on resolving conflicts via optimizer-centric loss scaling and gradient manipulation strategies, yet fail to deliver consistent gains. In this paper, we argue that the shared representation space, where task interactions naturally occur, offers rich information and potential for operations complementary to existing optimizers, especially for facilitating the inter-task complementarity, which is rarely explored in MTO. This intuition leads to Rep-MTL, which exploits the representation-level task saliency to quantify interactions between task-specific optimization and shared representation learning. By steering these saliencies through entropy-based penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate negative transfer by maintaining the effective training of individual tasks instead pure conflict-solving, while explicitly promoting complementary information sharing. Experiments are conducted on four challenging MTL benchmarks covering both task-shift and domain-shift scenarios. The results show that Rep-MTL, even paired with the basic equal weighting policy, achieves competitive performance gains with favorable efficiency. Beyond standard performance metrics, Power Law exponent analysis demonstrates Rep-MTL's efficacy in balancing task-specific learning and cross-task sharing. The project page is available at HERE."
  },
  {
    "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence",
    "url": "http://arxiv.org/abs/2507.21046v1",
    "arxiv_id": "2507.21046v1",
    "authors": [
      "Huan-ang Gao",
      "Jiayi Geng",
      "Wenyue Hua",
      "Mengkang Hu",
      "Xinzhe Juan",
      "Hongzhang Liu",
      "Shilong Liu",
      "Jiahao Qiu",
      "Xuan Qi",
      "Yiran Wu",
      "Hongru Wang",
      "Han Xiao",
      "Yuhang Zhou",
      "Shaokun Zhang",
      "Jiayi Zhang",
      "Jinyu Xiang",
      "Yixiong Fang",
      "Qiwen Zhao",
      "Dongrui Liu",
      "Qihan Ren",
      "Cheng Qian",
      "Zhenghailong Wang",
      "Minda Hu",
      "Huazheng Wang",
      "Qingyun Wu",
      "Heng Ji",
      "Mengdi Wang"
    ],
    "published": "2025-07-28T17:59:05+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks."
  },
  {
    "title": "Quantum Walks on Arbitrary Spatial Networks with Rydberg Atoms",
    "url": "http://arxiv.org/abs/2507.21011v1",
    "arxiv_id": "2507.21011v1",
    "authors": [
      "Gabriel Almeida",
      "Raul Santos",
      "Lara Janiurek",
      "Yasser Omar"
    ],
    "published": "2025-07-28T17:22:53+00:00",
    "summary": "Rydberg atoms provide a highly promising platform for quantum computation, leveraging their strong tunable interactions to encode and manipulate information in the electronic states of individual atoms. Key advantages of Rydberg atoms include scalability, reconfigurable connectivity, and native multi-qubit gates, making them particularly well-suited for addressing complex network problems. These problems can often be framed as graph-based tasks, which can be efficiently addressed using quantum walks. In this work, we propose a general implementation of staggered quantum walks with Rydberg atoms, with a particular focus on spatial networks. We also present an efficient algorithm for constructing the tessellations required for the staggered quantum walk. Finally, we demonstrate that our proposal achieves quadratic speedup in spatial search algorithms."
  },
  {
    "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning",
    "url": "http://arxiv.org/abs/2507.20999v1",
    "arxiv_id": "2507.20999v1",
    "authors": [
      "Yining Huang",
      "Bin Li",
      "Keke Tang",
      "Meilian Chen"
    ],
    "published": "2025-07-28T17:11:26+00:00",
    "summary": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or layer-wise allocation rather than explicitly tailoring data and parameters to different response demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes two distinct modes of thought-System 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we draw an analogy that different \"subregions\" of an LLM's parameters might similarly specialize for tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework that partitions both data and parameters by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically, we classify task data via multi-model role-playing and voting, and partition parameters based on importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show that the two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or surpassing SOTA PEFT baselines."
  },
  {
    "title": "Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts",
    "url": "http://arxiv.org/abs/2507.19477v1",
    "arxiv_id": "2507.19477v1",
    "authors": [
      "Sang-Woo Lee",
      "Sohee Yang",
      "Donghyun Kwak",
      "Noah Y. Siegel"
    ],
    "published": "2025-07-25T17:59:13+00:00",
    "summary": "Many recent papers have studied the development of superforecaster-level event forecasting LLMs. While methodological problems with early studies cast doubt on the use of LLMs for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art LLMs are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting LLMs. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of LLM-based event forecasting training: noisiness-sparsity, knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers' interest in these directions."
  },
  {
    "title": "Efficient Lines Detection for Robot Soccer",
    "url": "http://arxiv.org/abs/2507.19469v1",
    "arxiv_id": "2507.19469v1",
    "authors": [
      "Jo\u00e3o G. Melo",
      "Jo\u00e3o P. Mafaldo",
      "Edna Barros"
    ],
    "published": "2025-07-25T17:54:51+00:00",
    "summary": "Self-localization is essential in robot soccer, where accurate detection of visual field features, such as lines and boundaries, is critical for reliable pose estimation. This paper presents a lightweight and efficient method for detecting soccer field lines using the ELSED algorithm, extended with a classification step that analyzes RGB color transitions to identify lines belonging to the field. We introduce a pipeline based on Particle Swarm Optimization (PSO) for threshold calibration to optimize detection performance, requiring only a small number of annotated samples. Our approach achieves accuracy comparable to a state-of-the-art deep learning model while offering higher processing speed, making it well-suited for real-time applications on low-power robotic platforms."
  },
  {
    "title": "Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization",
    "url": "http://arxiv.org/abs/2507.19459v1",
    "arxiv_id": "2507.19459v1",
    "authors": [
      "Pol Francesch Huc",
      "Emily Bates",
      "Simone D'Amico"
    ],
    "published": "2025-07-25T17:43:29+00:00",
    "summary": "The advent of novel view synthesis techniques such as NeRF and 3D Gaussian Splatting (3DGS) has enabled learning precise 3D models only from posed monocular images. Although these methods are attractive, they hold two major limitations that prevent their use in space applications: they require poses during training, and have high computational cost at training and inference. To address these limitations, this work contributes: (1) a Convolutional Neural Network (CNN) based primitive initializer for 3DGS using monocular images; (2) a pipeline capable of training with noisy or implicit pose estimates; and (3) and analysis of initialization variants that reduce the training cost of precise 3D models. A CNN takes a single image as input and outputs a coarse 3D model represented as an assembly of primitives, along with the target's pose relative to the camera. This assembly of primitives is then used to initialize 3DGS, significantly reducing the number of training iterations and input images needed -- by at least an order of magnitude. For additional flexibility, the CNN component has multiple variants with different pose estimation techniques. This work performs a comparison between these variants, evaluating their effectiveness for downstream 3DGS training under noisy or implicit pose estimates. The results demonstrate that even with imperfect pose supervision, the pipeline is able to learn high-fidelity 3D representations, opening the door for the use of novel view synthesis in space applications."
  },
  {
    "title": "Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints",
    "url": "http://arxiv.org/abs/2507.19458v1",
    "arxiv_id": "2507.19458v1",
    "authors": [
      "Amir Fard",
      "Arnold X. -X. Yuan"
    ],
    "published": "2025-07-25T17:42:34+00:00",
    "summary": "Budget planning and maintenance optimization are crucial for infrastructure asset management, ensuring cost-effectiveness and sustainability. However, the complexity arising from combinatorial action spaces, diverse asset deterioration, stringent budget constraints, and environmental uncertainty significantly limits existing methods' scalability. This paper proposes a Hierarchical Deep Reinforcement Learning methodology specifically tailored to multi-year infrastructure planning. Our approach decomposes the problem into two hierarchical levels: a high-level Budget Planner allocating annual budgets within explicit feasibility bounds, and a low-level Maintenance Planner prioritizing assets within the allocated budget. By structurally separating macro-budget decisions from asset-level prioritization and integrating linear programming projection within a hierarchical Soft Actor-Critic framework, the method efficiently addresses exponential growth in the action space and ensures rigorous budget compliance. A case study evaluating sewer networks of varying sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed approach. Compared to conventional Deep Q-Learning and enhanced genetic algorithms, our methodology converges more rapidly, scales effectively, and consistently delivers near-optimal solutions even as network size grows."
  },
  {
    "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning",
    "url": "http://arxiv.org/abs/2507.19457v1",
    "arxiv_id": "2507.19457v1",
    "authors": [
      "Lakshya A Agrawal",
      "Shangyin Tan",
      "Dilara Soylu",
      "Noah Ziems",
      "Rishi Khare",
      "Krista Opsahl-Ong",
      "Arnav Singhvi",
      "Herumb Shandilya",
      "Michael J Ryan",
      "Meng Jiang",
      "Christopher Potts",
      "Koushik Sen",
      "Alexandros G. Dimakis",
      "Ion Stoica",
      "Dan Klein",
      "Matei Zaharia",
      "Omar Khattab"
    ],
    "published": "2025-07-25T17:42:32+00:00",
    "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization."
  },
  {
    "title": "Layer-Aware Representation Filtering: Purifying Finetuning Data to Preserve LLM Safety Alignment",
    "url": "http://arxiv.org/abs/2507.18631v1",
    "arxiv_id": "2507.18631v1",
    "authors": [
      "Hao Li",
      "Lijun Li",
      "Zhenghao Lu",
      "Xianyi Wei",
      "Rui Li",
      "Jing Shao",
      "Lei Sha"
    ],
    "published": "2025-07-24T17:59:24+00:00",
    "summary": "With rapid advancement and increasing accessibility of LLMs, fine-tuning aligned models has become a critical step for adapting them to real-world applications, which makes the safety of this fine-tuning process more important than ever. However, recent studies have highlighted a critical challenge: even when fine-tuning with seemingly benign downstream datasets, the safety of aligned LLMs can be compromised, making them more susceptible to malicious instructions. In this paper, we show that fine-tuning datasets often contain samples with safety-degrading features that are not easily identifiable on the surface. These samples can significantly degrade the safety alignment of LLMs during fine-tuning. To address this issue, we propose LARF, a \\textbf{L}ayer-\\textbf{A}ware \\textbf{R}epresentation \\textbf{F}iltering method. This method identifies safety-sensitive layers within the LLM and leverages their representations to detect which data samples in the post-training dataset contain safety-degrading features. Experimental results demonstrate that LARF can effectively identify benign data with safety-degrading features. After removing such data, the safety alignment degradation caused by fine-tuning is mitigated. Please see our code at \\href{https://github.com/LLLeoLi/LARF}{https://github.com/LLLeoLi/LARF}."
  },
  {
    "title": "Checklists Are Better Than Reward Models For Aligning Language Models",
    "url": "http://arxiv.org/abs/2507.18624v1",
    "arxiv_id": "2507.18624v1",
    "authors": [
      "Vijay Viswanathan",
      "Yanchao Sun",
      "Shuang Ma",
      "Xiang Kong",
      "Meng Cao",
      "Graham Neubig",
      "Tongshuang Wu"
    ],
    "published": "2025-07-24T17:58:00+00:00",
    "summary": "Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this -- typically using fixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item - using both AI judges and specialized verifier programs - then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods applied to a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only method to improve performance on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. These results establish checklist feedback as a key tool for improving language models' support of queries that express a multitude of needs."
  },
  {
    "title": "Moving Out: Physically-grounded Human-AI Collaboration",
    "url": "http://arxiv.org/abs/2507.18623v1",
    "arxiv_id": "2507.18623v1",
    "authors": [
      "Xuhui Kang",
      "Sung-Wook Lee",
      "Haolin Liu",
      "Yuyan Wang",
      "Yen-Ling Kuo"
    ],
    "published": "2025-07-24T17:57:18+00:00",
    "summary": "The ability to adapt to physical actions and constraints in an environment is crucial for embodied agents (e.g., robots) to effectively collaborate with humans. Such physically grounded human-AI collaboration must account for the increased complexity of the continuous state-action space and constrained dynamics caused by physical constraints. In this paper, we introduce \\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a wide range of collaboration modes affected by physical attributes and constraints, such as moving heavy items together and maintaining consistent actions to move a big item around a corner. Using Moving Out, we designed two tasks and collected human-human interaction data to evaluate models' abilities to adapt to diverse human behaviors and unseen physical attributes. To address the challenges in physical environments, we propose a novel method, BASS (Behavior Augmentation, Simulation, and Selection), to enhance the diversity of agents and their understanding of the outcome of actions. Our experiments show that BASS outperforms state-of-the-art models in AI-AI and human-AI collaboration. The project page is available at \\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\\_ai/}."
  },
  {
    "title": "Hybrid quantum-classical algorithm for near-optimal planning in POMDPs",
    "url": "http://arxiv.org/abs/2507.18606v1",
    "arxiv_id": "2507.18606v1",
    "authors": [
      "Gilberto Cunha",
      "Alexandra Ram\u00f4a",
      "Andr\u00e9 Sequeira",
      "Michael de Oliveira",
      "Lu\u00eds Barbosa"
    ],
    "published": "2025-07-24T17:42:30+00:00",
    "summary": "Reinforcement learning (RL) provides a principled framework for decision-making in partially observable environments, which can be modeled as Markov decision processes and compactly represented through dynamic decision Bayesian networks. Recent advances demonstrate that inference on sparse Bayesian networks can be accelerated using quantum rejection sampling combined with amplitude amplification, leading to a computational speedup in estimating acceptance probabilities.\\\\ Building on this result, we introduce Quantum Bayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead algorithm for model-based RL in partially observable environments. We present a rigorous, oracle-free time complexity analysis under fault-tolerant assumptions for the quantum device. Unlike standard treatments that assume a black-box oracle, we explicitly specify the inference process, allowing our bounds to more accurately reflect the true computational cost. We show that, for environments whose dynamics form a sparse Bayesian network, horizon-based near-optimal planning can be achieved sub-quadratically faster through quantum-enhanced belief updates.   Furthermore, we present numerical experiments benchmarking QBRL against its classical counterpart on simple yet illustrative decision-making tasks. Our results offer a detailed analysis of how the quantum computational advantage translates into decision-making performance, highlighting that the magnitude of the advantage can vary significantly across different deployment settings."
  },
  {
    "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law",
    "url": "http://arxiv.org/abs/2507.18576v1",
    "arxiv_id": "2507.18576v1",
    "authors": [
      "Shanghai AI Lab",
      ":",
      "Yicheng Bao",
      "Guanxu Chen",
      "Mingkang Chen",
      "Yunhao Chen",
      "Chiyu Chen",
      "Lingjie Chen",
      "Sirui Chen",
      "Xinquan Chen",
      "Jie Cheng",
      "Yu Cheng",
      "Dengke Deng",
      "Yizhuo Ding",
      "Dan Ding",
      "Xiaoshan Ding",
      "Yi Ding",
      "Zhichen Dong",
      "Lingxiao Du",
      "Yuyu Fan",
      "Xinshun Feng",
      "Yanwei Fu",
      "Yuxuan Gao",
      "Ruijun Ge",
      "Tianle Gu",
      "Lujun Gui",
      "Jiaxuan Guo",
      "Qianxi He",
      "Yuenan Hou",
      "Xuhao Hu",
      "Hong Huang",
      "Kaichen Huang",
      "Shiyang Huang",
      "Yuxian Jiang",
      "Shanzhe Lei",
      "Jie Li",
      "Lijun Li",
      "Hao Li",
      "Juncheng Li",
      "Xiangtian Li",
      "Yafu Li",
      "Lingyu Li",
      "Xueyan Li",
      "Haotian Liang",
      "Dongrui Liu",
      "Qihua Liu",
      "Zhixuan Liu",
      "Bangwei Liu",
      "Huacan Liu",
      "Yuexiao Liu",
      "Zongkai Liu",
      "Chaochao Lu",
      "Yudong Lu",
      "Xiaoya Lu",
      "Zhenghao Lu",
      "Qitan Lv",
      "Caoyuan Ma",
      "Jiachen Ma",
      "Xiaoya Ma",
      "Zhongtian Ma",
      "Lingyu Meng",
      "Ziqi Miao",
      "Yazhe Niu",
      "Yuezhang Peng",
      "Yuan Pu",
      "Han Qi",
      "Chen Qian",
      "Xingge Qiao",
      "Jingjing Qu",
      "Jiashu Qu",
      "Wanying Qu",
      "Wenwen Qu",
      "Xiaoye Qu",
      "Qihan Ren",
      "Qingnan Ren",
      "Qingyu Ren",
      "Jing Shao",
      "Wenqi Shao",
      "Shuai Shao",
      "Dongxing Shi",
      "Xin Song",
      "Xinhao Song",
      "Yan Teng",
      "Xuan Tong",
      "Yingchun Wang",
      "Xuhong Wang",
      "Shujie Wang",
      "Xin Wang",
      "Yige Wang",
      "Yixu Wang",
      "Yuanfu Wang",
      "Futing Wang",
      "Ruofan Wang",
      "Wenjie Wang",
      "Yajie Wang",
      "Muhao Wei",
      "Xiaoyu Wen",
      "Fenghua Weng",
      "Yuqi Wu",
      "Yingtong Xiong",
      "Xingcheng Xu",
      "Chao Yang",
      "Yue Yang",
      "Yang Yao",
      "Yulei Ye",
      "Zhenyun Yin",
      "Yi Yu",
      "Bo Zhang",
      "Qiaosheng Zhang",
      "Jinxuan Zhang",
      "Yexin Zhang",
      "Yinqiang Zheng",
      "Hefeng Zhou",
      "Zhanhui Zhou",
      "Pengyu Zhu",
      "Qingzi Zhu",
      "Yubo Zhu",
      "Bowen Zhou"
    ],
    "published": "2025-07-24T16:49:19+00:00",
    "summary": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that demonstrates the coevolution of capabilities and safety. It is developed by our proposed SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. Unlike previous alignment methods such as RLHF that simply learn human preferences, SafeLadder enables SafeWork-R1 to develop intrinsic safety reasoning and self-reflection abilities, giving rise to safety `aha' moments. Notably, SafeWork-R1 achieves an average improvement of $46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks without compromising general capabilities, and delivers state-of-the-art safety performance compared to leading proprietary models such as GPT-4.1 and Claude Opus 4. To further bolster its reliability, we implement two distinct inference-time intervention methods and a deliberative search mechanism, enforcing step-level verification. Finally, we further develop SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and capability can co-evolve synergistically, highlighting the generalizability of our framework in building robust, reliable, and trustworthy general-purpose AI."
  },
  {
    "title": "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains",
    "url": "http://arxiv.org/abs/2507.17746v1",
    "arxiv_id": "2507.17746v1",
    "authors": [
      "Anisha Gunjal",
      "Anthony Wang",
      "Elaine Lau",
      "Vaskar Nath",
      "Bing Liu",
      "Sean Hendryx"
    ],
    "published": "2025-07-23T17:57:55+00:00",
    "summary": "Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world tasks often requires balancing objective and subjective evaluation criteria. However, many such tasks lack a single, unambiguous ground truth-making it difficult to define reliable reward signals for post-training language models. While traditional preference-based methods offer a workaround, they rely on opaque reward functions that are difficult to interpret and prone to spurious correlations. We introduce $\\textbf{Rubrics as Rewards}$ (RaR), a framework that uses structured, checklist-style rubrics as interpretable reward signals for on-policy training with GRPO. Our best RaR method yields up to a $28\\%$ relative improvement on HealthBench-1k compared to simple Likert-based approaches, while matching or surpassing the performance of reward signals derived from expert-written references. By treating rubrics as structured reward signals, we show that RaR enables smaller-scale judge models to better align with human preferences and sustain robust performance across model scales."
  },
  {
    "title": "Online Submission and Evaluation System Design for Competition Operations",
    "url": "http://arxiv.org/abs/2507.17730v1",
    "arxiv_id": "2507.17730v1",
    "authors": [
      "Zhe Chen",
      "Daniel Harabor",
      "Ryan Hechnenberger",
      "Nathan R. Sturtevant"
    ],
    "published": "2025-07-23T17:44:10+00:00",
    "summary": "Research communities have developed benchmark datasets across domains to compare the performance of algorithms and techniques However, tracking the progress in these research areas is not easy, as publications appear in different venues at the same time, and many of them claim to represent the state-of-the-art. To address this, research communities often organise periodic competitions to evaluate the performance of various algorithms and techniques, thereby tracking advancements in the field. However, these competitions pose a significant operational burden. The organisers must manage and evaluate a large volume of submissions. Furthermore, participants typically develop their solutions in diverse environments, leading to compatibility issues during the evaluation of their submissions. This paper presents an online competition system that automates the submission and evaluation process for a competition. The competition system allows organisers to manage large numbers of submissions efficiently, utilising isolated environments to evaluate submissions. This system has already been used successfully for several competitions, including the Grid-Based Pathfinding Competition and the League of Robot Runners competition."
  },
  {
    "title": "Megrez2 Technical Report",
    "url": "http://arxiv.org/abs/2507.17728v1",
    "arxiv_id": "2507.17728v1",
    "authors": [
      "Boxun Li",
      "Yadong Li",
      "Zhiyuan Li",
      "Congyi Liu",
      "Weilin Liu",
      "Guowei Niu",
      "Zheyue Tan",
      "Haiyang Xu",
      "Zhuyu Yao",
      "Tao Yuan",
      "Dong Zhou",
      "Yueqing Zhuang",
      "Bo Zhao",
      "Guohao Dai",
      "Yu Wang"
    ],
    "published": "2025-07-23T17:43:07+00:00",
    "summary": "We present Megrez2, a novel lightweight and high-performance language model architecture optimized for device native deployment. Megrez2 introduces a novel cross-layer expert sharing mechanism, which significantly reduces total parameter count by reusing expert modules across adjacent transformer layers while maintaining most of the model's capacity. It also incorporates pre-gated routing, enabling memory-efficient expert loading and faster inference. As the first instantiation of the Megrez2 architecture, we introduce the Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and further enhanced through supervised fine-tuning and reinforcement learning with verifiable rewards. With only 3B activated and 7.5B stored parameters, Megrez2-Preview demonstrates competitive or superior performance compared to larger models on a wide range of tasks, including language understanding, instruction following, mathematical reasoning, and code generation. These results highlight the effectiveness of the Megrez2 architecture to achieve a balance between accuracy, efficiency, and deployability, making it a strong candidate for real-world, resource-constrained applications."
  },
  {
    "title": "CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation",
    "url": "http://arxiv.org/abs/2507.17727v1",
    "arxiv_id": "2507.17727v1",
    "authors": [
      "Robel Mamo",
      "Taeyeong Choi"
    ],
    "published": "2025-07-23T17:41:55+00:00",
    "summary": "State-of-the-art visual under-canopy navigation methods are designed with deep learning-based perception models to distinguish traversable space from crop rows. While these models have demonstrated successful performance, they require large amounts of training data to ensure reliability in real-world field deployment. However, data collection is costly, demanding significant human resources for in-field sampling and annotation. To address this challenge, various data augmentation techniques are commonly employed during model training, such as color jittering, Gaussian blur, and horizontal flip, to diversify training data and enhance model robustness. In this paper, we hypothesize that utilizing only these augmentation techniques may lead to suboptimal performance, particularly in complex under-canopy environments with frequent occlusions, debris, and non-uniform spacing of crops. Instead, we propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut) which masks random regions out in input images that are spatially distributed around crop rows on the sides to encourage trained models to capture high-level contextual features even when fine-grained information is obstructed. Our extensive experiments with a public cornfield dataset demonstrate that masking-based augmentations are effective for simulating occlusions and significantly improving robustness in semantic keypoint predictions for visual navigation. In particular, we show that biasing the mask distribution toward crop rows in CA-Cut is critical for enhancing both prediction accuracy and generalizability across diverse environments achieving up to a 36.9% reduction in prediction error. In addition, we conduct ablation studies to determine the number of masks, the size of each mask, and the spatial distribution of masks to maximize overall performance."
  },
  {
    "title": "BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems",
    "url": "http://arxiv.org/abs/2507.17722v1",
    "arxiv_id": "2507.17722v1",
    "authors": [
      "Malsha Ashani Mahawatta Dona",
      "Beatriz Cabrero-Daniel",
      "Yinan Yu",
      "Christian Berger"
    ],
    "published": "2025-07-23T17:32:17+00:00",
    "summary": "Large language models (LLMs) are growingly extended to process multimodal data such as text and video simultaneously. Their remarkable performance in understanding what is shown in images is surpassing specialized neural networks (NNs) such as Yolo that is supporting only a well-formed but very limited vocabulary, ie., objects that they are able to detect. When being non-restricted, LLMs and in particular state-of-the-art vision language models (VLMs) show impressive performance to describe even complex traffic situations. This is making them potentially suitable components for automotive perception systems to support the understanding of complex traffic situations or edge case situation. However, LLMs and VLMs are prone to hallucination, which mean to either potentially not seeing traffic agents such as vulnerable road users who are present in a situation, or to seeing traffic agents who are not there in reality. While the latter is unwanted making an ADAS or autonomous driving systems (ADS) to unnecessarily slow down, the former could lead to disastrous decisions from an ADS. In our work, we are systematically assessing the performance of 3 state-of-the-art VLMs on a diverse subset of traffic situations sampled from the Waymo Open Dataset to support safety guardrails for capturing such hallucinations in VLM-supported perception systems. We observe that both, proprietary and open VLMs exhibit remarkable image understanding capabilities even paying thorough attention to fine details sometimes difficult to spot for us humans. However, they are also still prone to making up elements in their descriptions to date requiring hallucination detection strategies such as BetterCheck that we propose in our work."
  },
  {
    "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning",
    "url": "http://arxiv.org/abs/2507.16815v1",
    "arxiv_id": "2507.16815v1",
    "authors": [
      "Chi-Pin Huang",
      "Yueh-Hua Wu",
      "Min-Hung Chen",
      "Yu-Chiang Frank Wang",
      "Fu-En Yang"
    ],
    "published": "2025-07-22T17:59:46+00:00",
    "summary": "Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks."
  },
  {
    "title": "Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning",
    "url": "http://arxiv.org/abs/2507.16814v1",
    "arxiv_id": "2507.16814v1",
    "authors": [
      "Junhao Shen",
      "Haiteng Zhao",
      "Yuzhe Gu",
      "Songyang Gao",
      "Kuikun Liu",
      "Haian Huang",
      "Jianfei Gao",
      "Dahua Lin",
      "Wenwei Zhang",
      "Kai Chen"
    ],
    "published": "2025-07-22T17:59:34+00:00",
    "summary": "Enhancing large vision-language models (LVLMs) with visual slow-thinking reasoning is crucial for solving complex multimodal tasks. However, since LVLMs are mainly trained with vision-language alignment, it is difficult to adopt on-policy reinforcement learning (RL) to develop the slow thinking ability because the rollout space is restricted by its initial abilities. Off-policy RL offers a way to go beyond the current policy, but directly distilling trajectories from external models may cause visual hallucinations due to mismatched visual perception abilities across models. To address these issues, this paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for vision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy behavior model by combining on-policy visual understanding from a trainable LVLM with off-policy slow-thinking reasoning from a language model, assigns outcome-based rewards to reasoning, and propagates visual rewards backward. Then LVLM learns slow-thinking reasoning ability from the obtained reasoning trajectories using propagated rewards via off-policy RL algorithms. Extensive experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in average, reaching state-of-the-art performance among open-source LVLMs on multiple multimodal reasoning benchmarks, and even outperforms some closed-source models (e.g., GPT-4.1) on the challenging MathVision and OlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively. Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy RL methods, offering a better policy initialization for further on-policy training."
  },
  {
    "title": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
    "url": "http://arxiv.org/abs/2507.16806v1",
    "arxiv_id": "2507.16806v1",
    "authors": [
      "Mehul Damani",
      "Isha Puri",
      "Stewart Slocum",
      "Idan Shenfeld",
      "Leshem Choshen",
      "Yoon Kim",
      "Jacob Andreas"
    ],
    "published": "2025-07-22T17:56:01+00:00",
    "summary": "When language models (LMs) are trained via reinforcement learning (RL) to generate natural language \"reasoning chains\", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or \"hallucinate\") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models."
  },
  {
    "title": "Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2507.16796v1",
    "arxiv_id": "2507.16796v1",
    "authors": [
      "Mian Ibad Ali Shah",
      "Enda Barrett",
      "Karl Mason"
    ],
    "published": "2025-07-22T17:46:28+00:00",
    "summary": "This paper presents a novel framework for Peer-to-Peer (P2P) energy trading that integrates uncertainty-aware prediction with multi-agent reinforcement learning (MARL), addressing a critical gap in current literature. In contrast to previous works relying on deterministic forecasts, the proposed approach employs a heteroscedastic probabilistic transformer-based prediction model called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify prediction uncertainty, which is essential for robust decision-making in the stochastic environment of P2P energy trading. The KTU model leverages domain-specific features and is trained with a custom loss function that ensures reliable probabilistic forecasts and confidence intervals for each prediction. Integrating these uncertainty-aware forecasts into the MARL framework enables agents to optimize trading strategies with a clear understanding of risk and variability. Experimental results show that the uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to 5.7% without P2P trading and 3.2% with P2P trading, while increasing electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These improvements are even more pronounced when P2P trading is enabled, highlighting the synergy between advanced forecasting and market mechanisms for resilient, economically efficient energy communities."
  },
  {
    "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
    "url": "http://arxiv.org/abs/2507.16784v1",
    "arxiv_id": "2507.16784v1",
    "authors": [
      "Hongyin Luo",
      "Nathaniel Morgan",
      "Tina Li",
      "Derek Zhao",
      "Ai Vy Ngo",
      "Philip Schroeder",
      "Lijie Yang",
      "Assaf Ben-Kish",
      "Jack O'Brien",
      "James Glass"
    ],
    "published": "2025-07-22T17:30:04+00:00",
    "summary": "To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use."
  },
  {
    "title": "Diffusion Beats Autoregressive in Data-Constrained Settings",
    "url": "http://arxiv.org/abs/2507.15857v1",
    "arxiv_id": "2507.15857v1",
    "authors": [
      "Mihir Prabhudesai",
      "Menging Wu",
      "Amir Zadeh",
      "Katerina Fragkiadaki",
      "Deepak Pathak"
    ],
    "published": "2025-07-21T17:59:57+00:00",
    "summary": "Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We interpret this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike AR's fixed left-to-right factorization. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: https://diffusion-scaling.github.io."
  },
  {
    "title": "The Impact of Language Mixing on Bilingual LLM Reasoning",
    "url": "http://arxiv.org/abs/2507.15849v1",
    "arxiv_id": "2507.15849v1",
    "authors": [
      "Yihao Li",
      "Jiayi Xin",
      "Miranda Muqing Miao",
      "Qi Long",
      "Lyle Ungar"
    ],
    "published": "2025-07-21T17:56:09+00:00",
    "summary": "Proficient multilingual speakers often intentionally switch languages in the middle of a conversation. Similarly, recent reasoning-focused bilingual large language models (LLMs) with strong capabilities in both languages exhibit language mixing--alternating languages within their chain of thought. Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy, suggesting that language mixing may benefit reasoning. In this work, we study language switching in Chinese-English bilingual reasoning models. We identify reinforcement learning with verifiable rewards (RLVR) as the critical training stage that leads to language mixing. We demonstrate that language mixing can enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6 percentage points on math reasoning tasks. Additionally, a lightweight probe can be trained to predict whether a potential language switch would benefit or harm reasoning, and when used to guide decoding, increases accuracy by up to 6.25 percentage points. Our findings suggest that language mixing is not merely a byproduct of multilingual training, but is a strategic reasoning behavior."
  },
  {
    "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding",
    "url": "http://arxiv.org/abs/2507.15846v1",
    "arxiv_id": "2507.15846v1",
    "authors": [
      "Fei Tang",
      "Zhangxuan Gu",
      "Zhengxi Lu",
      "Xuyang Liu",
      "Shuheng Shen",
      "Changhua Meng",
      "Wen Wang",
      "Wenqi Zhang",
      "Yongliang Shen",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "published": "2025-07-21T17:53:42+00:00",
    "summary": "Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G$^2$, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks."
  },
  {
    "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning",
    "url": "http://arxiv.org/abs/2507.15844v1",
    "arxiv_id": "2507.15844v1",
    "authors": [
      "Shangke Lyu",
      "Linjuan Wu",
      "Yuchen Yan",
      "Xingyu Wu",
      "Hao Li",
      "Yongliang Shen",
      "Peisheng Jiang",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "published": "2025-07-21T17:52:34+00:00",
    "summary": "Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity."
  },
  {
    "title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers",
    "url": "http://arxiv.org/abs/2507.15833v1",
    "arxiv_id": "2507.15833v1",
    "authors": [
      "Ian Chuang",
      "Andrew Lee",
      "Dechen Gao",
      "Jinyu Zou",
      "Iman Soltani"
    ],
    "published": "2025-07-21T17:44:10+00:00",
    "summary": "Human vision is a highly active process driven by gaze, which directs attention and fixation to task-relevant regions and dramatically reduces visual processing. In contrast, robot learning systems typically rely on passive, uniform processing of raw camera images. In this work, we explore how incorporating human-like active gaze into robotic policies can enhance both efficiency and performance. We build on recent advances in foveated image processing and apply them to an Active Vision robot system that emulates both human head movement and eye tracking. Extending prior work on the AV-ALOHA robot simulation platform, we introduce a framework for simultaneously collecting eye-tracking data and robot demonstrations from a human operator as well as a simulation benchmark and dataset for training robot policies that incorporate human gaze. Given the widespread use of Vision Transformers (ViTs) in robot learning, we integrate gaze information into ViTs using a foveated patch tokenization scheme inspired by recent work in image segmentation. Compared to uniform patch tokenization, this significantly reduces the number of tokens-and thus computation-without sacrificing visual fidelity near regions of interest. We also explore two approaches to gaze imitation and prediction from human data. The first is a two-stage model that predicts gaze to guide foveation and action; the second integrates gaze into the action space, allowing the policy to jointly predict gaze and actions end-to-end. Our results show that our method for foveated robot vision not only drastically reduces computational overhead, but also improves performance for high precision tasks and robustness to unseen distractors. Together, these findings suggest that human-inspired visual processing offers a useful inductive bias for robotic vision systems. https://ian-chuang.github.io/gaze-av-aloha/"
  },
  {
    "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
    "url": "http://arxiv.org/abs/2507.14111v1",
    "arxiv_id": "2507.14111v1",
    "authors": [
      "Xiaoya Li",
      "Xiaofei Sun",
      "Albert Wang",
      "Jiwei Li",
      "Chris Shum"
    ],
    "published": "2025-07-18T17:43:56+00:00",
    "summary": "The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.   CUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.   The capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources."
  },
  {
    "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
    "url": "http://arxiv.org/abs/2507.14107v1",
    "arxiv_id": "2507.14107v1",
    "authors": [
      "Viraj Nishesh Darji",
      "Callie C. Liao",
      "Duoduo Liao"
    ],
    "published": "2025-07-18T17:39:03+00:00",
    "summary": "Bridge maintenance and safety are essential for transportation authorities, and Non-Destructive Evaluation (NDE) techniques are critical to assessing structural integrity. However, interpreting NDE data can be time-consuming and requires expertise, potentially delaying decision-making. Recent advancements in Large Language Models (LLMs) offer new ways to automate and improve this analysis. This pilot study introduces a holistic assessment of LLM capabilities for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in providing detailed bridge condition analyses. It establishes a framework for integrating LLMs into bridge inspection workflows, indicating that LLM-assisted analysis can enhance efficiency without compromising accuracy. In this study, several LLMs are explored with prompts specifically designed to enhance the quality of image descriptions, which are applied to interpret five different NDE contour maps obtained through technologies for assessing bridge conditions. Each LLM model is evaluated based on its ability to produce detailed descriptions, identify defects, provide actionable recommendations, and demonstrate overall accuracy. The research indicates that four of the nine models provide better image descriptions, effectively covering a wide range of topics related to the bridge's condition. The outputs from these four models are summarized using five different LLMs to form a comprehensive overview of the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more effective summaries. The findings suggest that LLMs have the potential to significantly improve efficiency and accuracy. This pilot study presents an innovative approach that leverages LLMs for image captioning in parallel and summarization, enabling faster decision-making in bridge maintenance and enhancing infrastructure management and safety assessments."
  },
  {
    "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
    "url": "http://arxiv.org/abs/2507.14101v1",
    "arxiv_id": "2507.14101v1",
    "authors": [
      "Diego Figueira",
      "Cibele Freire"
    ],
    "published": "2025-07-18T17:30:14+00:00",
    "summary": "We introduce 'project-connex' tree-width as a measure of tractability for counting and aggregate conjunctive queries over semirings with 'group-by' projection (also known as 'AJAR' or 'FAQ' queries). This elementary measure allows to obtain comparable complexity bounds to the ones obtained by previous structural conditions tailored for efficient evaluation of semiring aggregate queries, enumeration algorithms of conjunctive queries, and tractability of counting answers to conjunctive queries.   Project-connex tree decompositions are defined as the natural extension of the known notion of 'free-connex' decompositions. They allow for a unified, simple and intuitive algorithmic manipulation for evaluation of aggregate queries and explain some existing tractability results on conjunctive query enumeration, counting conjunctive query evaluation, and evaluation of semiring aggregate queries. Using this measure we also recover results relating tractable classes of counting conjunctive queries and bounded free-connex tree-width, or the constant-time delay enumeration of semiring aggregate queries over bounded project-connex classes. We further show that project-connex tree decompositions can be obtained via algorithms for computing classical tree decompositions."
  },
  {
    "title": "Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation",
    "url": "http://arxiv.org/abs/2507.14099v1",
    "arxiv_id": "2507.14099v1",
    "authors": [
      "Markus Buchholz",
      "Ignacio Carlucho",
      "Michele Grimaldi",
      "Maria Koskinopoulou",
      "Yvan R. Petillot"
    ],
    "published": "2025-07-18T17:25:54+00:00",
    "summary": "Autonomous motion planning is critical for efficient and safe underwater manipulation in dynamic marine environments. Current motion planning methods often fail to effectively utilize prior motion experiences and adapt to real-time uncertainties inherent in underwater settings. In this paper, we introduce an Adaptive Heuristic Motion Planner framework that integrates a Heuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning for autonomous underwater manipulation. Our approach employs the Probabilistic Roadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite cost function that accounts for distance, uncertainty, energy consumption, and execution time. By leveraging HMS, our framework significantly reduces the search space, thereby boosting computational performance and enabling real-time planning capabilities. Bayesian Networks are utilized to dynamically update uncertainty estimates based on real-time sensor data and environmental conditions, thereby refining the joint probability of path success. Through extensive simulations and real-world test scenarios, we showcase the advantages of our method in terms of enhanced performance and robustness. This probabilistic approach significantly advances the capability of autonomous underwater robots, ensuring optimized motion planning in the face of dynamic marine challenges."
  },
  {
    "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
    "url": "http://arxiv.org/abs/2507.14097v1",
    "arxiv_id": "2507.14097v1",
    "authors": [
      "Hari Iyer",
      "Neel Macwan",
      "Atharva Jitendra Hude",
      "Heejin Jeong",
      "Shenghan Guo"
    ],
    "published": "2025-07-18T17:24:50+00:00",
    "summary": "Human motion simulation (HMS) supports cost-effective evaluation of worker behavior, safety, and productivity in industrial tasks. However, existing methods often suffer from low motion fidelity. This study introduces Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and text-to-motion models to enhance simulation quality for physical tasks. G-AI-HMS tackles two key challenges: (1) translating task descriptions into motion-aware language using Large Language Models aligned with MotionGPT's training vocabulary, and (2) validating AI-enhanced motions against real human movements using computer vision. Posture estimation algorithms are applied to real-time videos to extract joint landmarks, and motion similarity metrics are used to compare them with AI-enhanced sequences. In a case study involving eight tasks, the AI-enhanced motions showed lower error than human created descriptions in most scenarios, performing better in six tasks based on spatial accuracy, four tasks based on alignment after pose normalization, and seven tasks based on overall temporal similarity. Statistical analysis showed that AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and temporal misalignment while retaining comparable posture accuracy."
  },
  {
    "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2507.13348v1",
    "arxiv_id": "2507.13348v1",
    "authors": [
      "Senqiao Yang",
      "Junyi Li",
      "Xin Lai",
      "Bei Yu",
      "Hengshuang Zhao",
      "Jiaya Jia"
    ],
    "published": "2025-07-17T17:59:55+00:00",
    "summary": "Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink."
  },
  {
    "title": "Latent Policy Steering with Embodiment-Agnostic Pretrained World Models",
    "url": "http://arxiv.org/abs/2507.13340v1",
    "arxiv_id": "2507.13340v1",
    "authors": [
      "Yiqi Wang",
      "Mrinal Verghese",
      "Jeff Schneider"
    ],
    "published": "2025-07-17T17:57:57+00:00",
    "summary": "Learning visuomotor policies via imitation has proven effective across a wide range of robotic domains. However, the performance of these policies is heavily dependent on the number of training demonstrations, which requires expensive data collection in the real world. In this work, we aim to reduce data collection efforts when learning visuomotor robot policies by leveraging existing or cost-effective data from a wide range of embodiments, such as public robot datasets and the datasets of humans playing with objects (human data from play). Our approach leverages two key insights. First, we use optic flow as an embodiment-agnostic action representation to train a World Model (WM) across multi-embodiment datasets, and finetune it on a small amount of robot data from the target embodiment. Second, we develop a method, Latent Policy Steering (LPS), to improve the output of a behavior-cloned policy by searching in the latent space of the WM for better action sequences. In real world experiments, we observe significant improvements in the performance of policies trained with a small amount of data (over 50% relative improvement with 30 demonstrations and over 20% relative improvement with 50 demonstrations) by combining the policy with a WM pretrained on two thousand episodes sampled from the existing Open X-embodiment dataset across different robots or a cost-effective human dataset from play."
  },
  {
    "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner",
    "url": "http://arxiv.org/abs/2507.13332v1",
    "arxiv_id": "2507.13332v1",
    "authors": [
      "Zhouqi Hua",
      "Wenwei Zhang",
      "Chengqi Lyu",
      "Yuzhe Gu",
      "Songyang Gao",
      "Kuikun Liu",
      "Kai Chen"
    ],
    "published": "2025-07-17T17:50:07+00:00",
    "summary": "Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data."
  },
  {
    "title": "Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour",
    "url": "http://arxiv.org/abs/2507.13277v1",
    "arxiv_id": "2507.13277v1",
    "authors": [
      "Emma M. A. Harrison"
    ],
    "published": "2025-07-17T16:38:14+00:00",
    "summary": "Robots are increasingly integrated across industries, particularly in healthcare. However, many valuable applications for quadrupedal robots remain overlooked. This research explores the effectiveness of three reinforcement learning algorithms in training a simulated quadruped robot for autonomous navigation and obstacle avoidance. The goal is to develop a robotic guide dog simulation capable of path following and obstacle avoidance, with long-term potential for real-world assistance to guide dogs and visually impaired individuals. It also seeks to expand research into medical 'pets', including robotic guide and alert dogs.   A comparative analysis of thirteen related research papers shaped key evaluation criteria, including collision detection, pathfinding algorithms, sensor usage, robot type, and simulation platforms. The study focuses on sensor inputs, collision frequency, reward signals, and learning progression to determine which algorithm best supports robotic navigation in complex environments.   Custom-made environments were used to ensure fair evaluation of all three algorithms under controlled conditions, allowing consistent data collection. Results show that Proximal Policy Optimization (PPO) outperformed Deep Q-Network (DQN) and Q-learning across all metrics, particularly in average and median steps to goal per episode.   By analysing these results, this study contributes to robotic navigation, AI and medical robotics, offering insights into the feasibility of AI-driven quadruped mobility and its role in assistive robotics."
  },
  {
    "title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation",
    "url": "http://arxiv.org/abs/2507.13266v1",
    "arxiv_id": "2507.13266v1",
    "authors": [
      "Jiazheng Li",
      "Hong Lu",
      "Kaiyue Wen",
      "Zaiwen Yang",
      "Jiaxuan Gao",
      "Hongzhou Lin",
      "Yi Wu",
      "Jingzhao Zhang"
    ],
    "published": "2025-07-17T16:21:47+00:00",
    "summary": "Reinforcement learning (RL) has become a key component in training large language reasoning models (LLMs). However, recent studies questions its effectiveness in improving multi-step reasoning-particularly on hard problems. To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%) on AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical explanations that QuestA improves sample efficiency, offering a practical and generalizable pathway for expanding reasoning capability through RL."
  },
  {
    "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding",
    "url": "http://arxiv.org/abs/2507.12463v1",
    "arxiv_id": "2507.12463v1",
    "authors": [
      "Renjie Li",
      "Ruijie Ye",
      "Mingyang Wu",
      "Hao Frank Yang",
      "Zhiwen Fan",
      "Hezhen Hu",
      "Zhengzhong Tu"
    ],
    "published": "2025-07-16T17:59:30+00:00",
    "summary": "Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behavior$\\unicode{x2014}$such as motion, trajectories, and intention$\\unicode{x2014}$a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasks$\\unicode{x2014}$ranging from motion prediction to motion generation and human behavior question answering$\\unicode{x2014}$thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io."
  },
  {
    "title": "Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios",
    "url": "http://arxiv.org/abs/2507.12449v1",
    "arxiv_id": "2507.12449v1",
    "authors": [
      "Van-Hoang-Anh Phan",
      "Chi-Tam Nguyen",
      "Doan-Trung Au",
      "Thanh-Danh Phan",
      "Minh-Thien Duong",
      "My-Ha Le"
    ],
    "published": "2025-07-16T17:41:14+00:00",
    "summary": "Obstacle avoidance is essential for ensuring the safety of autonomous vehicles. Accurate perception and motion planning are crucial to enabling vehicles to navigate complex environments while avoiding collisions. In this paper, we propose an efficient obstacle avoidance pipeline that leverages a camera-only perception module and a Frenet-Pure Pursuit-based planning strategy. By integrating advancements in computer vision, the system utilizes YOLOv11 for object detection and state-of-the-art monocular depth estimation models, such as Depth Anything V2, to estimate object distances. A comparative analysis of these models provides valuable insights into their accuracy, efficiency, and robustness in real-world conditions. The system is evaluated in diverse scenarios on a university campus, demonstrating its effectiveness in handling various obstacles and enhancing autonomous navigation. The video presenting the results of the obstacle avoidance experiments is available at: https://www.youtube.com/watch?v=FoXiO5S_tA8"
  },
  {
    "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos",
    "url": "http://arxiv.org/abs/2507.12440v1",
    "arxiv_id": "2507.12440v1",
    "authors": [
      "Ruihan Yang",
      "Qinxi Yu",
      "Yecheng Wu",
      "Rui Yan",
      "Borui Li",
      "An-Chieh Cheng",
      "Xueyan Zou",
      "Yunhao Fang",
      "Hongxu Yin",
      "Sifei Liu",
      "Song Han",
      "Yao Lu",
      "Xiaolong Wang"
    ],
    "published": "2025-07-16T17:27:44+00:00",
    "summary": "Real robot data collection for imitation learning has led to significant advancements in robotic manipulation. However, the requirement for robot hardware in the process fundamentally constrains the scale of the data. In this paper, we explore training Vision-Language-Action (VLA) models using egocentric human videos. The benefit of using human videos is not only for their scale but more importantly for the richness of scenes and tasks. With a VLA trained on human video that predicts human wrist and hand actions, we can perform Inverse Kinematics and retargeting to convert the human actions to robot actions. We fine-tune the model using a few robot manipulation demonstrations to obtain the robot policy, namely EgoVLA. We propose a simulation benchmark called Isaac Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation tasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid Manipulation Benchmark and show significant improvements over baselines and ablate the importance of human data. Videos can be found on our website: https://rchalyang.github.io/EgoVLA"
  },
  {
    "title": "Design and Development of an Automated Contact Angle Tester (ACAT) for Surface Wettability Measurement",
    "url": "http://arxiv.org/abs/2507.12431v1",
    "arxiv_id": "2507.12431v1",
    "authors": [
      "Connor Burgess",
      "Kyle Douin",
      "Amir Kordijazi"
    ],
    "published": "2025-07-16T17:20:10+00:00",
    "summary": "The Automated Contact Angle Tester (ACAT) is a fully integrated robotic work cell developed to automate the measurement of surface wettability on 3D-printed materials. Designed for precision, repeatability, and safety, ACAT addresses the limitations of manual contact angle testing by combining programmable robotics, precise liquid dispensing, and a modular software-hardware architecture. The system is composed of three core subsystems: (1) an electrical system including power, control, and safety circuits compliant with industrial standards such as NEC 70, NFPA 79, and UL 508A; (2) a software control system based on a Raspberry Pi and Python, featuring fault detection, GPIO logic, and operator interfaces; and (3) a mechanical system that includes a 3-axis Cartesian robot, pneumatic actuation, and a precision liquid dispenser enclosed within a safety-certified frame. The ACAT enables high-throughput, automated surface characterization and provides a robust platform for future integration into smart manufacturing and materials discovery workflows. This paper details the design methodology, implementation strategies, and system integration required to develop the ACAT platform."
  },
  {
    "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models",
    "url": "http://arxiv.org/abs/2507.12428v1",
    "arxiv_id": "2507.12428v1",
    "authors": [
      "Yik Siu Chan",
      "Zheng-Xin Yong",
      "Stephen H. Bach"
    ],
    "published": "2025-07-16T17:16:03+00:00",
    "summary": "Open-weights reasoning language models generate long chains-of-thought (CoTs) before producing a final response, which improves performance but introduces additional alignment risks, with harmful content often appearing in both the CoTs and the final outputs. In this work, we investigate if we can use CoTs to predict final response misalignment. We evaluate a range of monitoring approaches, including humans, highly-capable large language models, and text classifiers, using either CoT text or activations. First, we find that a simple linear probe trained on CoT activations can significantly outperform all text-based methods in predicting whether a final response will be safe or unsafe. CoT texts are often unfaithful and can mislead humans and classifiers, while model latents (i.e., CoT activations) offer a more reliable predictive signal. Second, the probe makes accurate predictions before reasoning completes, achieving strong performance even when applied to early CoT segments. These findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation."
  },
  {
    "title": "Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation",
    "url": "http://arxiv.org/abs/2507.11540v1",
    "arxiv_id": "2507.11540v1",
    "authors": [
      "Zhen Xu",
      "Hongyu Zhou",
      "Sida Peng",
      "Haotong Lin",
      "Haoyu Guo",
      "Jiahao Shao",
      "Peishan Yang",
      "Qinglin Yang",
      "Sheng Miao",
      "Xingyi He",
      "Yifan Wang",
      "Yue Wang",
      "Ruizhen Hu",
      "Yiyi Liao",
      "Xiaowei Zhou",
      "Hujun Bao"
    ],
    "published": "2025-07-15T17:59:59+00:00",
    "summary": "Depth estimation is a fundamental task in 3D computer vision, crucial for applications such as 3D reconstruction, free-viewpoint rendering, robotics, autonomous driving, and AR/VR technologies. Traditional methods relying on hardware sensors like LiDAR are often limited by high costs, low resolution, and environmental sensitivity, limiting their applicability in real-world scenarios. Recent advances in vision-based methods offer a promising alternative, yet they face challenges in generalization and stability due to either the low-capacity model architectures or the reliance on domain-specific and small-scale datasets. The emergence of scaling laws and foundation models in other domains has inspired the development of \"depth foundation models\": deep neural networks trained on large datasets with strong zero-shot generalization capabilities. This paper surveys the evolution of deep learning architectures and paradigms for depth estimation across the monocular, stereo, multi-view, and monocular video settings. We explore the potential of these models to address existing challenges and provide a comprehensive overview of large-scale datasets that can facilitate their development. By identifying key architectures and training strategies, we aim to highlight the path towards robust depth foundation models, offering insights into their future research and applications."
  },
  {
    "title": "LLM-based ambiguity detection in natural language instructions for collaborative surgical robots",
    "url": "http://arxiv.org/abs/2507.11525v1",
    "arxiv_id": "2507.11525v1",
    "authors": [
      "Ana Davila",
      "Jacinto Colan",
      "Yasuhisa Hasegawa"
    ],
    "published": "2025-07-15T17:53:36+00:00",
    "summary": "Ambiguity in natural language instructions poses significant risks in safety-critical human-robot interaction, particularly in domains such as surgery. To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios. Our method employs an ensemble of LLM evaluators, each configured with distinct prompting techniques to identify linguistic, contextual, procedural, and critical ambiguities. A chain-of-thought evaluator is included to systematically analyze instruction structure for potential issues. Individual evaluator assessments are synthesized through conformal prediction, which yields non-conformity scores based on comparison to a labeled calibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed classification accuracy exceeding 60% in differentiating ambiguous from unambiguous surgical instructions. Our approach improves the safety and reliability of human-robot collaboration in surgery by offering a mechanism to identify potentially ambiguous instructions before robot action."
  },
  {
    "title": "AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air",
    "url": "http://arxiv.org/abs/2507.11515v1",
    "arxiv_id": "2507.11515v1",
    "authors": [
      "Shiyi Yang",
      "Xiaoxue Yu",
      "Rongpeng Li",
      "Jianhang Zhu",
      "Zhifeng Zhao",
      "Honggang Zhang"
    ],
    "published": "2025-07-15T17:36:37+00:00",
    "summary": "Operating Large Language Models (LLMs) on edge devices is increasingly challenged by limited communication bandwidth and strained computational and memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable. Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ fixed or heuristic rank configurations, and the subsequent over-the-air transmission of all LoRA parameters could be rather inefficient. To address this limitation, we develop AirLLM, a hierarchical diffusion policy framework for communication-aware LoRA adaptation. Specifically, AirLLM models the rank configuration as a structured action vector that spans all LoRA-inserted projections. To solve the underlying high-dimensional sequential decision-making problem, a Proximal Policy Optimization (PPO) agent generates coarse-grained decisions by jointly observing wireless states and linguistic complexity, which are then refined via Denoising Diffusion Implicit Models (DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The two modules are optimized alternatively, with the DDIM trained under the Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards. Experiments under varying signal-to-noise ratios demonstrate that AirLLM consistently enhances fine-tuning performance while significantly reducing transmission costs, highlighting the effectiveness of reinforcement-driven, diffusion-refined rank adaptation for scalable and efficient remote fine-tuning over the air."
  },
  {
    "title": "Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming",
    "url": "http://arxiv.org/abs/2507.11498v1",
    "arxiv_id": "2507.11498v1",
    "authors": [
      "Asad Ali Shahid",
      "Francesco Braghin",
      "Loris Roveda"
    ],
    "published": "2025-07-15T17:16:50+00:00",
    "summary": "Humanoid robots have seen remarkable advances in dexterity, balance, and locomotion, yet their role in expressive domains, such as music performance, remains largely unexplored. Musical tasks, like drumming, present unique challenges, including split-second timing, rapid contacts, and multi-limb coordination over pieces lasting minutes. In this paper, we introduce Robot Drummer, a humanoid system capable of expressive, high-precision drumming across a diverse repertoire of songs. We formulate humanoid drumming as sequential fulfillment of timed-contacts and transform drum scores in to a Rhythmic Contact Chain. To handle the long-horizon nature of musical performance, we decompose each piece into fixed-length segments and train a single policy across all segments in parallel using reinforcement learning. Through extensive experiments on over thirty popular rock, metal, and jazz tracks, our results demonstrate that Robot Drummer consistently achieves high F1 scores. The learned behaviors exhibit emergent human-like drumming strategies, such as cross-arm strikes, and adaptive sticks assignments, demonstrating the potential of reinforcement learning to bring humanoid robots into the domain of creative musical performance. Project page: \\href{https://robot-drummer.github.io}{robot-drummer.github.io}"
  },
  {
    "title": "Exploring the robustness of TractOracle methods in RL-based tractography",
    "url": "http://arxiv.org/abs/2507.11486v1",
    "arxiv_id": "2507.11486v1",
    "authors": [
      "Jeremi Levesque",
      "Antoine Th\u00e9berge",
      "Maxime Descoteaux",
      "Pierre-Marc Jodoin"
    ],
    "published": "2025-07-15T16:57:00+00:00",
    "summary": "Tractography algorithms leverage diffusion MRI to reconstruct the fibrous architecture of the brain's white matter. Among machine learning approaches, reinforcement learning (RL) has emerged as a promising framework for tractography, outperforming traditional methods in several key aspects. TractOracle-RL, a recent RL-based approach, reduces false positives by incorporating anatomical priors into the training process via a reward-based mechanism. In this paper, we investigate four extensions of the original TractOracle-RL framework by integrating recent advances in RL, and we evaluate their performance across five diverse diffusion MRI datasets. Results demonstrate that combining an oracle with the RL framework consistently leads to robust and reliable tractography, regardless of the specific method or dataset used. We also introduce a novel RL training scheme called Iterative Reward Training (IRT), inspired by the Reinforcement Learning from Human Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages bundle filtering methods to iteratively refine the oracle's guidance throughout training. Experimental results show that RL methods trained with oracle feedback significantly outperform widely used tractography techniques in terms of accuracy and anatomical validity."
  },
  {
    "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments",
    "url": "http://arxiv.org/abs/2507.10548v1",
    "arxiv_id": "2507.10548v1",
    "authors": [
      "Mingxian Lin",
      "Wei Huang",
      "Yitang Li",
      "Chengjie Jiang",
      "Kui Wu",
      "Fangwei Zhong",
      "Shengju Qian",
      "Xin Wang",
      "Xiaojuan Qi"
    ],
    "published": "2025-07-14T17:59:46+00:00",
    "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities."
  },
  {
    "title": "Disentangling Neural Disjunctive Normal Form Models",
    "url": "http://arxiv.org/abs/2507.10546v1",
    "arxiv_id": "2507.10546v1",
    "authors": [
      "Kexin Gu Baugh",
      "Vincent Perreault",
      "Matthew Baugh",
      "Luke Dickens",
      "Katsumi Inoue",
      "Alessandra Russo"
    ],
    "published": "2025-07-14T17:59:33+00:00",
    "summary": "Neural Disjunctive Normal Form (DNF) based models are powerful and interpretable approaches to neuro-symbolic learning and have shown promising results in classification and reinforcement learning settings without prior knowledge of the tasks. However, their performance is degraded by the thresholding of the post-training symbolic translation process. We show here that part of the performance degradation during translation is due to its failure to disentangle the learned knowledge represented in the form of the networks' weights. We address this issue by proposing a new disentanglement method; by splitting nodes that encode nested rules into smaller independent nodes, we are able to better preserve the models' performance. Through experiments on binary, multiclass, and multilabel classification tasks (including those requiring predicate invention), we demonstrate that our disentanglement method provides compact and interpretable logical representations for the neural DNF-based models, with performance closer to that of their pre-translation counterparts. Our code is available at https://github.com/kittykg/disentangling-ndnf-classification."
  },
  {
    "title": "MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation",
    "url": "http://arxiv.org/abs/2507.10543v1",
    "arxiv_id": "2507.10543v1",
    "authors": [
      "Juyi Sheng",
      "Ziyi Wang",
      "Peiming Li",
      "Mengyuan Liu"
    ],
    "published": "2025-07-14T17:59:08+00:00",
    "summary": "In robot manipulation, robot learning has become a prevailing approach. However, generative models within this field face a fundamental trade-off between the slow, iterative sampling of diffusion models and the architectural constraints of faster Flow-based methods, which often rely on explicit consistency losses. To address these limitations, we introduce MP1, which pairs 3D point-cloud inputs with the MeanFlow paradigm to generate action trajectories in one network function evaluation (1-NFE). By directly learning the interval-averaged velocity via the MeanFlow Identity, our policy avoids any additional consistency constraints. This formulation eliminates numerical ODE-solver errors during inference, yielding more precise trajectories. MP1 further incorporates CFG for improved trajectory controllability while retaining 1-NFE inference without reintroducing structural constraints. Because subtle scene-context variations are critical for robot learning, especially in few-shot learning, we introduce a lightweight Dispersive Loss that repels state embeddings during training, boosting generalization without slowing inference. We validate our method on the Adroit and Meta-World benchmarks, as well as in real-world scenarios. Experimental results show MP1 achieves superior average task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster than FlowPolicy. Our code is available at https://mp1-2254.github.io/."
  },
  {
    "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination",
    "url": "http://arxiv.org/abs/2507.10532v1",
    "arxiv_id": "2507.10532v1",
    "authors": [
      "Mingqi Wu",
      "Zhihao Zhang",
      "Qiaole Dong",
      "Zhiheng Xi",
      "Jun Zhao",
      "Senjie Jin",
      "Xiaoran Fan",
      "Yuhao Zhou",
      "Yanwei Fu",
      "Qin Liu",
      "Songyang Zhang",
      "Qi Zhang"
    ],
    "published": "2025-07-14T17:55:15+00:00",
    "summary": "The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions."
  },
  {
    "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance",
    "url": "http://arxiv.org/abs/2507.10500v1",
    "arxiv_id": "2507.10500v1",
    "authors": [
      "Kyungtae Han",
      "Yitao Chen",
      "Rohit Gupta",
      "Onur Altintas"
    ],
    "published": "2025-07-14T17:24:07+00:00",
    "summary": "While autonomous driving technologies continue to advance, current Advanced Driver Assistance Systems (ADAS) remain limited in their ability to interpret scene context or engage with drivers through natural language. These systems typically rely on predefined logic and lack support for dialogue-based interaction, making them inflexible in dynamic environments or when adapting to driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a modular framework that integrates Generative AI components including large language models, vision-to-text interpretation, and structured function calling to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS supports multi-turn dialogue grounded in visual and sensor context, allowing natural language recommendations and driver-confirmed ADAS control. Implemented in the CARLA simulator with cloud-based Generative AI, the system executes confirmed user intents as structured ADAS commands without requiring model fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and revisited multi-turn interactions, highlighting trade-offs such as increased latency from vision-based context retrieval and token growth from accumulated dialogue history. These results demonstrate the feasibility of combining conversational reasoning, scene perception, and modular ADAS control to support the next generation of intelligent driver assistance."
  },
  {
    "title": "Multi-frequency analysis of the ALMA and VLA high resolution continuum observations of the substructured disc around CI Tau. Preference for sub-mm-sized low-porosity amorphous carbonaceous grains",
    "url": "http://arxiv.org/abs/2507.08797v1",
    "arxiv_id": "2507.08797v1",
    "authors": [
      "Francesco Zagaria",
      "Stefano Facchini",
      "Pietro Curone",
      "Jonathan P. Williams",
      "Cathie J. Clarke",
      "\u00c1lvaro Ribas",
      "Marco Tazzari",
      "Enrique Mac\u00edas",
      "Richard A. Booth",
      "Giovanni P. Rosotti",
      "Leonardo Testi"
    ],
    "published": "2025-07-11T17:58:24+00:00",
    "summary": "(Abridged) We present high angular resolution and sensitivity ALMA 3.1 mm and VLA 9.1 mm observations of the disc around CI Tau. These new data were combined with similar-resolution archival ALMA 0.9 and 1.3 mm observations and new and archival VLA 7.1 mm, 2.0, 3.0, and 6.0 cm photometry to study the properties of dust in this system. At wavelengths <3.1 mm, CI Tau's continuum emission is very extended and highly substructured (with three gaps, four rings, and two additional gap-ring pairs identified by non-parametric visibility modelling). Instead, the VLA 9.1 mm data are dominated by a bright central component, only partially (< 50%) due to dust emission, surrounded by a marginally detected, faint, and smooth halo. We fitted the ALMA and VLA 9.1 mm data together, adopting a physical model that accounts for the effects of dust absorption and scattering. For our fiducial dust composition (\"Ricci\" opacities), we retrieved a flat maximum grain size distribution across the disc radius of $(7.1\\pm0.8)\\times10^{-2}$ cm, that we tentatively attributed to fragmentation of fragile dust or bouncing. We tested, for the first time, the dependence of our results on the adopted dust composition model to assess which mixture can best reproduce the observations. We found that the \"Ricci\" opacities work better than the traditionally adopted \"DSHARP\" ones, while graphite-rich mixtures perform significantly worse. We also show that, for our fiducial composition, the data prefer low-porosity (< 70%) grains, in contrast with claims of highly porous aggregates in younger sources, which we tentatively justified by time-dependent compaction. Our results are in line with constraints from disc population synthesis models and naturally arise from CI Tau's peculiar spectral behaviour, making this disc an ideal target for deeper cm-wavelength and dust polarisation follow-ups."
  },
  {
    "title": "One Token to Fool LLM-as-a-Judge",
    "url": "http://arxiv.org/abs/2507.08794v1",
    "arxiv_id": "2507.08794v1",
    "authors": [
      "Yulai Zhao",
      "Haolin Liu",
      "Dian Yu",
      "S. Y. Kung",
      "Haitao Mi",
      "Dong Yu"
    ],
    "published": "2025-07-11T17:55:22+00:00",
    "summary": "Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning openers like \"Thought process:\" and \"Let's solve this problem step by step.\" can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM."
  },
  {
    "title": "Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning",
    "url": "http://arxiv.org/abs/2507.08793v1",
    "arxiv_id": "2507.08793v1",
    "authors": [
      "James McCarthy",
      "Radu Marinescu",
      "Elizabeth Daly",
      "Ivana Dusparic"
    ],
    "published": "2025-07-11T17:54:54+00:00",
    "summary": "Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies that minimise the likelihood of rare and catastrophic constraint violations caused by an environment's inherent randomness. In general, risk-aversion leads to conservative exploration of the environment which typically results in converging to sub-optimal policies that fail to adequately maximise reward or, in some cases, fail to achieve the goal. In this paper, we propose an exploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic (ORAC), which constructs an exploratory policy by maximising a local upper confidence bound of the state-action reward value function whilst minimising a local lower confidence bound of the risk-averse state-action cost value function. Specifically, at each step, the weighting assigned to the cost value is increased or decreased if it exceeds or falls below the safety constraint value. This way the policy is encouraged to explore uncertain regions of the environment to discover high reward states whilst still satisfying the safety constraints. Our experimental results demonstrate that the ORAC approach prevents convergence to sub-optimal policies and improves significantly the reward-cost trade-off in various continuous control tasks such as Safety-Gymnasium and a complex building energy management environment CityLearn."
  },
  {
    "title": "Angular momentum dynamics of vortex particles in accelerators",
    "url": "http://arxiv.org/abs/2507.08763v1",
    "arxiv_id": "2507.08763v1",
    "authors": [
      "D. Karlovets",
      "D. Grosman",
      "I. Pavlov"
    ],
    "published": "2025-07-11T17:20:08+00:00",
    "summary": "Experiments with spin-polarized beams of leptons and hadrons typically employ plane-wave states with definite momenta and energies. In contrast, vortex states represent cylindrical waves carrying a well-defined orbital angular momentum projection along the propagation direction. This projection can be arbitrarily large, endowing such particles with magnetic moments orders of magnitude greater than those of plane-wave states. Consequently, vortex particles could complement - or even replace - spin-polarized beams in high-energy collisions, enabling access to observables beyond the reach of the conventional states. Although relativistic vortex beams have yet to be realized, we investigate the radiative and non-radiative dynamics of angular momentum for vortex particles in accelerators. We compute the timescale for angular momentum loss via photon emission, finding it significantly longer than typical acceleration times. The non-radiative dynamics is governed by precession, with the orbital angular momentum precessing at a frequency markedly different from that of spin. Similar to spin tunes in circular accelerators, this can induce resonances that disrupt the beam's orbital momentum - occurring far more frequently for vortex beams than for spin-polarized ones. Thus, vortex particle acceleration can be more feasible in linacs, while Siberian snakes could serve as a tool for angular momentum manipulations."
  },
  {
    "title": "Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data",
    "url": "http://arxiv.org/abs/2507.08761v1",
    "arxiv_id": "2507.08761v1",
    "authors": [
      "Jeonghye Kim",
      "Yongjae Shin",
      "Whiyoung Jung",
      "Sunghoon Hong",
      "Deunsol Yoon",
      "Youngchul Sung",
      "Kanghoon Lee",
      "Woohyung Lim"
    ],
    "published": "2025-07-11T17:16:02+00:00",
    "summary": "Reinforcement learning with offline data suffers from Q-value extrapolation errors. To address this issue, we first demonstrate that linear extrapolation of the Q-function beyond the data range is particularly problematic. To mitigate this, we propose guiding the gradual decrease of Q-values outside the data range, which is achieved through reward scaling with layer normalization (RS-LN) and a penalization mechanism for infeasible actions (PA). By combining RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a range of tasks, demonstrating superior performance compared to state-of-the-art algorithms in both offline training and online fine-tuning on the D4RL benchmark, with notable success in the challenging AntMaze Ultra task."
  },
  {
    "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology",
    "url": "http://arxiv.org/abs/2507.07999v1",
    "arxiv_id": "2507.07999v1",
    "authors": [
      "Haochen Wang",
      "Xiangtai Li",
      "Zilong Huang",
      "Anran Wang",
      "Jiacong Wang",
      "Tao Zhang",
      "Jiani Zheng",
      "Sule Bai",
      "Zijian Kang",
      "Jiashi Feng",
      "Zhuochen Wang",
      "Zhaoxiang Zhang"
    ],
    "published": "2025-07-10T17:59:58+00:00",
    "summary": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human \"thinking with images\". However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR."
  },
  {
    "title": "Single-pass Adaptive Image Tokenization for Minimum Program Search",
    "url": "http://arxiv.org/abs/2507.07995v1",
    "arxiv_id": "2507.07995v1",
    "authors": [
      "Shivam Duggal",
      "Sanghyun Byun",
      "William T. Freeman",
      "Antonio Torralba",
      "Phillip Isola"
    ],
    "published": "2025-07-10T17:59:53+00:00",
    "summary": "According to Algorithmic Information Theory (AIT) -- Intelligent representations compress data into the shortest possible program that can reconstruct its content, exhibiting low Kolmogorov Complexity (KC). In contrast, most visual representation learning systems use fixed-length representations for all inputs, ignoring variations in complexity or familiarity. Recent adaptive tokenization methods address this by allocating variable-length representations but typically require test-time search over multiple encodings to find the most predictive one. Inspired by Kolmogorov Complexity principles, we propose a single-pass adaptive tokenizer, KARL, which predicts the appropriate number of tokens for an image in a single forward pass, halting once its approximate KC is reached. The token count serves as a proxy for the minimum description length. KARL's training procedure closely resembles the Upside-Down Reinforcement Learning paradigm, as it learns to conditionally predict token halting based on a desired reconstruction quality. KARL matches the performance of recent adaptive tokenizers while operating in a single pass. We present scaling laws for KARL, analyzing the role of encoder/decoder size, continuous vs. discrete tokenization and more. Additionally, we offer a conceptual study drawing an analogy between Adaptive Image Tokenization and Algorithmic Information Theory, examining the predicted image complexity (KC) across axes such as structure vs. noise and in- vs. out-of-distribution familiarity -- revealing alignment with human intuition."
  },
  {
    "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
    "url": "http://arxiv.org/abs/2507.07996v1",
    "arxiv_id": "2507.07996v1",
    "authors": [
      "Ziyue Li",
      "Yang Li",
      "Tianyi Zhou"
    ],
    "published": "2025-07-10T17:59:53+00:00",
    "summary": "Can a pretrained neural network adapt its architecture to different inputs without any finetuning? Do we need all layers for simple tasks, and are they adequate for challenging tasks? We found that the layers of a pretrained large language model (LLM) can be manipulated as separate modules to build a better and even shallower model customized for each test sample. In particular, each layer from the pretrained model can be skipped/pruned or repeated multiple times as recurrent neural networks (RNN), and stacked with others in arbitrary orders, yielding a chain-of-layers (CoLa) per sample. This compositional space greatly expands the scope of existing works on looped/recurrent pretrained modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree Search (MCTS) protocol to explore and identify the optimal CoLa for each sample from math and commonsense reasoning benchmarks. Compared to a static model of a fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same layer(s) (slow thinking), and combining both, offering more flexible, dynamic architectures for different inputs. We conduct an extensive analysis of the MCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples with correct predictions by the original LLM, we can find shorter CoLa, suggesting a large space for improving inference efficiency; (2) For >60% of samples with originally incorrect predictions, we can identify CoLa achieving correct predictions, suggesting a large space of performance enhancement. Our results highlight the shortcomings of using a fixed architecture of pre-trained LLMs for inference on different samples and pave the way to unlock the generalization power of test-time depth adaptation."
  },
  {
    "title": "Purcell enhancement of photogalvanic currents in a van der Waals plasmonic self-cavity",
    "url": "http://arxiv.org/abs/2507.07987v1",
    "arxiv_id": "2507.07987v1",
    "authors": [
      "Xinyu Li",
      "Jesse Hagelstein",
      "Gunda Kipp",
      "Felix Sturm",
      "Kateryna Kusyak",
      "Yunfei Huang",
      "Benedikt F. Schulte",
      "Alexander M. Potts",
      "Jonathan Stensberg",
      "Victoria Quir\u00f3s-Cordero",
      "Chiara Trovatello",
      "Zhi Hao Peng",
      "Chaowei Hu",
      "Jonathan M. DeStefano",
      "Michael Fechner",
      "Takashi Taniguchi",
      "Kenji Watanabe",
      "P. James Schuck",
      "Xiaodong Xu",
      "Jiun-Haw Chu",
      "Xiaoyang Zhu",
      "Angel Rubio",
      "Marios H. Michael",
      "Matthew W. Day",
      "Hope M. Bretscher",
      "James W. McIver"
    ],
    "published": "2025-07-10T17:58:15+00:00",
    "summary": "Cavities provide a means to manipulate the optical and electronic responses of quantum materials by selectively enhancing light-matter interaction at specific frequencies and momenta. While cavities typically involve external structures, exfoliated flakes of van der Waals (vdW) materials can form intrinsic self-cavities due to their small finite dimensions, confining electromagnetic fields into plasmonic cavity modes, characterized by standing-wave current distributions. While cavity-enhanced phenomena are well-studied at optical frequencies, the impact of self-cavities on nonlinear electronic responses--such as photogalvanic currents--remains largely unexplored, particularly in the terahertz regime, critical for emerging ultrafast optoelectronic technologies. Here, we report a self-cavity-induced Purcell enhancement of photogalvanic currents in the vdW semimetal WTe$_2$. Using ultrafast optoelectronic circuitry, we measured coherent near-field THz emission resulting from nonlinear photocurrents excited at the sample edges. We observed enhanced emission at finite frequencies, tunable via excitation fluence and sample geometry, which we attribute to plasmonic interference effects controlled by the cavity boundaries. We developed an analytical theory that captures the cavity resonance conditions and spectral response across multiple devices. Our findings establish WTe$_2$ as a bias-free, geometry-tunable THz emitter and demonstrate the potential of self-cavity engineering for controlling nonlinear, nonequilibrium dynamics in quantum materials."
  },
  {
    "title": "EXPO: Stable Reinforcement Learning with Expressive Policies",
    "url": "http://arxiv.org/abs/2507.07986v1",
    "arxiv_id": "2507.07986v1",
    "authors": [
      "Perry Dong",
      "Qiyang Li",
      "Dorsa Sadigh",
      "Chelsea Finn"
    ],
    "published": "2025-07-10T17:57:46+00:00",
    "summary": "We study the problem of training and fine-tuning expressive policies with online reinforcement learning (RL) given an offline dataset. Training expressive policy classes with online RL present a unique challenge of stable value maximization. Unlike simpler Gaussian policies commonly used in online RL, expressive policies like diffusion and flow-matching policies are parameterized by a long denoising chain, which hinders stable gradient propagation from actions to policy parameters when optimizing against some value function. Our key insight is that we can address stable value maximization by avoiding direct optimization over value with the expressive policy and instead construct an on-the-fly RL policy to maximize Q-value. We propose Expressive Policy Optimization (EXPO), a sample-efficient online RL algorithm that utilizes an on-the-fly policy to maximize value with two parameterized policies -- a larger expressive base policy trained with a stable imitation learning objective and a light-weight Gaussian edit policy that edits the actions sampled from the base policy toward a higher value distribution. The on-the-fly policy optimizes the actions from the base policy with the learned edit policy and chooses the value maximizing action from the base and edited actions for both sampling and temporal-difference (TD) backup. Our approach yields up to 2-3x improvement in sample efficiency on average over prior methods both in the setting of fine-tuning a pretrained policy given offline data and in leveraging offline data to train online."
  },
  {
    "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
    "url": "http://arxiv.org/abs/2507.07095v1",
    "arxiv_id": "2507.07095v1",
    "authors": [
      "Ke Fan",
      "Shunlin Lu",
      "Minyue Dai",
      "Runyi Yu",
      "Lixing Xiao",
      "Zhiyang Dou",
      "Junting Dong",
      "Lizhuang Ma",
      "Jingbo Wang"
    ],
    "published": "2025-07-09T17:52:04+00:00",
    "summary": "Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion-the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation. The code is available at https://github.com/VankouF/MotionMillion-Codes."
  },
  {
    "title": "Graph-Based Complexity Metrics for Multi-Agent Curriculum Learning: A Validated Approach to Task Ordering in Cooperative Coordination Environments",
    "url": "http://arxiv.org/abs/2507.07074v1",
    "arxiv_id": "2507.07074v1",
    "authors": [
      "Farhaan Ebadulla",
      "Dharini Hindlatti",
      "Srinivaasan NS",
      "Apoorva VH",
      "Ayman Aftab"
    ],
    "published": "2025-07-09T17:31:35+00:00",
    "summary": "Multi-agent reinforcement learning (MARL) faces significant challenges in task sequencing and curriculum design, particularly for cooperative coordination scenarios. While curriculum learning has demonstrated success in single-agent domains, principled approaches for multi-agent coordination remain limited due to the absence of validated task complexity metrics. This approach presents a graph-based coordination complexity metric that integrates agent dependency entropy, spatial interference patterns, and goal overlap analysis to predict task difficulty in multi-agent environments. The complexity metric achieves strong empirical validation with rho = 0.952 correlation (p < 0.001) between predicted complexity and empirical difficulty determined by random agent performance evaluation. This approach evaluates the curriculum learning framework using MADDPG across two distinct coordination environments: achieving 56x performance improvement in tight coordination tasks (MultiWalker) and demonstrating systematic task progression in cooperative navigation (Simple Spread). Through systematic analysis, coordination tightness emerges as a predictor of curriculum learning effectiveness, where environments requiring strict agent interdependence benefit substantially from structured progression. This approach provides a validated complexity metric for multi-agent curriculum design and establishes empirical guidelines for multi-robot coordination applications."
  },
  {
    "title": "Emergent Multiferroic Altermagnets and Spin Control via Noncollinear Molecular Polarization",
    "url": "http://arxiv.org/abs/2507.07039v1",
    "arxiv_id": "2507.07039v1",
    "authors": [
      "Ziye Zhu",
      "Yuntian Liu",
      "Xunkai Duan",
      "Jiayong Zhang",
      "Bowen Hao",
      "Su-Huai Wei",
      "Igor Zutic",
      "Tong Zhou"
    ],
    "published": "2025-07-09T17:05:24+00:00",
    "summary": "Altermagnets, with spin splitting and vanishing magnetization, have been attributed to many fascinating phenomena and potential applications. In particular, integrating ferroelectricity with altermagnetism to enable magnetoelectric coupling and electric control of spin has drawn significant attention. However, its experimental realization and precise spin manipulation remain elusive. Here, by focusing on molecular ferroelectrics, the first discovered ferroelectrics renowned for their highly controllable molecular polarizations and structural flexibility, we reveal that these obstacles can be removed by an emergent multiferroic altermagnets with tunable spin polarization in a large class of fabricated organic materials. Using a symmetry-based design and a tight-binding model, we uncover the underlying mechanism of such molecular ferroelectric altermagnets and demonstrate how noncollinear molecular polarization can switch the spin polarization on and off and even reverse its sign. From the first-principles calculations, we verify the feasibility of these materials in a series of well-established hybrid organic-inorganic perovskites and metal-organic frameworks. Our findings bridge molecular ferroelectrics and altermagnetic spintronics, highlighting an unexplored potential of multifunctional organic multiferroics."
  },
  {
    "title": "First Return, Entropy-Eliciting Explore",
    "url": "http://arxiv.org/abs/2507.07017v1",
    "arxiv_id": "2507.07017v1",
    "authors": [
      "Tianyu Zheng",
      "Tianshun Xing",
      "Qingshui Gu",
      "Taoran Liang",
      "Xingwei Qu",
      "Xin Zhou",
      "Yizhi Li",
      "Zhoufutu Wen",
      "Chenghua Lin",
      "Wenhao Huang",
      "Qian Liu",
      "Ge Zhang",
      "Zejun Ma"
    ],
    "published": "2025-07-09T16:45:48+00:00",
    "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Large Language Models (LLMs) but it struggles with unstable exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework that identifies high-uncertainty decision points in reasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback. Our method provides targeted guidance without relying on dense supervision. Empirical results on mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable training, produces longer and more coherent responses, and increases the proportion of fully correct trajectories. These results highlight the framework's effectiveness in improving LLM reasoning through more robust and structured exploration."
  },
  {
    "title": "When Context Is Not Enough: Modeling Unexplained Variability in Car-Following Behavior",
    "url": "http://arxiv.org/abs/2507.07012v1",
    "arxiv_id": "2507.07012v1",
    "authors": [
      "Chengyuan Zhang",
      "Zhengbing He",
      "Cathy Wu",
      "Lijun Sun"
    ],
    "published": "2025-07-09T16:42:41+00:00",
    "summary": "Modeling car-following behavior is fundamental to microscopic traffic simulation, yet traditional deterministic models often fail to capture the full extent of variability and unpredictability in human driving. While many modern approaches incorporate context-aware inputs (e.g., spacing, speed, relative speed), they frequently overlook structured stochasticity that arises from latent driver intentions, perception errors, and memory effects -- factors that are not directly observable from context alone. To fill the gap, this study introduces an interpretable stochastic modeling framework that captures not only context-dependent dynamics but also residual variability beyond what context can explain. Leveraging deep neural networks integrated with nonstationary Gaussian processes (GPs), our model employs a scenario-adaptive Gibbs kernel to learn dynamic temporal correlations in acceleration decisions, where the strength and duration of correlations between acceleration decisions evolve with the driving context. This formulation enables a principled, data-driven quantification of uncertainty in acceleration, speed, and spacing, grounded in both observable context and latent behavioral variability. Comprehensive experiments on the naturalistic vehicle trajectory dataset collected from the German highway, i.e., the HighD dataset, demonstrate that the proposed stochastic simulation method within this framework surpasses conventional methods in both predictive performance and interpretable uncertainty quantification. The integration of interpretability and accuracy makes this framework a promising tool for traffic analysis and safety-critical applications."
  },
  {
    "title": "EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow",
    "url": "http://arxiv.org/abs/2507.06224v1",
    "arxiv_id": "2507.06224v1",
    "authors": [
      "Yixiang Chen",
      "Peiyan Li",
      "Yan Huang",
      "Jiabing Yang",
      "Kehan Chen",
      "Liang Wang"
    ],
    "published": "2025-07-08T17:57:03+00:00",
    "summary": "Current language-guided robotic manipulation systems often require low-level action-labeled datasets for imitation learning. While object-centric flow prediction methods mitigate this issue, they remain limited to scenarios involving rigid objects with clear displacement and minimal occlusion. In this work, we present Embodiment-Centric Flow (EC-Flow), a framework that directly learns manipulation from action-unlabeled videos by predicting embodiment-centric flow. Our key insight is that incorporating the embodiment's inherent kinematics significantly enhances generalization to versatile manipulation scenarios, including deformable object handling, occlusions, and non-object-displacement tasks. To connect the EC-Flow with language instructions and object interactions, we further introduce a goal-alignment module by jointly optimizing movement consistency and goal-image prediction. Moreover, translating EC-Flow to executable robot actions only requires a standard robot URDF (Unified Robot Description Format) file to specify kinematic constraints across joints, which makes it easy to use in practice. We validate EC-Flow on both simulation (Meta-World) and real-world tasks, demonstrating its state-of-the-art performance in occluded object handling (62% improvement), deformable object manipulation (45% improvement), and non-object-displacement tasks (80% improvement) than prior state-of-the-art object-centric flow methods. For more information, see our project website at https://ec-flow1.github.io ."
  },
  {
    "title": "Is Diversity All You Need for Scalable Robotic Manipulation?",
    "url": "http://arxiv.org/abs/2507.06219v1",
    "arxiv_id": "2507.06219v1",
    "authors": [
      "Modi Shi",
      "Li Chen",
      "Jin Chen",
      "Yuxiang Lu",
      "Chiming Liu",
      "Guanghui Ren",
      "Ping Luo",
      "Di Huang",
      "Maoqing Yao",
      "Hongyang Li"
    ],
    "published": "2025-07-08T17:52:44+00:00",
    "summary": "Data scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensions-task (what to do), embodiment (which robot to use), and expert (who demonstrates)-challenging the conventional intuition of \"more diverse is better\". Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for cross-embodiment transfer-models trained on high-quality single-embodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as a key contributing factor. Based on this insight, we propose a distribution debiasing method to mitigate velocity ambiguity, the yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to using 2.5 times pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively."
  },
  {
    "title": "UQLM: A Python Package for Uncertainty Quantification in Large Language Models",
    "url": "http://arxiv.org/abs/2507.06196v1",
    "arxiv_id": "2507.06196v1",
    "authors": [
      "Dylan Bouchard",
      "Mohit Singh Chauhan",
      "David Skarbrevik",
      "Ho-Kyeong Ra",
      "Viren Bajaj",
      "Zeya Ahmad"
    ],
    "published": "2025-07-08T17:22:32+00:00",
    "summary": "Hallucinations, defined as instances where Large Language Models (LLMs) generate false or misleading content, pose a significant challenge that impacts the safety and trust of downstream applications. We introduce UQLM, a Python package for LLM hallucination detection using state-of-the-art uncertainty quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers that compute response-level confidence scores ranging from 0 to 1. This library provides an off-the-shelf solution for UQ-based hallucination detection that can be easily integrated to enhance the reliability of LLM outputs."
  },
  {
    "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review",
    "url": "http://arxiv.org/abs/2507.06185v1",
    "arxiv_id": "2507.06185v1",
    "authors": [
      "Zhicheng Lin"
    ],
    "published": "2025-07-08T17:11:13+00:00",
    "summary": "In July 2025, 18 academic manuscripts on the preprint website arXiv were found to contain hidden instructions known as prompts designed to manipulate AI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\" were concealed using techniques like white-colored text. Author responses varied: one planned to withdraw the affected paper, while another defended the practice as legitimate testing of reviewer compliance. This commentary analyzes this practice as a novel form of research misconduct. We examine the technique of prompt injection in large language models (LLMs), revealing four types of hidden prompts, ranging from simple positive review commands to detailed evaluation frameworks. The defense that prompts served as \"honeypots\" to detect reviewers improperly using AI fails under examination--the consistently self-serving nature of prompt instructions indicates intent to manipulate. Publishers maintain inconsistent policies: Elsevier prohibits AI use in peer review entirely, while Springer Nature permits limited use with disclosure requirements. The incident exposes systematic vulnerabilities extending beyond peer review to any automated system processing scholarly texts, including plagiarism detection and citation indexing. Our analysis underscores the need for coordinated technical screening at submission portals and harmonized policies governing generative AI (GenAI) use in academic evaluation."
  },
  {
    "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization",
    "url": "http://arxiv.org/abs/2507.06181v1",
    "arxiv_id": "2507.06181v1",
    "authors": [
      "Zhongyuan Peng",
      "Yifan Yao",
      "Kaijing Ma",
      "Shuyue Guo",
      "Yizhe Li",
      "Yichi Zhang",
      "Chenchen Zhang",
      "Yifan Zhang",
      "Zhouliang Yu",
      "Luming Li",
      "Minghao Liu",
      "Yihang Xia",
      "Jiawei Shen",
      "Yuchen Wu",
      "Yixin Cao",
      "Zhaoxiang Zhang",
      "Wenhao Huang",
      "Jiaheng Liu",
      "Ge Zhang"
    ],
    "published": "2025-07-08T17:03:39+00:00",
    "summary": "Translating natural language mathematical statements into formal, executable code is a fundamental challenge in automated theorem proving. While prior work has focused on generation and compilation success, little attention has been paid to the critic phase-the evaluation of whether generated formalizations truly capture the semantic intent of the original problem. In this paper, we introduce CriticLean, a novel critic-guided reinforcement learning framework that elevates the role of the critic from a passive validator to an active learning component. Specifically, first, we propose the CriticLeanGPT, trained via supervised fine-tuning and reinforcement learning, to rigorously assess the semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench, a benchmark designed to measure models' ability to distinguish semantically correct from incorrect formalizations, and demonstrate that our trained CriticLeanGPT models can significantly outperform strong open- and closed-source baselines. Building on the CriticLean framework, we construct FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich domain diversity, broad difficulty coverage, and high correctness based on human evaluation. Overall, our findings highlight that optimizing the critic phase is essential for producing reliable formalizations, and we hope our CriticLean will provide valuable insights for future advances in formal mathematical reasoning."
  },
  {
    "title": "Beyond One Shot, Beyond One Perspective: Cross-View and Long-Horizon Distillation for Better LiDAR Representations",
    "url": "http://arxiv.org/abs/2507.05260v1",
    "arxiv_id": "2507.05260v1",
    "authors": [
      "Xiang Xu",
      "Lingdong Kong",
      "Song Wang",
      "Chuanwei Zhou",
      "Qingshan Liu"
    ],
    "published": "2025-07-07T17:59:58+00:00",
    "summary": "LiDAR representation learning aims to extract rich structural and semantic information from large-scale, readily available datasets, reducing reliance on costly human annotations. However, existing LiDAR representation strategies often overlook the inherent spatiotemporal cues in LiDAR sequences, limiting their effectiveness. In this work, we propose LiMA, a novel long-term image-to-LiDAR Memory Aggregation framework that explicitly captures longer range temporal correlations to enhance LiDAR representation learning. LiMA comprises three key components: 1) a Cross-View Aggregation module that aligns and fuses overlapping regions across neighboring camera views, constructing a more unified and redundancy-free memory bank; 2) a Long-Term Feature Propagation mechanism that efficiently aligns and integrates multi-frame image features, reinforcing temporal coherence during LiDAR representation learning; and 3) a Cross-Sequence Memory Alignment strategy that enforces consistency across driving sequences, improving generalization to unseen environments. LiMA maintains high pretraining efficiency and incurs no additional computational overhead during downstream tasks. Extensive experiments on mainstream LiDAR-based perception benchmarks demonstrate that LiMA significantly improves both LiDAR semantic segmentation and 3D object detection. We hope this work inspires more effective pretraining paradigms for autonomous driving. The code has be made publicly accessible for future research."
  },
  {
    "title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning",
    "url": "http://arxiv.org/abs/2507.05255v1",
    "arxiv_id": "2507.05255v1",
    "authors": [
      "Yana Wei",
      "Liang Zhao",
      "Jianjian Sun",
      "Kangheng Lin",
      "Jisheng Yin",
      "Jingcheng Hu",
      "Yinmin Zhang",
      "En Yu",
      "Haoran Lv",
      "Zejia Weng",
      "Jia Wang",
      "Chunrui Han",
      "Yuang Peng",
      "Qi Han",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Daxin Jiang",
      "Vishal M. Patel"
    ],
    "published": "2025-07-07T17:59:03+00:00",
    "summary": "The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps, surpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners."
  },
  {
    "title": "From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving",
    "url": "http://arxiv.org/abs/2507.05254v1",
    "arxiv_id": "2507.05254v1",
    "authors": [
      "Fabian Konstantinidis",
      "Ariel Dallari Guerreiro",
      "Raphael Trumpp",
      "Moritz Sackmann",
      "Ulrich Hofmann",
      "Marco Caccamo",
      "Christoph Stiller"
    ],
    "published": "2025-07-07T17:58:53+00:00",
    "summary": "Accurate motion prediction of surrounding traffic participants is crucial for the safe and efficient operation of automated vehicles in dynamic environments. Marginal prediction models commonly forecast each agent's future trajectories independently, often leading to sub-optimal planning decisions for an automated vehicle. In contrast, joint prediction models explicitly account for the interactions between agents, yielding socially and physically consistent predictions on a scene level. However, existing approaches differ not only in their problem formulation but also in the model architectures and implementation details used, making it difficult to compare them. In this work, we systematically investigate different approaches to joint motion prediction, including post-processing of the marginal predictions, explicitly training the model for joint predictions, and framing the problem as a generative task. We evaluate each approach in terms of prediction accuracy, multi-modality, and inference efficiency, offering a comprehensive analysis of the strengths and limitations of each approach. Several prediction examples are available at https://frommarginaltojointpred.github.io/."
  },
  {
    "title": "Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving",
    "url": "http://arxiv.org/abs/2507.05251v1",
    "arxiv_id": "2507.05251v1",
    "authors": [
      "Elahe Delavari",
      "Feeza Khan Khanzada",
      "Jaerock Kwon"
    ],
    "published": "2025-07-07T17:58:08+00:00",
    "summary": "Reinforcement Learning (RL) offers a promising framework for autonomous driving by enabling agents to learn control policies through interaction with environments. However, large and high-dimensional action spaces often used to support fine-grained control can impede training efficiency and increase exploration costs. In this study, we introduce and evaluate two novel structured action space modification strategies for RL in autonomous driving: dynamic masking and relative action space reduction. These approaches are systematically compared against fixed reduction schemes and full action space baselines to assess their impact on policy learning and performance. Our framework leverages a multimodal Proximal Policy Optimization agent that processes both semantic image sequences and scalar vehicle states. The proposed dynamic and relative strategies incorporate real-time action masking based on context and state transitions, preserving action consistency while eliminating invalid or suboptimal choices. Through comprehensive experiments across diverse driving routes, we show that action space reduction significantly improves training stability and policy performance. The dynamic and relative schemes, in particular, achieve a favorable balance between learning speed, control precision, and generalization. These findings highlight the importance of context-aware action space design for scalable and reliable RL in autonomous driving tasks."
  },
  {
    "title": "Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models",
    "url": "http://arxiv.org/abs/2507.05248v1",
    "arxiv_id": "2507.05248v1",
    "authors": [
      "Ziqi Miao",
      "Lijun Li",
      "Yuan Xiong",
      "Zhenhua Liu",
      "Pengyu Zhu",
      "Jing Shao"
    ],
    "published": "2025-07-07T17:56:05+00:00",
    "summary": "Contextual priming, where earlier stimuli covertly bias later judgments, offers an unexplored attack surface for large language models (LLMs). We uncover a contextual priming vulnerability in which the previous response in the dialogue can steer its subsequent behavior toward policy-violating content. Building on this insight, we propose Response Attack, which uses an auxiliary LLM to generate a mildly harmful response to a paraphrased version of the original malicious query. They are then formatted into the dialogue and followed by a succinct trigger prompt, thereby priming the target model to generate harmful content. Across eight open-source and proprietary LLMs, RA consistently outperforms seven state-of-the-art jailbreak techniques, achieving higher attack success rates. To mitigate this threat, we construct and release a context-aware safety fine-tuning dataset, which significantly reduces the attack success rate while preserving model capabilities. The code and data are available at https://github.com/Dtc7w3PQ/Response-Attack."
  },
  {
    "title": "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real",
    "url": "http://arxiv.org/abs/2507.02864v1",
    "arxiv_id": "2507.02864v1",
    "authors": [
      "Renhao Wang",
      "Haoran Geng",
      "Tingle Li",
      "Feishi Wang",
      "Gopala Anumanchipalli",
      "Philipp Wu",
      "Trevor Darrell",
      "Boyi Li",
      "Pieter Abbeel",
      "Jitendra Malik",
      "Alexei A. Efros"
    ],
    "published": "2025-07-03T17:59:58+00:00",
    "summary": "Robots must integrate multiple sensory modalities to act effectively in the real world. Yet, learning such multimodal policies at scale remains challenging. Simulation offers a viable solution, but while vision has benefited from high-fidelity simulators, other modalities (e.g. sound) can be notoriously difficult to simulate. As a result, sim-to-real transfer has succeeded primarily in vision-based tasks, with multimodal transfer still largely unrealized. In this work, we tackle these challenges by introducing MultiGen, a framework that integrates large-scale generative models into traditional physics simulators, enabling multisensory simulation. We showcase our framework on the dynamic task of robot pouring, which inherently relies on multimodal feedback. By synthesizing realistic audio conditioned on simulation video, our method enables training on rich audiovisual trajectories -- without any real robot data. We demonstrate effective zero-shot transfer to real-world pouring with novel containers and liquids, highlighting the potential of generative modeling to both simulate hard-to-model modalities and close the multimodal sim-to-real gap."
  },
  {
    "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans",
    "url": "http://arxiv.org/abs/2507.02861v1",
    "arxiv_id": "2507.02861v1",
    "authors": [
      "Zhening Huang",
      "Xiaoyang Wu",
      "Fangcheng Zhong",
      "Hengshuang Zhao",
      "Matthias Nie\u00dfner",
      "Joan Lasenby"
    ],
    "published": "2025-07-03T17:59:55+00:00",
    "summary": "We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor environments into compact, realistic, and interactive 3D virtual replicas. LiteReality not only reconstructs scenes that visually resemble reality but also supports key features essential for graphics pipelines -- such as object individuality, articulation, high-quality physically based rendering materials, and physically based interaction. At its core, LiteReality first performs scene understanding and parses the results into a coherent 3D layout and objects with the help of a structured scene graph. It then reconstructs the scene by retrieving the most visually similar 3D artist-crafted models from a curated asset database. Next, the Material Painting module enhances realism by recovering high-quality, spatially varying materials. Finally, the reconstructed scene is integrated into a simulation engine with basic physical properties to enable interactive behavior. The resulting scenes are compact, editable, and fully compatible with standard graphics pipelines, making them suitable for applications in AR/VR, gaming, robotics, and digital twins. In addition, LiteReality introduces a training-free object retrieval module that achieves state-of-the-art similarity performance on the Scan2CAD benchmark, along with a robust material painting module capable of transferring appearances from images of any style to 3D assets -- even under severe misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of LiteReality on both real-life scans and public datasets. Project page: https://litereality.github.io; Video: https://www.youtube.com/watch?v=ecK9m3LXg2c"
  },
  {
    "title": "RefTok: Reference-Based Tokenization for Video Generation",
    "url": "http://arxiv.org/abs/2507.02862v1",
    "arxiv_id": "2507.02862v1",
    "authors": [
      "Xiang Fan",
      "Xiaohang Sun",
      "Kushan Thakkar",
      "Zhu Liu",
      "Vimal Bhat",
      "Ranjay Krishna",
      "Xiang Hao"
    ],
    "published": "2025-07-03T17:59:55+00:00",
    "summary": "Effectively handling temporal redundancy remains a key challenge in learning video models. Prevailing approaches often treat each set of frames independently, failing to effectively capture the temporal dependencies and redundancies inherent in videos. To address this limitation, we introduce RefTok, a novel reference-based tokenization method capable of capturing complex temporal dynamics and contextual information. Our method encodes and decodes sets of frames conditioned on an unquantized reference frame. When decoded, RefTok preserves the continuity of motion and the appearance of objects across frames. For example, RefTok retains facial details despite head motion, reconstructs text correctly, preserves small patterns, and maintains the legibility of handwriting from the context. Across 4 video datasets (K600, UCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms current state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all evaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or higher compression ratios. When a video generation model is trained using RefTok's latents on the BAIR Robot Pushing task, the generations not only outperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters, across all generation metrics by an average of 27.9%."
  },
  {
    "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs",
    "url": "http://arxiv.org/abs/2507.02851v1",
    "arxiv_id": "2507.02851v1",
    "authors": [
      "Purbesh Mitra",
      "Sennur Ulukus"
    ],
    "published": "2025-07-03T17:55:43+00:00",
    "summary": "Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively."
  },
  {
    "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason",
    "url": "http://arxiv.org/abs/2507.02841v1",
    "arxiv_id": "2507.02841v1",
    "authors": [
      "Kaiyi Zhang",
      "Ang Lv",
      "Jinpeng Li",
      "Yongbo Wang",
      "Feng Wang",
      "Haoyuan Hu",
      "Rui Yan"
    ],
    "published": "2025-07-03T17:51:06+00:00",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their ``comfort zone,'' lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model's exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its ``comfort zone'' and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks."
  },
  {
    "title": "AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation",
    "url": "http://arxiv.org/abs/2507.01961v1",
    "arxiv_id": "2507.01961v1",
    "authors": [
      "Sixiang Chen",
      "Jiaming Liu",
      "Siyuan Qian",
      "Han Jiang",
      "Lily Li",
      "Renrui Zhang",
      "Zhuoyang Liu",
      "Chenyang Gu",
      "Chengkai Hou",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Shanghang Zhang"
    ],
    "published": "2025-07-02T17:59:54+00:00",
    "summary": "Recently, mobile manipulation has attracted increasing attention for enabling language-conditioned robotic control in household tasks. However, existing methods still face challenges in coordinating mobile base and manipulator, primarily due to two limitations. On the one hand, they fail to explicitly model the influence of the mobile base on manipulator control, which easily leads to error accumulation under high degrees of freedom. On the other hand, they treat the entire mobile manipulation process with the same visual observation modality (e.g., either all 2D or all 3D), overlooking the distinct multimodal perception requirements at different stages during mobile manipulation. To address this, we propose the Adaptive Coordination Diffusion Transformer (AC-DiT), which enhances mobile base and manipulator coordination for end-to-end mobile manipulation. First, since the motion of the mobile base directly influences the manipulator's actions, we introduce a mobility-to-body conditioning mechanism that guides the model to first extract base motion representations, which are then used as context prior for predicting whole-body actions. This enables whole-body control that accounts for the potential impact of the mobile base's motion. Second, to meet the perception requirements at different stages of mobile manipulation, we design a perception-aware multimodal conditioning strategy that dynamically adjusts the fusion weights between various 2D visual images and 3D point clouds, yielding visual features tailored to the current perceptual needs. This allows the model to, for example, adaptively rely more on 2D inputs when semantic information is crucial for action prediction, while placing greater emphasis on 3D geometric information when precise spatial understanding is required. We validate AC-DiT through extensive experiments on both simulated and real-world mobile manipulation tasks."
  },
  {
    "title": "Kwai Keye-VL Technical Report",
    "url": "http://arxiv.org/abs/2507.01949v1",
    "arxiv_id": "2507.01949v1",
    "authors": [
      "Kwai Keye Team",
      "Biao Yang",
      "Bin Wen",
      "Changyi Liu",
      "Chenglong Chu",
      "Chengru Song",
      "Chongling Rao",
      "Chuan Yi",
      "Da Li",
      "Dunju Zang",
      "Fan Yang",
      "Guorui Zhou",
      "Hao Peng",
      "Haojie Ding",
      "Jiaming Huang",
      "Jiangxia Cao",
      "Jiankang Chen",
      "Jingyun Hua",
      "Jin Ouyang",
      "Kaibing Chen",
      "Kaiyu Jiang",
      "Kaiyu Tang",
      "Kun Gai",
      "Shengnan Zhang",
      "Siyang Mao",
      "Sui Huang",
      "Tianke Zhang",
      "Tingting Gao",
      "Wei Chen",
      "Wei Yuan",
      "Xiangyu Wu",
      "Xiao Hu",
      "Xingyu Lu",
      "Yang Zhou",
      "Yi-Fan Zhang",
      "Yiping Yang",
      "Yulong Chen",
      "Zhenhua Wu",
      "Zhenyu Li",
      "Zhixin Ling",
      "Ziming Li",
      "Dehua Ma",
      "Di Xu",
      "Haixuan Gao",
      "Hang Li",
      "Jiawei Guo",
      "Jing Wang",
      "Lejian Ren",
      "Muhao Wei",
      "Qianqian Wang",
      "Qigen Hu",
      "Shiyao Wang",
      "Tao Yu",
      "Xinchen Luo",
      "Yan Li",
      "Yiming Liang",
      "Yuhang Hu",
      "Zeyi Lu",
      "Zhuoran Yang",
      "Zixing Zhang"
    ],
    "published": "2025-07-02T17:57:28+00:00",
    "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today's digital landscape. To bridge this gap, we introduce \\textbf{Kwai Keye-VL}, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode ``cold-start'' data mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think with image'', and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the \\textbf{KC-MMBench}, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage."
  },
  {
    "title": "Characterizing control between interacting subsystems with deep Jacobian estimation",
    "url": "http://arxiv.org/abs/2507.01946v1",
    "arxiv_id": "2507.01946v1",
    "authors": [
      "Adam J. Eisen",
      "Mitchell Ostrow",
      "Sarthak Chandra",
      "Leo Kozachkov",
      "Earl K. Miller",
      "Ila R. Fiete"
    ],
    "published": "2025-07-02T17:55:53+00:00",
    "summary": "Biological function arises through the dynamical interactions of multiple subsystems, including those between brain areas, within gene regulatory networks, and more. A common approach to understanding these systems is to model the dynamics of each subsystem and characterize communication between them. An alternative approach is through the lens of control theory: how the subsystems control one another. This approach involves inferring the directionality, strength, and contextual modulation of control between subsystems. However, methods for understanding subsystem control are typically linear and cannot adequately describe the rich contextual effects enabled by nonlinear complex systems. To bridge this gap, we devise a data-driven nonlinear control-theoretic framework to characterize subsystem interactions via the Jacobian of the dynamics. We address the challenge of learning Jacobians from time-series data by proposing the JacobianODE, a deep learning method that leverages properties of the Jacobian to directly estimate it for arbitrary dynamical systems from data alone. We show that JacobianODEs outperform existing Jacobian estimation methods on challenging systems, including high-dimensional chaos. Applying our approach to a multi-area recurrent neural network (RNN) trained on a working memory selection task, we show that the \"sensory\" area gains greater control over the \"cognitive\" area over learning. Furthermore, we leverage the JacobianODE to directly control the trained RNN, enabling precise manipulation of its behavior. Our work lays the foundation for a theoretically grounded and data-driven understanding of interactions among biological subsystems."
  },
  {
    "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations",
    "url": "http://arxiv.org/abs/2507.01930v1",
    "arxiv_id": "2507.01930v1",
    "authors": [
      "Wenhao Wang",
      "Yanyan Li",
      "Long Jiao",
      "Jiawei Yuan"
    ],
    "published": "2025-07-02T17:44:17+00:00",
    "summary": "Large Language Models (LLMs) have revolutionized robotic autonomy, including Unmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential of LLMs for translating human instructions into executable control code for UAV operations. However, LLMs still face challenges from logical reasoning and complex decision-making, leading to concerns about the reliability of LLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop control framework that enables reliable UAV operations powered by effective feedback and refinement using two LLM modules, i.e., a Code Generator and an Evaluator. Our framework transforms numerical state observations from UAV operations into natural language trajectory descriptions to enhance the evaluator LLM's understanding of UAV dynamics for precise feedback generation. Our framework also enables a simulation-based refinement process, and hence eliminates the risks to physical UAVs caused by incorrect code execution during the refinement. Extensive experiments on UAV control tasks with different complexities are conducted. The experimental results show that our framework can achieve reliable UAV operations using LLMs, which significantly outperforms baseline approaches in terms of success rate and completeness with the increase of task complexity."
  },
  {
    "title": "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective",
    "url": "http://arxiv.org/abs/2507.01925v1",
    "arxiv_id": "2507.01925v1",
    "authors": [
      "Yifan Zhong",
      "Fengshuo Bai",
      "Shaofei Cai",
      "Xuchuan Huang",
      "Zhang Chen",
      "Xiaowei Zhang",
      "Yuanfei Wang",
      "Shaoyang Guo",
      "Tianrui Guan",
      "Ka Nam Lui",
      "Zhiquan Qi",
      "Yitao Liang",
      "Yuanpei Chen",
      "Yaodong Yang"
    ],
    "published": "2025-07-02T17:34:52+00:00",
    "summary": "The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of \\textit{action tokens} that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence."
  },
  {
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.24119v1",
    "arxiv_id": "2506.24119v1",
    "authors": [
      "Bo Liu",
      "Leon Guertler",
      "Simon Yu",
      "Zichen Liu",
      "Penghui Qi",
      "Daniel Balcells",
      "Mickel Liu",
      "Cheston Tan",
      "Weiyan Shi",
      "Min Lin",
      "Wee Sun Lee",
      "Natasha Jaques"
    ],
    "published": "2025-06-30T17:58:13+00:00",
    "summary": "Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development."
  },
  {
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.24119v2",
    "arxiv_id": "2506.24119v2",
    "authors": [
      "Bo Liu",
      "Leon Guertler",
      "Simon Yu",
      "Zichen Liu",
      "Penghui Qi",
      "Daniel Balcells",
      "Mickel Liu",
      "Cheston Tan",
      "Weiyan Shi",
      "Min Lin",
      "Wee Sun Lee",
      "Natasha Jaques"
    ],
    "published": "2025-06-30T17:58:13+00:00",
    "summary": "Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development."
  },
  {
    "title": "Scaling Human Judgment in Community Notes with LLMs",
    "url": "http://arxiv.org/abs/2506.24118v1",
    "arxiv_id": "2506.24118v1",
    "authors": [
      "Haiwen Li",
      "Soham De",
      "Manon Revel",
      "Andreas Haupt",
      "Brad Miller",
      "Keith Coleman",
      "Jay Baxter",
      "Martin Saveski",
      "Michiel A. Bakker"
    ],
    "published": "2025-06-30T17:57:32+00:00",
    "summary": "This paper argues for a new paradigm for Community Notes in the LLM era: an open ecosystem where both humans and LLMs can write notes, and the decision of which notes are helpful enough to show remains in the hands of humans. This approach can accelerate the delivery of notes, while maintaining trust and legitimacy through Community Notes' foundational principle: A community of diverse human raters collectively serve as the ultimate evaluator and arbiter of what is helpful. Further, the feedback from this diverse community can be used to improve LLMs' ability to produce accurate, unbiased, broadly helpful notes--what we term Reinforcement Learning from Community Feedback (RLCF). This becomes a two-way street: LLMs serve as an asset to humans--helping deliver context quickly and with minimal effort--while human feedback, in turn, enhances the performance of LLMs. This paper describes how such a system can work, its benefits, key new risks and challenges it introduces, and a research agenda to solve those challenges and realize the potential of this approach."
  },
  {
    "title": "Modified non-local damage model: resolving spurious damage evolution",
    "url": "http://arxiv.org/abs/2506.24099v1",
    "arxiv_id": "2506.24099v1",
    "authors": [
      "Roshan Philip Saji",
      "Panos Pantidis",
      "Mostafa E. Mobasher"
    ],
    "published": "2025-06-30T17:50:24+00:00",
    "summary": "Accurate prediction of damage and fracture evolution is critical for the safety design and preventive maintenance of engineering structures, however existing computational methods face significant limitations. On one hand, discrete damage and phase-field models are often computationally prohibitive for real world applications and they are less generalizable across different material classes. On the other hand, conventional gradient damage models which are based on phenomenological laws, though more computationally efficient, they suffer from unrealistic widening of the damage-band as damage progresses. This paper presents a modified non-local gradient damage model (MNLD) that overcomes these shortcomings by introducing modifications to the stress degradation function and forcing term in the Helmholtz free energy expression. These two modifications ensure that as damage approaches its maximum value, both the thermodynamic damage driving force for damage vanishes and the evolution of the forcing term decays. Consequently, the damage band retains a non-growing constant width throughout its evolution. The proposed approach builds on insights gained from two intermediate models, which addressed the necessary conditions separately before integrating them into a unified formulation. Numerical validation is performed on several 1D and 2D benchmark problems, demonstrating that the proposed model can reliably produce fixed-width damage bands. The proposed approach can be implemented within existing gradient damage-based finite element frameworks with minimal implementation changes. The results highlight the potential of this approach to resolve the decades-long challenge of spurious widening in gradient damage models, offering an effective and practical solution for engineering applications."
  },
  {
    "title": "Time Shift Governor-Guided MPC with Collision Cone CBFs for Safe Adaptive Cruise Control in Dynamic Environments",
    "url": "http://arxiv.org/abs/2506.24083v1",
    "arxiv_id": "2506.24083v1",
    "authors": [
      "Robin Inho Kee",
      "Taehyeun Kim",
      "Anouck Girard",
      "Ilya Kolmanovsky"
    ],
    "published": "2025-06-30T17:39:31+00:00",
    "summary": "This paper introduces a Time Shift Governor (TSG)-guided Model Predictive Controller with Control Barrier Functions (CBFs)-based constraints for adaptive cruise control (ACC). This MPC-CBF approach is defined for obstacle-free curved road tracking, while following distance and obstacle avoidance constraints are handled using standard CBFs and relaxed Collision Cone CBFs. In order to address scenarios involving rapidly moving obstacles or rapidly changing leading vehicle's behavior, the TSG augmentation is employed which alters the target reference to enforce constraints. Simulation results demonstrate the effectiveness of the TSG-guided MPC-CBF approach."
  },
  {
    "title": "SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks",
    "url": "http://arxiv.org/abs/2506.24081v1",
    "arxiv_id": "2506.24081v1",
    "authors": [
      "Rahul Kumar",
      "Wenqi Wei",
      "Ying Mao",
      "Junaid Farooq",
      "Ying Wang",
      "Juntao Chen"
    ],
    "published": "2025-06-30T17:36:31+00:00",
    "summary": "We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to sabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks. SQUASH is executed by inserting SWAP gate(s) into the variational quantum circuit of the victim HQNN. Unlike conventional noise-based or adversarial input attacks, SQUASH directly manipulates the circuit structure, leading to qubit misalignment and disrupting quantum state evolution. This attack is highly stealthy, as it does not require access to training data or introduce detectable perturbations in input states. Our results demonstrate that SQUASH significantly degrades classification performance, with untargeted SWAP attacks reducing accuracy by up to 74.08\\% and targeted SWAP attacks reducing target class accuracy by up to 79.78\\%. These findings reveal a critical vulnerability in HQNN implementations, underscoring the need for more resilient architectures against circuit-level adversarial interventions."
  },
  {
    "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
    "url": "http://arxiv.org/abs/2506.22434v1",
    "arxiv_id": "2506.22434v1",
    "authors": [
      "Xi Chen",
      "Mingkang Zhu",
      "Shaoteng Liu",
      "Xiaoyang Wu",
      "Xiaogang Xu",
      "Yu Liu",
      "Xiang Bai",
      "Hengshuang Zhao"
    ],
    "published": "2025-06-27T17:59:27+00:00",
    "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks."
  },
  {
    "title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy",
    "url": "http://arxiv.org/abs/2506.22432v1",
    "arxiv_id": "2506.22432v1",
    "authors": [
      "Yuhao Liu",
      "Tengfei Wang",
      "Fang Liu",
      "Zhenwei Wang",
      "Rynson W. H. Lau"
    ],
    "published": "2025-06-27T17:59:01+00:00",
    "summary": "Recent advances in deep generative modeling have unlocked unprecedented opportunities for video synthesis. In real-world applications, however, users often seek tools to faithfully realize their creative editing intentions with precise and consistent control. Despite the progress achieved by existing methods, ensuring fine-grained alignment with user intentions remains an open and challenging problem. In this work, we present Shape-for-Motion, a novel framework that incorporates a 3D proxy for precise and consistent video editing. Shape-for-Motion achieves this by converting the target object in the input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be performed directly on the proxy and then inferred back to the video frames. To simplify the editing process, we design a novel Dual-Propagation Strategy that allows users to perform edits on the 3D mesh of a single frame, and the edits are then automatically propagated to the 3D meshes of the other frames. The 3D meshes for different frames are further projected onto the 2D space to produce the edited geometry and texture renderings, which serve as inputs to a decoupled video diffusion model for generating edited results. Our framework supports various precise and physically-consistent manipulations across the video frames, including pose editing, rotation, scaling, translation, texture modification, and object composition. Our approach marks a key step toward high-quality, controllable video editing workflows. Extensive experiments demonstrate the superiority and effectiveness of our approach. Project page: https://shapeformotion.github.io/"
  },
  {
    "title": "ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks",
    "url": "http://arxiv.org/abs/2506.22423v1",
    "arxiv_id": "2506.22423v1",
    "authors": [
      "Pritam Dash",
      "Ethan Chan",
      "Nathan P. Lawrence",
      "Karthik Pattabiraman"
    ],
    "published": "2025-06-27T17:46:33+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception, navigation, and control. However, these sensors are susceptible to physical attacks, such as GPS spoofing, that can corrupt state estimates and lead to unsafe behavior. While reinforcement learning (RL) offers adaptive control capabilities, existing safe RL methods are ineffective against such attacks. We present ARMOR (Adaptive Robust Manipulation-Optimized State Representations), an attack-resilient, model-free RL controller that enables robust UAV operation under adversarial sensor manipulation. Instead of relying on raw sensor observations, ARMOR learns a robust latent representation of the UAV's physical state via a two-stage training framework. In the first stage, a teacher encoder, trained with privileged attack information, generates attack-aware latent states for RL policy training. In the second stage, a student encoder is trained via supervised learning to approximate the teacher's latent states using only historical sensor data, enabling real-world deployment without privileged information. Our experiments show that ARMOR outperforms conventional methods, ensuring UAV safety. Additionally, ARMOR improves generalization to unseen attacks and reduces training cost by eliminating the need for iterative adversarial training."
  },
  {
    "title": "Spherical Pendulum with Quad-Rotor Thrust Vectoring Actuation -- A Novel Mechatronics and Control Benchmark Platform",
    "url": "http://arxiv.org/abs/2506.22410v1",
    "arxiv_id": "2506.22410v1",
    "authors": [
      "Yuchen Li",
      "Omar Curiel",
      "Sheng-Fan Wen",
      "Tsu-Chin Tsao"
    ],
    "published": "2025-06-27T17:37:15+00:00",
    "summary": "Motor-actuated pendulums have been established as arguably the most common laboratory prototypes used in control system education because of the relevance to robot manipulator control in industry. Meanwhile, multi-rotor drones like quadcopters have become popular in industrial applications but have not been broadly employed in control education laboratory. Platforms with pendulums and multi-rotor copters present classical yet intriguing multi-degree of freedom (DoF) dynamics and coordinate systems for the control system investigation. In this paper, we introduce a novel control platform in which a 2-DoF pendulum capable of azimuth and elevation rotation is actuated through vectored thrust generated by a quadcopter. Designed as a benchmark for mechatronics and nonlinear control education and research, the system integrates detailed mechatronic implementation with different control strategies. Specifically, we apply and compare small perturbation linearization (SPL), state feedback linearization (SFL), and partial feedback linearization (PFL) to the nonlinear system dynamics. The performances are evaluated by time specifications of step response and Root-Mean-Square (RMS) error of trajectory tracking. The robustness of the closed-loop system is validated under external disturbances, and both simulation and experimental results are presented to highlight the strengths and limitations of the nonlinear model-based control approaches."
  },
  {
    "title": "Economic Model Predictive Control with a Non-Fixed Reference Trajectory for Optimal Microgrid Dispatch",
    "url": "http://arxiv.org/abs/2506.22406v1",
    "arxiv_id": "2506.22406v1",
    "authors": [
      "Avik Ghosh",
      "Adil Khurram",
      "Jan Kleissl",
      "Sonia Martinez"
    ],
    "published": "2025-06-27T17:27:40+00:00",
    "summary": "Economic Model Predictive Control (EMPC), instead of stabilizing a reference trajectory/state in the objective function like a Tracking MPC, optimizes the economic performance over the prediction horizon, making it attractive for economical microgrid (MG) dispatch. However, the demand charge component in the monthly electricity cost, make it difficult to be encapsulated in additive stage costs, and can make solutions violate the principle of optimality if naively introduced in the objective function. Moreover, previous EMPC based works mostly rely on a-priori knowledge of an optimal economic steady state or optimal periodic trajectory for performance guarantees, which are not useful or possibly don't exist respectively, for real-time economical MG dispatch where load/generation forecasts are known only 24-48 h in advance. This paper, first, proposes an EMPC formulation for a generic deterministic discrete non-linear time varying system with hard state and input constraints, without any a-priori requirements of an optimal economic steady state or optimal periodic trajectory. It is proved that under mild assumptions on terminal cost and region, the asymptotic average economic cost of the proposed method is no worse than the asymptotic average economic cost of any other non-fixed arbitrary reference trajectory which is known only until the current time-step. The EMPC framework is then leveraged for optimal MG dispatch by showing that the problem can be reformulated to satisfy the assumptions required for the asymptotic performance guarantee. Realistic simulations at the Port of San Diego MG demonstrated that the proposed method can also reduce monthly electricity costs in closed-loop with respect to reference trajectories generated by directly optimizing the electricity cost function over the prediction horizon or by tracking an ideal grid import curve in a majority of the cases."
  },
  {
    "title": "Whole-Body Conditioned Egocentric Video Prediction",
    "url": "http://arxiv.org/abs/2506.21552v1",
    "arxiv_id": "2506.21552v1",
    "authors": [
      "Yutong Bai",
      "Danny Tran",
      "Amir Bar",
      "Yann LeCun",
      "Trevor Darrell",
      "Jitendra Malik"
    ],
    "published": "2025-06-26T17:59:59+00:00",
    "summary": "We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human."
  },
  {
    "title": "SiM3D: Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark",
    "url": "http://arxiv.org/abs/2506.21549v1",
    "arxiv_id": "2506.21549v1",
    "authors": [
      "Alex Costanzino",
      "Pierluigi Zama Ramirez",
      "Luigi Lella",
      "Matteo Ragaglia",
      "Alessandro Oliva",
      "Giuseppe Lisanti",
      "Luigi Di Stefano"
    ],
    "published": "2025-06-26T17:59:55+00:00",
    "summary": "We propose SiM3D, the first benchmark considering the integration of multiview and multimodal information for comprehensive 3D anomaly detection and segmentation (ADS), where the task is to produce a voxel-based Anomaly Volume. Moreover, SiM3D focuses on a scenario of high interest in manufacturing: single-instance anomaly detection, where only one object, either real or synthetic, is available for training. In this respect, SiM3D stands out as the first ADS benchmark that addresses the challenge of generalising from synthetic training data to real test data. SiM3D includes a novel multimodal multiview dataset acquired using top-tier industrial sensors and robots. The dataset features multiview high-resolution images (12 Mpx) and point clouds (7M points) for 333 instances of eight types of objects, alongside a CAD model for each type. We also provide manually annotated 3D segmentation GTs for anomalous test samples. To establish reference baselines for the proposed multiview 3D ADS task, we adapt prominent singleview methods and assess their performance using novel metrics that operate on Anomaly Volumes."
  },
  {
    "title": "SAM4D: Segment Anything in Camera and LiDAR Streams",
    "url": "http://arxiv.org/abs/2506.21547v1",
    "arxiv_id": "2506.21547v1",
    "authors": [
      "Jianyun Xu",
      "Song Wang",
      "Ziqian Ni",
      "Chunyong Hu",
      "Sheng Yang",
      "Jianke Zhu",
      "Qiang Li"
    ],
    "published": "2025-06-26T17:59:14+00:00",
    "summary": "We present SAM4D, a multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. Unified Multi-modal Positional Encoding (UMPE) is introduced to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. Additionally, we propose Motion-aware Cross-modal Memory Attention (MCMA), which leverages ego-motion compensation to enhance temporal consistency and long-horizon feature retrieval, ensuring robust segmentation across dynamically changing autonomous driving scenes. To avoid annotation bottlenecks, we develop a multi-modal automated data engine that synergizes VFM-driven video masklets, spatiotemporal 4D reconstruction, and cross-modal masklet fusion. This framework generates camera-LiDAR aligned pseudo-labels at a speed orders of magnitude faster than human annotation while preserving VFM-derived semantic fidelity in point cloud representations. We conduct extensive experiments on the constructed Waymo-4DSeg, which demonstrate the powerful cross-modal segmentation ability and great potential in data annotation of proposed SAM4D."
  },
  {
    "title": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation",
    "url": "http://arxiv.org/abs/2506.21546v1",
    "arxiv_id": "2506.21546v1",
    "authors": [
      "Xinzhuo Li",
      "Adheesh Juvekar",
      "Xingyou Liu",
      "Muntasir Wahed",
      "Kiet A. Nguyen",
      "Ismini Lourentzou"
    ],
    "published": "2025-06-26T17:59:12+00:00",
    "summary": "Recent progress in vision-language segmentation has significantly advanced grounded visual understanding. However, these models often exhibit hallucinations by producing segmentation masks for objects not grounded in the image content or by incorrectly labeling irrelevant regions. Existing evaluation protocols for segmentation hallucination primarily focus on label or textual hallucinations without manipulating the visual context, limiting their capacity to diagnose critical failures. In response, we introduce HalluSegBench, the first benchmark specifically designed to evaluate hallucinations in visual grounding through the lens of counterfactual visual reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual instance pairs spanning 281 unique object classes, and a set of newly introduced metrics that quantify hallucination sensitivity under visually coherent scene edits. Experiments on HalluSegBench with state-of-the-art vision-language segmentation models reveal that vision-driven hallucinations are significantly more prevalent than label-driven ones, with models often persisting in false segmentation, highlighting the need for counterfactual reasoning to diagnose grounding fidelity."
  },
  {
    "title": "WorldVLA: Towards Autoregressive Action World Model",
    "url": "http://arxiv.org/abs/2506.21539v1",
    "arxiv_id": "2506.21539v1",
    "authors": [
      "Jun Cen",
      "Chaohui Yu",
      "Hangjie Yuan",
      "Yuming Jiang",
      "Siteng Huang",
      "Jiayan Guo",
      "Xin Li",
      "Yibing Song",
      "Hao Luo",
      "Fan Wang",
      "Deli Zhao",
      "Hao Chen"
    ],
    "published": "2025-06-26T17:55:40+00:00",
    "summary": "We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. Our WorldVLA intergrates Vision-Language-Action (VLA) model and world model in one single framework. The world model predicts future images by leveraging both action and image understanding, with the purpose of learning the underlying physics of the environment to improve action generation. Meanwhile, the action model generates the subsequent actions based on image observations, aiding in visual understanding and in turn helps visual generation of the world model. We demonstrate that WorldVLA outperforms standalone action and world models, highlighting the mutual enhancement between the world model and the action model. In addition, we find that the performance of the action model deteriorates when generating sequences of actions in an autoregressive manner. This phenomenon can be attributed to the model's limited generalization capability for action prediction, leading to the propagation of errors from earlier actions to subsequent ones. To address this issue, we propose an attention mask strategy that selectively masks prior actions during the generation of the current action, which shows significant performance improvement in the action chunk generation task."
  },
  {
    "title": "IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals",
    "url": "http://arxiv.org/abs/2506.20671v1",
    "arxiv_id": "2506.20671v1",
    "authors": [
      "Markus Gross",
      "Aya Fahmy",
      "Danit Niwattananan",
      "Dominik Muhle",
      "Rui Song",
      "Daniel Cremers",
      "Henri Mee\u00df"
    ],
    "published": "2025-06-25T17:59:45+00:00",
    "summary": "Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly learning scene geometry and semantics, enabling downstream applications such as navigation in mobile robotics. The recent generalization to Panoptic Scene Completion (PSC) advances the SSC domain by integrating instance-level information, thereby enhancing object-level sensitivity in scene understanding. While PSC was introduced using LiDAR modality, methods based on camera images remain largely unexplored. Moreover, recent Transformer-based SSC approaches utilize a fixed set of learned queries to reconstruct objects within the scene volume. Although these queries are typically updated with image context during training, they remain static at test time, limiting their ability to dynamically adapt specifically to the observed scene. To overcome these limitations, we propose IPFormer, the first approach that leverages context-adaptive instance proposals at train and test time to address vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively initializes these queries as panoptic instance proposals derived from image context and further refines them through attention-based encoding and decoding to reason about semantic instance-voxel relationships. Experimental results show that our approach surpasses state-of-the-art methods in overall panoptic metrics PQ$^\\dagger$ and PQ-All, matches performance in individual metrics, and achieves a runtime reduction exceeding 14$\\times$. Furthermore, our ablation studies reveal that dynamically deriving instance proposals from image context, as opposed to random initialization, leads to a 3.62% increase in PQ-All and a remarkable average improvement of 18.65% in combined Thing-metrics. These results highlight our introduction of context-adaptive instance proposals as a pioneering effort in addressing vision-based 3D Panoptic Scene Completion."
  },
  {
    "title": "MMSearch-R1: Incentivizing LMMs to Search",
    "url": "http://arxiv.org/abs/2506.20670v1",
    "arxiv_id": "2506.20670v1",
    "authors": [
      "Jinming Wu",
      "Zihao Deng",
      "Wei Li",
      "Yiding Liu",
      "Bo You",
      "Bo Li",
      "Zejun Ma",
      "Ziwei Liu"
    ],
    "published": "2025-06-25T17:59:42+00:00",
    "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search."
  },
  {
    "title": "DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy",
    "url": "http://arxiv.org/abs/2506.20668v1",
    "arxiv_id": "2506.20668v1",
    "authors": [
      "Sungjae Park",
      "Homanga Bharadhwaj",
      "Shubham Tulsiani"
    ],
    "published": "2025-06-25T17:59:01+00:00",
    "summary": "We propose DemoDiffusion, a simple and scalable method for enabling robots to perform manipulation tasks in natural environments by imitating a single human demonstration. Our approach is based on two key insights. First, the hand motion in a human demonstration provides a useful prior for the robot's end-effector trajectory, which we can convert into a rough open-loop robot motion trajectory via kinematic retargeting. Second, while this retargeted motion captures the overall structure of the task, it may not align well with plausible robot actions in-context. To address this, we leverage a pre-trained generalist diffusion policy to modify the trajectory, ensuring it both follows the human motion and remains within the distribution of plausible robot actions. Our approach avoids the need for online reinforcement learning or paired human-robot data, enabling robust adaptation to new tasks and scenes with minimal manual effort. Experiments in both simulation and real-world settings show that DemoDiffusion outperforms both the base policy and the retargeted trajectory, enabling the robot to succeed even on tasks where the pre-trained generalist policy fails entirely. Project page: https://demodiffusion.github.io/"
  },
  {
    "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind",
    "url": "http://arxiv.org/abs/2506.20664v1",
    "arxiv_id": "2506.20664v1",
    "authors": [
      "Andrei Lupu",
      "Timon Willi",
      "Jakob Foerster"
    ],
    "published": "2025-06-25T17:55:27+00:00",
    "summary": "As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the \"mental\" states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments.   We validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents."
  },
  {
    "title": "Continuous operation of a coherent 3,000-qubit system",
    "url": "http://arxiv.org/abs/2506.20660v1",
    "arxiv_id": "2506.20660v1",
    "authors": [
      "Neng-Chun Chiu",
      "Elias C. Trapp",
      "Jinen Guo",
      "Mohamed H. Abobeih",
      "Luke M. Stewart",
      "Simon Hollerith",
      "Pavel Stroganov",
      "Marcin Kalinowski",
      "Alexandra A. Geim",
      "Simon J. Evered",
      "Sophie H. Li",
      "Lisa M. Peters",
      "Dolev Bluvstein",
      "Tout T. Wang",
      "Markus Greiner",
      "Vladan Vuleti\u0107",
      "Mikhail D. Lukin"
    ],
    "published": "2025-06-25T17:53:56+00:00",
    "summary": "Neutral atoms are a promising platform for quantum science, enabling advances in areas ranging from quantum simulations and computation to metrology, atomic clocks and quantum networking. While atom losses typically limit these systems to a pulsed mode, continuous operation could significantly enhance cycle rates, remove bottlenecks in metrology, and enable deep-circuit quantum evolution through quantum error correction. Here we demonstrate an experimental architecture for high-rate, continuous reloading and operation of a large-scale atom array system while realizing coherent storage and manipulation of quantum information. Our approach utilizes a series of two optical lattice conveyor belts to transport atom reservoirs into the science region, where atoms are repeatedly extracted into optical tweezers without affecting the coherence of qubits stored nearby. Using a reloading rate of 300,000 atoms in tweezers per second, we create over 30,000 initialized qubits per second, which we leverage to assemble and maintain an array of over 3,000 atoms for more than two hours. Furthermore, we demonstrate persistent refilling of the array with atomic qubits in either a spin-polarized or a coherent superposition state while preserving the quantum state of stored qubits. Our results pave the way for realization of large-scale continuously operated atomic clocks, sensors, and fault-tolerant quantum computers."
  },
  {
    "title": "Unified Vision-Language-Action Model",
    "url": "http://arxiv.org/abs/2506.19850v1",
    "arxiv_id": "2506.19850v1",
    "authors": [
      "Yuqi Wang",
      "Xinghang Li",
      "Wenxuan Wang",
      "Junbo Zhang",
      "Yingyan Li",
      "Yuntao Chen",
      "Xinlong Wang",
      "Zhaoxiang Zhang"
    ],
    "published": "2025-06-24T17:59:57+00:00",
    "summary": "Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving."
  },
  {
    "title": "JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.19846v1",
    "arxiv_id": "2506.19846v1",
    "authors": [
      "Ai Han",
      "Junxing Hu",
      "Pu Wei",
      "Zhiqian Zhang",
      "Yuhang Guo",
      "Jiawei Lu",
      "Zicheng Zhang"
    ],
    "published": "2025-06-24T17:59:31+00:00",
    "summary": "Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm for increasingly complex tasks. However, joint evolution across heterogeneous agents remains challenging due to cooperative inefficiency and training instability. In this paper, we propose the joint evolution dynamics for MARL called JoyAgents-R1, which first applies Group Relative Policy Optimization (GRPO) to the joint training of heterogeneous multi-agents. By iteratively refining agents' large language models (LLMs) and memories, the method achieves holistic equilibrium with optimal decision-making and memory capabilities. Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on the behavior of each agent across entire reasoning trajectories to enhance GRPO sampling efficiency while maintaining policy diversity. Then, our marginal benefit-driven selection strategy identifies top-$K$ sampling groups with maximal reward fluctuations, enabling targeted agent model updates that improve training stability and maximize joint benefits through cost-effective parameter adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution mechanism that repurposes GRPO rewards as cost-free supervisory signals to eliminate repetitive reasoning and accelerate convergence. Experiments across general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves performance comparable to that of larger LLMs while built on smaller open-source models."
  },
  {
    "title": "Temporal-IRL: Modeling Port Congestion and Berth Scheduling with Inverse Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.19843v1",
    "arxiv_id": "2506.19843v1",
    "authors": [
      "Guo Li",
      "Zixiang Xu",
      "Wei Zhang",
      "Yikuan Hu",
      "Xinyu Yang",
      "Nikolay Aristov",
      "Mingjie Tang",
      "Elenna R Dugundji"
    ],
    "published": "2025-06-24T17:59:12+00:00",
    "summary": "Predicting port congestion is crucial for maintaining reliable global supply chains. Accurate forecasts enableimprovedshipment planning, reducedelaysand costs, and optimizeinventoryanddistributionstrategies, thereby ensuring timely deliveries and enhancing supply chain resilience. To achieve accurate predictions, analyzing vessel behavior and their stay times at specific port terminals is essential, focusing particularly on berth scheduling under various conditions. Crucially, the model must capture and learn the underlying priorities and patterns of berth scheduling. Berth scheduling and planning are influenced by a range of factors, including incoming vessel size, waiting times, and the status of vessels within the port terminal. By observing historical Automatic Identification System (AIS) positions of vessels, we reconstruct berth schedules, which are subsequently utilized to determine the reward function via Inverse Reinforcement Learning (IRL). For this purpose, we modeled a specific terminal at the Port of New York/New Jersey and developed Temporal-IRL. This Temporal-IRL model learns berth scheduling to predict vessel sequencing at the terminal and estimate vessel port stay, encompassing both waiting and berthing times, to forecast port congestion. Utilizing data from Maher Terminal spanning January 2015 to September 2023, we trained and tested the model, achieving demonstrably excellent results."
  },
  {
    "title": "ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model",
    "url": "http://arxiv.org/abs/2506.19842v1",
    "arxiv_id": "2506.19842v1",
    "authors": [
      "Tengbo Yu",
      "Guanxing Lu",
      "Zaijia Yang",
      "Haoyuan Deng",
      "Season Si Chen",
      "Jiwen Lu",
      "Wenbo Ding",
      "Guoqiang Hu",
      "Yansong Tang",
      "Ziwei Wang"
    ],
    "published": "2025-06-24T17:59:06+00:00",
    "summary": "Multi-task robotic bimanual manipulation is becoming increasingly popular as it enables sophisticated tasks that require diverse dual-arm collaboration patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to understanding the multi-body spatiotemporal dynamics. An existing method ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual representation via Gaussian world model for single-arm settings, which ignores the interaction of multiple embodiments for dual-arm systems with significant performance drop. In this paper, we propose ManiGaussian++, an extension of ManiGaussian framework that improves multi-task bimanual manipulation by digesting multi-body scene dynamics through a hierarchical Gaussian world model. To be specific, we first generate task-oriented Gaussian Splatting from intermediate visual features, which aims to differentiate acting and stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build a hierarchical Gaussian world model with the leader-follower architecture, where the multi-body spatiotemporal dynamics is mined for intermediate visual representation via future scene prediction. The leader predicts Gaussian Splatting deformation caused by motions of the stabilizing arm, through which the follower generates the physical consequences resulted from the movement of the acting arm. As a result, our method significantly outperforms the current state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in 10 simulated tasks, and achieves 60% success rate on average in 9 challenging real-world tasks. Our code is available at https://github.com/April-Yz/ManiGaussian_Bimanual."
  },
  {
    "title": "Look to Locate: Vision-Based Multisensory Navigation with 3-D Digital Maps for GNSS-Challenged Environments",
    "url": "http://arxiv.org/abs/2506.19827v1",
    "arxiv_id": "2506.19827v1",
    "authors": [
      "Ola Elmaghraby",
      "Eslam Mounier",
      "Paulo Ricardo Marques de Araujo",
      "Aboelmagd Noureldin"
    ],
    "published": "2025-06-24T17:44:03+00:00",
    "summary": "In Global Navigation Satellite System (GNSS)-denied environments such as indoor parking structures or dense urban canyons, achieving accurate and robust vehicle positioning remains a significant challenge. This paper proposes a cost-effective, vision-based multi-sensor navigation system that integrates monocular depth estimation, semantic filtering, and visual map registration (VMR) with 3-D digital maps. Extensive testing in real-world indoor and outdoor driving scenarios demonstrates the effectiveness of the proposed system, achieving sub-meter accuracy of 92% indoors and more than 80% outdoors, with consistent horizontal positioning and heading average root mean-square errors of approximately 0.98 m and 1.25 {\\deg}, respectively. Compared to the baselines examined, the proposed solution significantly reduced drift and improved robustness under various conditions, achieving positioning accuracy improvements of approximately 88% on average. This work highlights the potential of cost-effective monocular vision systems combined with 3D maps for scalable, GNSS-independent navigation in land vehicles."
  },
  {
    "title": "TC-Light: Temporally Consistent Relighting for Dynamic Long Videos",
    "url": "http://arxiv.org/abs/2506.18904v1",
    "arxiv_id": "2506.18904v1",
    "authors": [
      "Yang Liu",
      "Chuanchen Luo",
      "Zimo Tang",
      "Yingyan Li",
      "Yuran Yang",
      "Yuanyong Ning",
      "Lue Fan",
      "Junran Peng",
      "Zhaoxiang Zhang"
    ],
    "published": "2025-06-23T17:59:58+00:00",
    "summary": "Editing illumination in long videos with complex dynamics has significant value in various downstream tasks, including visual content creation and manipulation, as well as data scaling up for embodied AI through sim2real and real2real transfer. Nevertheless, existing video relighting techniques are predominantly limited to portrait videos or fall into the bottleneck of temporal consistency and computation efficiency. In this paper, we propose TC-Light, a novel paradigm characterized by the proposed two-stage post optimization mechanism. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible relighting results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/."
  },
  {
    "title": "MinD: Unified Visual Imagination and Control via Hierarchical World Models",
    "url": "http://arxiv.org/abs/2506.18897v1",
    "arxiv_id": "2506.18897v1",
    "authors": [
      "Xiaowei Chi",
      "Kuangzhi Ge",
      "Jiaming Liu",
      "Siyuan Zhou",
      "Peidong Jia",
      "Zichen He",
      "Yuzhen Liu",
      "Tingguang Li",
      "Lei Han",
      "Sirui Han",
      "Shanghang Zhang",
      "Yike Guo"
    ],
    "published": "2025-06-23T17:59:06+00:00",
    "summary": "Video generation models (VGMs) offer a promising pathway for unified world modeling in robotics by integrating simulation, prediction, and manipulation. However, their practical application remains limited due to (1) slowgeneration speed, which limits real-time interaction, and (2) poor consistency between imagined videos and executable actions. To address these challenges, we propose Manipulate in Dream (MinD), a hierarchical diffusion-based world model framework that employs a dual-system design for vision-language manipulation. MinD executes VGM at low frequencies to extract video prediction features, while leveraging a high-frequency diffusion policy for real-time interaction. This architecture enables low-latency, closed-loop control in manipulation with coherent visual guidance. To better coordinate the two systems, we introduce a video-action diffusion matching module (DiffMatcher), with a novel co-training strategy that uses separate schedulers for each diffusion model. Specifically, we introduce a diffusion-forcing mechanism to DiffMatcher that aligns their intermediate representations during training, helping the fast action model better understand video-based predictions. Beyond manipulation, MinD also functions as a world simulator, reliably predicting task success or failure in latent space before execution. Trustworthy analysis further shows that VGMs can preemptively evaluate task feasibility and mitigate risks. Extensive experiments across multiple benchmarks demonstrate that MinD achieves state-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of unified world modeling in robotics."
  },
  {
    "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs",
    "url": "http://arxiv.org/abs/2506.18896v1",
    "arxiv_id": "2506.18896v1",
    "authors": [
      "Jiaru Zou",
      "Ling Yang",
      "Jingwen Gu",
      "Jiahao Qiu",
      "Ke Shen",
      "Jingrui He",
      "Mengdi Wang"
    ],
    "published": "2025-06-23T17:59:02+00:00",
    "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux"
  },
  {
    "title": "GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM",
    "url": "http://arxiv.org/abs/2506.18885v1",
    "arxiv_id": "2506.18885v1",
    "authors": [
      "Annika Thomas",
      "Aneesa Sonawalla",
      "Alex Rose",
      "Jonathan P. How"
    ],
    "published": "2025-06-23T17:55:42+00:00",
    "summary": "3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset."
  },
  {
    "title": "RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base",
    "url": "http://arxiv.org/abs/2506.18856v1",
    "arxiv_id": "2506.18856v1",
    "authors": [
      "Kuanning Wang",
      "Yuqian Fu",
      "Tianyu Wang",
      "Yanwei Fu",
      "Longfei Liang",
      "Yu-Gang Jiang",
      "Xiangyang Xue"
    ],
    "published": "2025-06-23T17:19:41+00:00",
    "summary": "Accurate 6D pose estimation is key for robotic manipulation, enabling precise object localization for tasks like grasping. We present RAG-6DPose, a retrieval-augmented approach that leverages 3D CAD models as a knowledge base by integrating both visual and geometric cues. Our RAG-6DPose roughly contains three stages: 1) Building a Multi-Modal CAD Knowledge Base by extracting 2D visual features from multi-view CAD rendered images and also attaching 3D points; 2) Retrieving relevant CAD features from the knowledge base based on the current query image via our ReSPC module; and 3) Incorporating retrieved CAD information to refine pose predictions via retrieval-augmented decoding. Experimental results on standard benchmarks and real-world robotic tasks demonstrate the effectiveness and robustness of our approach, particularly in handling occlusions and novel viewpoints. Supplementary material is available on our project website: https://sressers.github.io/RAG-6DPose ."
  },
  {
    "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning",
    "url": "http://arxiv.org/abs/2506.17219v1",
    "arxiv_id": "2506.17219v1",
    "authors": [
      "Yanzhi Zhang",
      "Zhaoxi Zhang",
      "Haoxiang Guan",
      "Yilin Cheng",
      "Yitong Duan",
      "Chen Wang",
      "Yue Wang",
      "Shuxin Zheng",
      "Jiyan He"
    ],
    "published": "2025-06-20T17:59:52+00:00",
    "summary": "Reinforcement learning has emerged as a powerful paradigm for post-training large language models (LLMs) to improve reasoning. Approaches like Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) have shown strong results, but they require extensive external supervision. We investigate an alternative class of methods, Reinforcement Learning from Internal Feedback (RLIF), which relies solely on intrinsic model-derived signals instead of external rewards. In particular, we leverage unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty. Our theoretical analysis shows these internal objectives are partially equivalent, and we empirically evaluate various RLIF strategies on challenging math reasoning benchmarks. Experimental results demonstrate that RLIF can boost the reasoning performance of base LLMs at the beginning phase of the training, matching or surpassing RLVR techniques on these tasks. However, when training progresses, performance degrades even below the model before training. Moreover, we find that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already instruction-tuned. We further analyze this limitation by mixing model weights and explain the reason of RLIF's training behaviors, providing practical guidelines for integrating internal feedback signals into LLM training. We hope our analysis of internal feedback will inform more principled and effective strategies for LLM post-training."
  },
  {
    "title": "Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens",
    "url": "http://arxiv.org/abs/2506.17218v1",
    "arxiv_id": "2506.17218v1",
    "authors": [
      "Zeyuan Yang",
      "Xueyang Yu",
      "Delin Chen",
      "Maohao Shen",
      "Chuang Gan"
    ],
    "published": "2025-06-20T17:59:31+00:00",
    "summary": "Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation."
  },
  {
    "title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation",
    "url": "http://arxiv.org/abs/2506.17213v1",
    "arxiv_id": "2506.17213v1",
    "authors": [
      "Xiuyu Yang",
      "Shuhan Tan",
      "Philipp Kr\u00e4henb\u00fchl"
    ],
    "published": "2025-06-20T17:59:21+00:00",
    "summary": "An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen"
  },
  {
    "title": "Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting",
    "url": "http://arxiv.org/abs/2506.17212v1",
    "arxiv_id": "2506.17212v1",
    "authors": [
      "Tianjiao Yu",
      "Vedant Shah",
      "Muntasir Wahed",
      "Ying Shen",
      "Kiet A. Nguyen",
      "Ismini Lourentzou"
    ],
    "published": "2025-06-20T17:59:12+00:00",
    "summary": "Articulated objects are common in the real world, yet modeling their structure and motion remains a challenging task for 3D reconstruction methods. In this work, we introduce Part$^{2}$GS, a novel framework for modeling articulated digital twins of multi-part objects with high-fidelity geometry and physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D Gaussian representation that encodes articulated components with learnable attributes, enabling structured, disentangled transformations that preserve high-fidelity geometry. To ensure physically consistent motion, we propose a motion-aware canonical representation guided by physics-based constraints, including contact enforcement, velocity consistency, and vector-field alignment. Furthermore, we introduce a field of repel points to prevent part collisions and maintain stable articulation paths, significantly improving motion coherence over baselines. Extensive evaluations on both synthetic and real-world datasets show that Part$^{2}$GS consistently outperforms state-of-the-art methods by up to 10$\\times$ in Chamfer Distance for movable parts."
  },
  {
    "title": "BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning",
    "url": "http://arxiv.org/abs/2506.17211v1",
    "arxiv_id": "2506.17211v1",
    "authors": [
      "Xuechen Zhang",
      "Zijian Huang",
      "Yingcong Li",
      "Chenshun Ni",
      "Jiasi Chen",
      "Samet Oymak"
    ],
    "published": "2025-06-20T17:59:07+00:00",
    "summary": "Small language models (SLMs) struggle to learn complex reasoning behaviors, especially when high-quality traces are scarce or difficult to learn from. The standard training approach combines a supervised fine-tuning (SFT) stage, often to distill capabilities of a larger model, followed by a reinforcement learning (RL)stage such as Group Relative Policy Optimization (GRPO). In this paper, we investigate the fundamental limitations of this SFT + RL paradigm and propose methods to overcome them. Under a suitable theoretical model, we demonstrate that the SFT + RL strategy can fail completely when (1) the expert's traces are too difficult for the small model to express, or (2) the small model's initialization has exponentially small likelihood of success. To address these, we introduce BREAD: a GRPO variant that unifies the SFT and RL stages via partial expert guidance and branched rollouts. When self-generated traces fail, BREAD adaptively inserts short expert prefixes/hints, allowing the small model to complete the rest of the reasoning path, and ensuring that each update includes at least one successful trace. This mechanism both densifies the reward signal and induces a natural learning curriculum. BREAD requires fewer than 40% of ground-truth traces, consistently outperforming standard GRPO while speeding up the training by about 3 times. Importantly, we demonstrate that BREAD helps the model solve problems that are otherwise unsolvable by the SFT + RL strategy, highlighting how branched rollouts and expert guidance can substantially boost SLM reasoning."
  },
  {
    "title": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards",
    "url": "http://arxiv.org/abs/2506.15684v1",
    "arxiv_id": "2506.15684v1",
    "authors": [
      "Qingming Liu",
      "Zhen Liu",
      "Dinghuai Zhang",
      "Kui Jia"
    ],
    "published": "2025-06-18T17:59:59+00:00",
    "summary": "Generating high-quality and photorealistic 3D assets remains a longstanding challenge in 3D vision and computer graphics. Although state-of-the-art generative models, such as diffusion models, have made significant progress in 3D generation, they often fall short of human-designed content due to limited ability to follow instructions, align with human preferences, or produce realistic textures, geometries, and physical attributes. In this paper, we introduce Nabla-R2D3, a highly effective and sample-efficient reinforcement learning alignment framework for 3D-native diffusion models using 2D rewards. Built upon the recently proposed Nabla-GFlowNet method, which matches the score function to reward gradients in a principled manner for reward finetuning, our Nabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D reward signals. Extensive experiments show that, unlike vanilla finetuning baselines which either struggle to converge or suffer from reward hacking, Nabla-R2D3 consistently achieves higher rewards and reduced prior forgetting within a few finetuning steps."
  },
  {
    "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos",
    "url": "http://arxiv.org/abs/2506.15680v1",
    "arxiv_id": "2506.15680v1",
    "authors": [
      "Kaifeng Zhang",
      "Baoyu Li",
      "Kris Hauser",
      "Yunzhu Li"
    ],
    "published": "2025-06-18T17:59:38+00:00",
    "summary": "Modeling the dynamics of deformable objects is challenging due to their diverse physical properties and the difficulty of estimating states from limited visual information. We address these challenges with a neural dynamics framework that combines object particles and spatial grids in a hybrid representation. Our particle-grid model captures global shape and motion information while predicting dense particle movements, enabling the modeling of objects with varied shapes and materials. Particles represent object shapes, while the spatial grid discretizes the 3D space to ensure spatial continuity and enhance learning efficiency. Coupled with Gaussian Splattings for visual rendering, our framework achieves a fully learning-based digital twin of deformable objects and generates 3D action-conditioned videos. Through experiments, we demonstrate that our model learns the dynamics of diverse objects -- such as ropes, cloths, stuffed animals, and paper bags -- from sparse-view RGB-D recordings of robot-object interactions, while also generalizing at the category level to unseen instances. Our approach outperforms state-of-the-art learning-based and physics-based simulators, particularly in scenarios with limited camera views. Furthermore, we showcase the utility of our learned models in model-based planning, enabling goal-conditioned object manipulation across a range of tasks. The project page is available at https://kywind.github.io/pgnd ."
  },
  {
    "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence",
    "url": "http://arxiv.org/abs/2506.15677v1",
    "arxiv_id": "2506.15677v1",
    "authors": [
      "Yining Hong",
      "Rui Sun",
      "Bingxuan Li",
      "Xingcheng Yao",
      "Maxine Wu",
      "Alexander Chien",
      "Da Yin",
      "Ying Nian Wu",
      "Zhecan James Wang",
      "Kai-Wei Chang"
    ],
    "published": "2025-06-18T17:58:17+00:00",
    "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/."
  },
  {
    "title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers",
    "url": "http://arxiv.org/abs/2506.15674v1",
    "arxiv_id": "2506.15674v1",
    "authors": [
      "Tommaso Green",
      "Martin Gubri",
      "Haritz Puerto",
      "Sangdoo Yun",
      "Seong Joon Oh"
    ],
    "published": "2025-06-18T17:57:01+00:00",
    "summary": "We study privacy leakage in the reasoning traces of large reasoning models used as personal agents. Unlike final outputs, reasoning traces are often assumed to be internal and safe. We challenge this assumption by showing that reasoning traces frequently contain sensitive user data, which can be extracted via prompt injections or accidentally leak into outputs. Through probing and agentic evaluations, we demonstrate that test-time compute approaches, particularly increased reasoning steps, amplify such leakage. While increasing the budget of those test-time compute approaches makes models more cautious in their final answers, it also leads them to reason more verbosely and leak more in their own thinking. This reveals a core tension: reasoning improves utility but enlarges the privacy attack surface. We argue that safety efforts must extend to the model's internal thinking, not just its outputs."
  },
  {
    "title": "Vision in Action: Learning Active Perception from Human Demonstrations",
    "url": "http://arxiv.org/abs/2506.15666v1",
    "arxiv_id": "2506.15666v1",
    "authors": [
      "Haoyu Xiong",
      "Xiaomeng Xu",
      "Jimmy Wu",
      "Yifan Hou",
      "Jeannette Bohg",
      "Shuran Song"
    ],
    "published": "2025-06-18T17:43:55+00:00",
    "summary": "We present Vision in Action (ViA), an active perception system for bimanual robot manipulation. ViA learns task-relevant active perceptual strategies (e.g., searching, tracking, and focusing) directly from human demonstrations. On the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to enable flexible, human-like head movements. To capture human active perception strategies, we design a VR-based teleoperation interface that creates a shared observation space between the robot and the human operator. To mitigate VR motion sickness caused by latency in the robot's physical movements, the interface uses an intermediate 3D scene representation, enabling real-time view rendering on the operator side while asynchronously updating the scene with the robot's latest observations. Together, these design elements enable the learning of robust visuomotor policies for three complex, multi-stage bimanual manipulation tasks involving visual occlusions, significantly outperforming baseline systems."
  },
  {
    "title": "GMT: General Motion Tracking for Humanoid Whole-Body Control",
    "url": "http://arxiv.org/abs/2506.14770v1",
    "arxiv_id": "2506.14770v1",
    "authors": [
      "Zixuan Chen",
      "Mazeyu Ji",
      "Xuxin Cheng",
      "Xuanbin Peng",
      "Xue Bin Peng",
      "Xiaolong Wang"
    ],
    "published": "2025-06-17T17:59:33+00:00",
    "summary": "The ability to track general whole-body motions in the real world is a useful way to build general-purpose humanoid robots. However, achieving this can be challenging due to the temporal and kinematic diversity of the motions, the policy's capability, and the difficulty of coordination of the upper and lower bodies. To address these issues, we propose GMT, a general and scalable motion-tracking framework that trains a single unified policy to enable humanoid robots to track diverse motions in the real world. GMT is built upon two core components: an Adaptive Sampling strategy and a Motion Mixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically balances easy and difficult motions during training. The MoE ensures better specialization of different regions of the motion manifold. We show through extensive experiments in both simulation and the real world the effectiveness of GMT, achieving state-of-the-art performance across a broad spectrum of motions using a unified general policy. Videos and additional information can be found at https://gmt-humanoid.github.io."
  },
  {
    "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion",
    "url": "http://arxiv.org/abs/2506.14769v1",
    "arxiv_id": "2506.14769v1",
    "authors": [
      "Jiahua Ma",
      "Yiran Qin",
      "Yixiong Li",
      "Xuanqi Liao",
      "Yulan Guo",
      "Ruimao Zhang"
    ],
    "published": "2025-06-17T17:59:12+00:00",
    "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions."
  },
  {
    "title": "RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills",
    "url": "http://arxiv.org/abs/2506.14763v1",
    "arxiv_id": "2506.14763v1",
    "authors": [
      "Chunru Lin",
      "Haotian Yuan",
      "Yian Wang",
      "Xiaowen Qiu",
      "Tsun-Hsuan Wang",
      "Minghao Guo",
      "Bohan Wang",
      "Yashraj Narang",
      "Dieter Fox",
      "Chuang Gan"
    ],
    "published": "2025-06-17T17:57:37+00:00",
    "summary": "Endowing robots with tool design abilities is critical for enabling them to solve complex manipulation tasks that would otherwise be intractable. While recent generative frameworks can automatically synthesize task settings, such as 3D scenes and reward functions, they have not yet addressed the challenge of tool-use scenarios. Simply retrieving human-designed tools might not be ideal since many tools (e.g., a rolling pin) are difficult for robotic manipulators to handle. Furthermore, existing tool design approaches either rely on predefined templates with limited parameter tuning or apply generic 3D generation methods that are not optimized for tool creation. To address these limitations, we propose RobotSmith, an automated pipeline that leverages the implicit physical knowledge embedded in vision-language models (VLMs) alongside the more accurate physics provided by physics simulations to design and use tools for robotic manipulation. Our system (1) iteratively proposes tool designs using collaborative VLM agents, (2) generates low-level robot trajectories for tool use, and (3) jointly optimizes tool geometry and usage for task performance. We evaluate our approach across a wide range of manipulation tasks involving rigid, deformable, and fluid objects. Experiments show that our method consistently outperforms strong baselines in terms of both task success rate and overall performance. Notably, our approach achieves a 50.0\\% average success rate, significantly surpassing other baselines such as 3D generation (21.4%) and tool retrieval (11.1%). Finally, we deploy our system in real-world settings, demonstrating that the generated tools and their usage plans transfer effectively to physical execution, validating the practicality and generalization capabilities of our approach."
  },
  {
    "title": "Markov Regime-Switching Intelligent Driver Model for Interpretable Car-Following Behavior",
    "url": "http://arxiv.org/abs/2506.14762v1",
    "arxiv_id": "2506.14762v1",
    "authors": [
      "Chengyuan Zhang",
      "Cathy Wu",
      "Lijun Sun"
    ],
    "published": "2025-06-17T17:55:42+00:00",
    "summary": "Accurate and interpretable car-following models are essential for traffic simulation and autonomous vehicle development. However, classical models like the Intelligent Driver Model (IDM) are fundamentally limited by their parsimonious and single-regime structure. They fail to capture the multi-modal nature of human driving, where a single driving state (e.g., speed, relative speed, and gap) can elicit many different driver actions. This forces the model to average across distinct behaviors, reducing its fidelity and making its parameters difficult to interpret. To overcome this, we introduce a regime-switching framework that allows driving behavior to be governed by different IDM parameter sets, each corresponding to an interpretable behavioral mode. This design enables the model to dynamically switch between interpretable behavioral modes, rather than averaging across diverse driving contexts. We instantiate the framework using a Factorial Hidden Markov Model with IDM dynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes (e.g., aggressive acceleration, steady-state following) from external traffic scenarios (e.g., free-flow, congestion, stop-and-go) through two independent latent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC) is used to jointly estimate the regime-specific parameters, transition dynamics, and latent state trajectories. Experiments on the HighD dataset demonstrate that FHMM-IDM uncovers interpretable structure in human driving, effectively disentangling internal driver actions from contextual traffic conditions and revealing dynamic regime-switching patterns. This framework provides a tractable and principled solution to modeling context-dependent driving behavior under uncertainty, offering improvements in the fidelity of traffic simulations, the efficacy of safety analyses, and the development of more human-centric ADAS."
  },
  {
    "title": "Fully Tunable Strong Spin-Orbit Interactions in Light Hole Germanium Quantum Channels",
    "url": "http://arxiv.org/abs/2506.14759v1",
    "arxiv_id": "2506.14759v1",
    "authors": [
      "Patrick Del Vecchio",
      "Stefano Bosco",
      "Daniel Loss",
      "Oussama Moutanabbir"
    ],
    "published": "2025-06-17T17:54:29+00:00",
    "summary": "Spin-orbit interaction (SOI) is a fundamental component for electrically driven spin qubits and hybrid superconducting-semiconducting systems. In particular, Rashba SOI (RSOI) is a key mechanism enabling all-electrical spin manipulation schemes. However, in common planar systems, RSOI is weak because of the small mixing between heavy holes (HH) and light holes (LH), and instead relies on complex strain and interface phenomena that are hard to reliably harness in experiment. Here, MOS-like epitaxial Ge on relaxed \\GeSn{} is introduced and shown to exhibit an inherently large, highly gate-tunable RSOI that is compatible with both spin qubits and hybrid devices. This large RSOI is a consequence of the LH-like ground state in Ge. Notably, the built-in asymmetry of the device causes the RSOI to completely vanish at specific gate fields, effectively acting as an on/off SOI switch. The LH $g$-tensor is less anisotropic than that of state-of-the-art HH qubits, alleviating precise magnetic field orientation requirements. The large in-plane $g$-factor also facilitates the integration of superconductors. Moreover, the out-of-plane $g$-factor is strongly gate-tunable and completely vanishes at specific gate fields. Thus, this material system combines the large RSOI with the scalability of planar devices, paving the way towards robust spin qubit applications and enabling access to new regimes of complex spin physics."
  },
  {
    "title": "Touch begins where vision ends: Generalizable policies for contact-rich manipulation",
    "url": "http://arxiv.org/abs/2506.13762v1",
    "arxiv_id": "2506.13762v1",
    "authors": [
      "Zifan Zhao",
      "Siddhant Haldar",
      "Jinda Cui",
      "Lerrel Pinto",
      "Raunaq Bhirangi"
    ],
    "published": "2025-06-16T17:59:48+00:00",
    "summary": "Data-driven approaches struggle with precise manipulation; imitation learning requires many hard-to-obtain demonstrations, while reinforcement learning yields brittle, non-generalizable policies. We introduce VisuoTactile Local (ViTaL) policy learning, a framework that solves fine-grained manipulation tasks by decomposing them into two phases: a reaching phase, where a vision-language model (VLM) enables scene-level reasoning to localize the object of interest, and a local interaction phase, where a reusable, scene-agnostic ViTaL policy performs contact-rich manipulation using egocentric vision and tactile sensing. This approach is motivated by the observation that while scene context varies, the low-level interaction remains consistent across task instances. By training local policies once in a canonical setting, they can generalize via a localize-then-execute strategy. ViTaL achieves around 90% success on contact-rich tasks in unseen environments and is robust to distractors. ViTaL's effectiveness stems from three key insights: (1) foundation models for segmentation enable training robust visual encoders via behavior cloning; (2) these encoders improve the generalizability of policies learned using residual RL; and (3) tactile sensing significantly boosts performance in contact-rich tasks. Ablation studies validate each of these insights, and we demonstrate that ViTaL integrates well with high-level VLMs, enabling robust, reusable low-level skills. Results and videos are available at https://vitalprecise.github.io."
  },
  {
    "title": "Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins",
    "url": "http://arxiv.org/abs/2506.13761v1",
    "arxiv_id": "2506.13761v1",
    "authors": [
      "Chuanruo Ning",
      "Kuan Fang",
      "Wei-Chiu Ma"
    ],
    "published": "2025-06-16T17:59:33+00:00",
    "summary": "Recent advancements in open-world robot manipulation have been largely driven by vision-language models (VLMs). While these models exhibit strong generalization ability in high-level planning, they struggle to predict low-level robot controls due to limited physical-world understanding. To address this issue, we propose a model predictive control framework for open-world manipulation that combines the semantic reasoning capabilities of VLMs with physically-grounded, interactive digital twins of the real-world environments. By constructing and simulating the digital twins, our approach generates feasible motion trajectories, simulates corresponding outcomes, and prompts the VLM with future observations to evaluate and select the most suitable outcome based on language instructions of the task. To further enhance the capability of pre-trained VLMs in understanding complex scenes for robotic control, we leverage the flexible rendering capabilities of the digital twin to synthesize the scene at various novel, unoccluded viewpoints. We validate our approach on a diverse set of complex manipulation tasks, demonstrating superior performance compared to baseline methods for language-conditioned robotic control using VLMs."
  },
  {
    "title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2506.13757v1",
    "arxiv_id": "2506.13757v1",
    "authors": [
      "Zewei Zhou",
      "Tianhui Cai",
      "Seth Z. Zhao",
      "Yun Zhang",
      "Zhiyu Huang",
      "Bolei Zhou",
      "Jiaqi Ma"
    ],
    "published": "2025-06-16T17:58:50+00:00",
    "summary": "Recent advancements in Vision-Language-Action (VLA) models have shown promise for end-to-end autonomous driving by leveraging world knowledge and reasoning capabilities. However, current VLA models often struggle with physically infeasible action outputs, complex model structures, or unnecessarily long reasoning. In this paper, we propose AutoVLA, a novel VLA model that unifies reasoning and action generation within a single autoregressive generation model for end-to-end autonomous driving. AutoVLA performs semantic reasoning and trajectory planning directly from raw visual inputs and language instructions. We tokenize continuous trajectories into discrete, feasible actions, enabling direct integration into the language model. For training, we employ supervised fine-tuning to equip the model with dual thinking modes: fast thinking (trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning). To further enhance planning performance and efficiency, we introduce a reinforcement fine-tuning method based on Group Relative Policy Optimization (GRPO), reducing unnecessary reasoning in straightforward scenarios. Extensive experiments across real-world and simulated datasets and benchmarks, including nuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of AutoVLA in both open-loop and closed-loop settings. Qualitative results showcase the adaptive reasoning and accurate planning capabilities of AutoVLA in diverse scenarios."
  },
  {
    "title": "MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering",
    "url": "http://arxiv.org/abs/2506.13755v1",
    "arxiv_id": "2506.13755v1",
    "authors": [
      "Arya Fayyazi",
      "Mehdi Kamal",
      "Massoud Pedram"
    ],
    "published": "2025-06-16T17:58:09+00:00",
    "summary": "This paper introduces MARCO (Multi-Agent Reinforcement learning with Conformal Optimization), a novel hardware-aware framework for efficient neural architecture search (NAS) targeting resource-constrained edge devices. By significantly reducing search time and maintaining accuracy under strict hardware constraints, MARCO bridges the gap between automated DNN design and CAD for edge AI deployment. MARCO's core technical contribution lies in its unique combination of multi-agent reinforcement learning (MARL) with Conformal Prediction (CP) to accelerate the hardware/software co-design process for deploying deep neural networks. Unlike conventional once-for-all (OFA) supernet approaches that require extensive pretraining, MARCO decomposes the NAS task into a hardware configuration agent (HCA) and a Quantization Agent (QA). The HCA optimizes high-level design parameters, while the QA determines per-layer bit-widths under strict memory and latency budgets using a shared reward signal within a centralized-critic, decentralized-execution (CTDE) paradigm. A key innovation is the integration of a calibrated CP surrogate model that provides statistical guarantees (with a user-defined miscoverage rate) to prune unpromising candidate architectures before incurring the high costs of partial training or hardware simulation. This early filtering drastically reduces the search space while ensuring that high-quality designs are retained with a high probability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that MARCO achieves a 3-4x reduction in total search time compared to an OFA baseline while maintaining near-baseline accuracy (within 0.3%). Furthermore, MARCO also reduces inference latency. Validation on a MAX78000 evaluation board confirms that simulator trends hold in practice, with simulator estimates deviating from measured values by less than 5%."
  },
  {
    "title": "Edge Nearest Neighbor in Sampling-Based Motion Planning",
    "url": "http://arxiv.org/abs/2506.13753v1",
    "arxiv_id": "2506.13753v1",
    "authors": [
      "Stav Ashur",
      "Nancy M. Amato",
      "Sariel Har-Peled"
    ],
    "published": "2025-06-16T17:57:41+00:00",
    "summary": "Neighborhood finders and nearest neighbor queries are fundamental parts of sampling based motion planning algorithms. Using different distance metrics or otherwise changing the definition of a neighborhood produces different algorithms with unique empiric and theoretical properties. In \\cite{l-pa-06} LaValle suggests a neighborhood finder for the Rapidly-exploring Random Tree RRT   algorithm \\cite{l-rrtnt-98} which finds the nearest neighbor of the sampled point on the swath of the tree, that is on the set of all of the points on the tree edges, using a hierarchical data structure. In this paper we implement such a neighborhood finder and show, theoretically and experimentally, that this results in more efficient algorithms, and suggest a variant of the Rapidly-exploring Random Graph RRG algorithm \\cite{f-isaom-10} that better exploits the exploration properties of the newly described subroutine for finding narrow passages."
  },
  {
    "title": "Compression Aware Certified Training",
    "url": "http://arxiv.org/abs/2506.11992v1",
    "arxiv_id": "2506.11992v1",
    "authors": [
      "Changming Xu",
      "Gagandeep Singh"
    ],
    "published": "2025-06-13T17:48:50+00:00",
    "summary": "Deep neural networks deployed in safety-critical, resource-constrained environments must balance efficiency and robustness. Existing methods treat compression and certified robustness as separate goals, compromising either efficiency or safety. We propose CACTUS (Compression Aware Certified Training Using network Sets), a general framework for unifying these objectives during training. CACTUS models maintain high certified accuracy even when compressed. We apply CACTUS for both pruning and quantization and show that it effectively trains models which can be efficiently compressed while maintaining high accuracy and certifiable robustness. CACTUS achieves state-of-the-art accuracy and certified performance for both pruning and quantization on a variety of datasets and input specifications."
  },
  {
    "title": "Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task",
    "url": "http://arxiv.org/abs/2506.11986v1",
    "arxiv_id": "2506.11986v1",
    "authors": [
      "Wuzhenghong Wen",
      "Su Pan",
      "yuwei Sun"
    ],
    "published": "2025-06-13T17:46:02+00:00",
    "summary": "Schema linking is a critical step in Text-to-SQL task, aiming to accurately predict the table names and column names required for the SQL query based on the given question. However, current fine-tuning approaches for schema linking models employ a rote-learning paradigm, excessively optimizing for ground truth schema linking outcomes while compromising reasoning ability. This limitation arises because of the difficulty in acquiring a high-quality reasoning sample for downstream tasks. To address this, we propose Schema-R1, a reasoning schema linking model trained using reinforcement learning. Specifically, Schema-R1 consists of three key steps: constructing small batches of high-quality reasoning samples, supervised fine-tuning for cold-start initialization, and rule-based reinforcement learning training. The final results demonstrate that our method effectively enhances the reasoning ability of the schema linking model, achieving a 10\\% improvement in filter accuracy compared to the existing method. Our code is available at https://github.com/hongWin/Schema-R1/."
  },
  {
    "title": "Self-Regulating Cars: Automating Traffic Control in Free Flow Road Networks",
    "url": "http://arxiv.org/abs/2506.11973v1",
    "arxiv_id": "2506.11973v1",
    "authors": [
      "Ankit Bhardwaj",
      "Rohail Asim",
      "Sachin Chauhan",
      "Yasir Zaki",
      "Lakshminarayanan Subramanian"
    ],
    "published": "2025-06-13T17:31:23+00:00",
    "summary": "Free-flow road networks, such as suburban highways, are increasingly experiencing traffic congestion due to growing commuter inflow and limited infrastructure. Traditional control mechanisms, such as traffic signals or local heuristics, are ineffective or infeasible in these high-speed, signal-free environments. We introduce self-regulating cars, a reinforcement learning-based traffic control protocol that dynamically modulates vehicle speeds to optimize throughput and prevent congestion, without requiring new physical infrastructure. Our approach integrates classical traffic flow theory, gap acceptance models, and microscopic simulation into a physics-informed RL framework. By abstracting roads into super-segments, the agent captures emergent flow dynamics and learns robust speed modulation policies from instantaneous traffic observations. Evaluated in the high-fidelity PTV Vissim simulator on a real-world highway network, our method improves total throughput by 5%, reduces average delay by 13%, and decreases total stops by 3% compared to the no-control setting. It also achieves smoother, congestion-resistant flow while generalizing across varied traffic patterns, demonstrating its potential for scalable, ML-driven traffic management."
  },
  {
    "title": "Visual Pre-Training on Unlabeled Images using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.11967v1",
    "arxiv_id": "2506.11967v1",
    "authors": [
      "Dibya Ghosh",
      "Sergey Levine"
    ],
    "published": "2025-06-13T17:25:27+00:00",
    "summary": "In reinforcement learning (RL), value-based algorithms learn to associate each observation with the states and rewards that are likely to be reached from it. We observe that many self-supervised image pre-training methods bear similarity to this formulation: learning features that associate crops of images with those of nearby views, e.g., by taking a different crop or color augmentation. In this paper, we complete this analogy and explore a method that directly casts pre-training on unlabeled image data like web crawls and video frames as an RL problem. We train a general value function in a dynamical system where an agent transforms an image by changing the view or adding image augmentations. Learning in this way resembles crop-consistency self-supervision, but through the reward function, offers a simple lever to shape feature learning using curated images or weakly labeled captions when they exist. Our experiments demonstrate improved representations when training on unlabeled images in the wild, including video data like EpicKitchens, scene data like COCO, and web-crawl data like CC12M."
  },
  {
    "title": "Universal cooling of quantum systems via randomized measurements",
    "url": "http://arxiv.org/abs/2506.11964v1",
    "arxiv_id": "2506.11964v1",
    "authors": [
      "Josias Langbehn",
      "George Mouloudakis",
      "Emma King",
      "Rapha\u00ebl Menu",
      "Igor Gornyi",
      "Giovanna Morigi",
      "Yuval Gefen",
      "Christiane P. Koch"
    ],
    "published": "2025-06-13T17:20:01+00:00",
    "summary": "It is generally believed that the design of cooling protocols requires knowledge of the system's spectrum. In contrast, cooling processes in nature occur whenever the system is coupled to a cold bath. But how does nature know how to cool quantum systems? Here we address this question by mimicking a natural cold bath with a reservoir of \"meter\" qubits that are initialized in their ground state and interact with the system sequentially for a specified time before being discarded. This protocol is equivalent to quantum measurements without keeping the measurement readouts. We show that a quantum system can be cooled without prior knowledge of the system's details when the interactions between the system and the meters, as well as the level splittings of the meters, are chosen randomly. For sufficiently small interaction strengths and long interaction times, the rotating-wave approximation becomes valid, ensuring that resonant energy-exchange processes, which lead to cooling, dominate over heating. This demonstrates that robust and scalable cooling of complex quantum systems can be achieved through generic, structure-independent protocols. Our findings thus offer a versatile universal framework for engineering, controlling, and manipulating quantum matter far from equilibrium, in particular, for the purposes of quantum information processing and quantum simulations."
  },
  {
    "title": "Fine-Grained Perturbation Guidance via Attention Head Selection",
    "url": "http://arxiv.org/abs/2506.10978v1",
    "arxiv_id": "2506.10978v1",
    "authors": [
      "Donghoon Ahn",
      "Jiwon Kang",
      "Sanghyun Lee",
      "Minjae Kim",
      "Jaewon Min",
      "Wooseok Jang",
      "Saungwu Lee",
      "Sayak Paul",
      "Susung Hong",
      "Seungryong Kim"
    ],
    "published": "2025-06-12T17:59:51+00:00",
    "summary": "Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose \"HeadHunter\", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies."
  },
  {
    "title": "Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop",
    "url": "http://arxiv.org/abs/2506.10968v1",
    "arxiv_id": "2506.10968v1",
    "authors": [
      "Justin Kerr",
      "Kush Hari",
      "Ethan Weber",
      "Chung Min Kim",
      "Brent Yi",
      "Tyler Bonnen",
      "Ken Goldberg",
      "Angjoo Kanazawa"
    ],
    "published": "2025-06-12T17:59:11+00:00",
    "summary": "Humans do not passively observe the visual world -- we actively look in order to act. Motivated by this principle, we introduce EyeRobot, a robotic system with gaze behavior that emerges from the need to complete real-world tasks. We develop a mechanical eyeball that can freely rotate to observe its surroundings and train a gaze policy to control it using reinforcement learning. We accomplish this by first collecting teleoperated demonstrations paired with a 360 camera. This data is imported into a simulation environment that supports rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze on top of robot demonstrations. We then introduce a BC-RL loop to train the hand and eye jointly: the hand (BC) agent is trained from rendered eye observations, and the eye (RL) agent is rewarded when the hand produces correct action predictions. In this way, hand-eye coordination emerges as the eye looks towards regions which allow the hand to complete the task. EyeRobot implements a foveal-inspired policy architecture allowing high resolution with a small compute budget, which we find also leads to the emergence of more stable fixation as well as improved ability to track objects and ignore distractors. We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring manipulation in an arc surrounding the robot arm. Our experiments suggest EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate manipulation over large workspaces with a single camera. See project site for videos: https://www.eyerobot.net/"
  },
  {
    "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation",
    "url": "http://arxiv.org/abs/2506.10966v1",
    "arxiv_id": "2506.10966v1",
    "authors": [
      "Ning Gao",
      "Yilun Chen",
      "Shuai Yang",
      "Xinyi Chen",
      "Yang Tian",
      "Hao Li",
      "Haifeng Huang",
      "Hanqing Wang",
      "Tai Wang",
      "Jiangmiao Pang"
    ],
    "published": "2025-06-12T17:59:04+00:00",
    "summary": "Robotic manipulation in real-world settings remains challenging, especially regarding robust generalization. Existing simulation platforms lack sufficient support for exploring how policies adapt to varied instructions and scenarios. Thus, they lag behind the growing interest in instruction-following foundation models like LLMs, whose adaptability is crucial yet remains underexplored in fair comparisons. To bridge this gap, we introduce GenManip, a realistic tabletop simulation platform tailored for policy generalization studies. It features an automatic pipeline via LLM-driven task-oriented scene graph to synthesize large-scale, diverse tasks using 10K annotated 3D object assets. To systematically assess generalization, we present GenManip-Bench, a benchmark of 200 scenarios refined via human-in-the-loop corrections. We evaluate two policy types: (1) modular manipulation systems integrating foundation models for perception, reasoning, and planning, and (2) end-to-end policies trained through scalable data collection. Results show that while data scaling benefits end-to-end methods, modular systems enhanced with foundation models generalize more effectively across diverse scenarios. We anticipate this platform to facilitate critical insights for advancing policy generalization in realistic conditions. Project Page: https://genmanip.axi404.top/."
  },
  {
    "title": "Build the web for agents, not agents for the web",
    "url": "http://arxiv.org/abs/2506.10953v1",
    "arxiv_id": "2506.10953v1",
    "authors": [
      "Xing Han L\u00f9",
      "Gaurav Kamath",
      "Marius Mosbach",
      "Siva Reddy"
    ],
    "published": "2025-06-12T17:53:58+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community."
  },
  {
    "title": "Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors",
    "url": "http://arxiv.org/abs/2506.10949v1",
    "arxiv_id": "2506.10949v1",
    "authors": [
      "Chen Yueh-Han",
      "Nitish Joshi",
      "Yulin Chen",
      "Maksym Andriushchenko",
      "Rico Angell",
      "He He"
    ],
    "published": "2025-06-12T17:50:58+00:00",
    "summary": "Current LLM safety defenses fail under decomposition attacks, where a malicious goal is decomposed into benign subtasks that circumvent refusals. The challenge lies in the existing shallow safety alignment techniques: they only detect harm in the immediate prompt and do not reason about long-range intent, leaving them blind to malicious intent that emerges over a sequence of seemingly benign instructions. We therefore propose adding an external monitor that observes the conversation at a higher granularity. To facilitate our study of monitoring decomposition attacks, we curate the largest and most diverse dataset to date, including question-answering, text-to-image, and agentic tasks. We verify our datasets by testing them on frontier LLMs and show an 87% attack success rate on average on GPT-4o. This confirms that decomposition attack is broadly effective. Additionally, we find that random tasks can be injected into the decomposed subtasks to further obfuscate malicious intents. To defend in real time, we propose a lightweight sequential monitoring framework that cumulatively evaluates each subtask. We show that a carefully prompt engineered lightweight monitor achieves a 93% defense success rate, beating reasoning models like o3 mini as a monitor. Moreover, it remains robust against random task injection and cuts cost by 90% and latency by 50%. Our findings suggest that lightweight sequential monitors are highly effective in mitigating decomposition attacks and are viable in deployment."
  },
  {
    "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring",
    "url": "http://arxiv.org/abs/2506.09996v1",
    "arxiv_id": "2506.09996v1",
    "authors": [
      "Yang Li",
      "Qiang Sheng",
      "Yehan Yang",
      "Xueyao Zhang",
      "Juan Cao"
    ],
    "published": "2025-06-11T17:59:58+00:00",
    "summary": "Though safety alignment has been applied to most large language models (LLMs), LLM service providers generally deploy a subsequent moderation as the external safety guardrail in real-world products. Existing moderators mainly practice a conventional full detection, which determines the harmfulness based on the complete LLM output, causing high service latency. Recent works pay more attention to partial detection where moderators oversee the generation midway and early stop the output if harmfulness is detected, but they directly apply moderators trained with the full detection paradigm to incomplete outputs, introducing a training-inference gap that lowers the performance. In this paper, we explore how to form a data-and-model solution that natively supports partial detection. For the data, we construct FineHarm, a dataset consisting of 29K prompt-response pairs with fine-grained annotations to provide reasonable supervision for token-level training. Then, we propose the streaming content monitor, which is trained with dual supervision of response- and token-level labels and can follow the output stream of LLM to make a timely judgment of harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is comparable to full detection, by only seeing the first 18% of tokens in responses on average. Moreover, the SCM can serve as a pseudo-harmfulness annotator for improving safety alignment and lead to a higher harmlessness score than DPO."
  },
  {
    "title": "eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures",
    "url": "http://arxiv.org/abs/2506.09994v1",
    "arxiv_id": "2506.09994v1",
    "authors": [
      "Venkatesh Pattabiraman",
      "Zizhou Huang",
      "Daniele Panozzo",
      "Denis Zorin",
      "Lerrel Pinto",
      "Raunaq Bhirangi"
    ],
    "published": "2025-06-11T17:59:46+00:00",
    "summary": "If human experience is any guide, operating effectively in unstructured environments -- like homes and offices -- requires robots to sense the forces during physical interaction. Yet, the lack of a versatile, accessible, and easily customizable tactile sensor has led to fragmented, sensor-specific solutions in robotic manipulation -- and in many cases, to force-unaware, sensorless approaches. With eFlesh, we bridge this gap by introducing a magnetic tactile sensor that is low-cost, easy to fabricate, and highly customizable. Building an eFlesh sensor requires only four components: a hobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired shape, and a magnetometer circuit board. The sensor is constructed from tiled, parameterized microstructures, which allow for tuning the sensor's geometry and its mechanical response. We provide an open-source design tool that converts convex OBJ/STL files into 3D-printable STLs for fabrication. This modular design framework enables users to create application-specific sensors, and to adjust sensitivity depending on the task. Our sensor characterization experiments demonstrate the capabilities of eFlesh: contact localization RMSE of 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for shear force. We also present a learned slip detection model that generalizes to unseen objects with 95% accuracy, and visuotactile control policies that improve manipulation performance by 40% over vision-only baselines -- achieving 91% average success rate for four precise tasks that require sub-mm accuracy for successful completion. All design files, code and the CAD-to-eFlesh STL conversion tool are open-sourced and available on https://e-flesh.com."
  },
  {
    "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation",
    "url": "http://arxiv.org/abs/2506.09990v1",
    "arxiv_id": "2506.09990v1",
    "authors": [
      "Wenbo Zhang",
      "Tianrun Hu",
      "Yanyuan Qiao",
      "Hanbo Zhang",
      "Yuchu Qin",
      "Yang Li",
      "Jiajun Liu",
      "Tao Kong",
      "Lingqiao Liu",
      "Xiao Ma"
    ],
    "published": "2025-06-11T17:59:13+00:00",
    "summary": "We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built upon Trajectory Autoregressive Modeling. Unlike conventional approaches that predict next step action(s) forward, CoA generates an entire trajectory by explicit backward reasoning with task-specific goals through an action-level Chain-of-Thought (CoT) process. This process is unified within a single autoregressive structure: (1) the first token corresponds to a stable keyframe action that encodes the task-specific goals; and (2) subsequent action tokens are generated autoregressively, conditioned on the initial keyframe and previously predicted actions. This backward action reasoning enforces a global-to-local structure, allowing each local action to be tightly constrained by the final goal. To further realize the action reasoning structure, CoA incorporates four complementary designs: continuous action token representation; dynamic stopping for variable-length trajectory generation; reverse temporal ensemble; and multi-token prediction to balance action chunk modeling with global structure. As a result, CoA gives strong spatial generalization capabilities while preserving the flexibility and simplicity of a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art performance across 60 RLBench tasks and 8 real-world manipulation tasks."
  },
  {
    "title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes",
    "url": "http://arxiv.org/abs/2506.09989v1",
    "arxiv_id": "2506.09989v1",
    "authors": [
      "Yiming Dou",
      "Wonseok Oh",
      "Yuqing Luo",
      "Antonio Loquercio",
      "Andrew Owens"
    ],
    "published": "2025-06-11T17:58:34+00:00",
    "summary": "We study the problem of making 3D scene reconstructions interactive by asking the following question: can we predict the sounds of human hands physically interacting with a scene? First, we record a video of a human manipulating objects within a 3D scene using their hands. We then use these action-sound pairs to train a rectified flow model to map 3D hand trajectories to their corresponding audio. At test time, a user can query the model for other actions, parameterized as sequences of hand poses, to estimate their corresponding sounds. In our experiments, we find that our generated sounds accurately convey material properties and actions, and that they are often indistinguishable to human observers from real sounds. Project page: https://www.yimingdou.com/hearing_hands/"
  },
  {
    "title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs",
    "url": "http://arxiv.org/abs/2506.09987v1",
    "arxiv_id": "2506.09987v1",
    "authors": [
      "Benno Krojer",
      "Mojtaba Komeili",
      "Candace Ross",
      "Quentin Garrido",
      "Koustuv Sinha",
      "Nicolas Ballas",
      "Mahmoud Assran"
    ],
    "published": "2025-06-11T17:57:32+00:00",
    "summary": "Existing benchmarks for assessing the spatio-temporal understanding and reasoning abilities of video language models are susceptible to score inflation due to the presence of shortcut solutions based on superficial visual or textual cues. This paper mitigates the challenges in accurately assessing model performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple shortcut-aware video QA benchmark for assessing the physical understanding of video language models. The benchmark is comprised of 55K high-quality multiple-choice video QA examples focusing on physical world understanding. Examples are curated from nine video data sources, spanning first-person egocentric and exocentric videos, robotic interaction data, and cognitive science intuitive physics benchmarks. To mitigate shortcut solutions that rely on superficial visual or textual cues and biases, each sample in MVP has a minimal-change pair -- a visually similar video accompanied by an identical question but an opposing answer. To answer a question correctly, a model must provide correct answers for both examples in the minimal-change pair; as such, models that solely rely on visual or textual biases would achieve below random performance. Human performance on MVP is 92.9\\%, while the best open-source state-of-the-art video-language model achieves 40.2\\% compared to random performance at 25\\%."
  },
  {
    "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.09049v1",
    "arxiv_id": "2506.09049v1",
    "authors": [
      "Li Kang",
      "Xiufeng Song",
      "Heng Zhou",
      "Yiran Qin",
      "Jie Yang",
      "Xiaohong Liu",
      "Philip Torr",
      "Lei Bai",
      "Zhenfei Yin"
    ],
    "published": "2025-06-10T17:59:44+00:00",
    "summary": "Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems."
  },
  {
    "title": "Gradual Metaprogramming",
    "url": "http://arxiv.org/abs/2506.09043v1",
    "arxiv_id": "2506.09043v1",
    "authors": [
      "Tianyu Chen",
      "Darshal Shetty",
      "Jeremy G. Siek",
      "Chao-Hong Chen",
      "Weixi Ma",
      "Arnaud Venet",
      "Rocky Liu"
    ],
    "published": "2025-06-10T17:58:32+00:00",
    "summary": "Data engineers increasingly use domain-specific languages (DSLs) to generate the code for data pipelines. Such DSLs are often embedded in Python. Unfortunately, there are challenges in debugging the generation of data pipelines: an error in a Python DSL script is often detected too late, after the execution of the script, and the source code location that triggers the error is hard to pinpoint.   In this paper, we focus on the F3 DSL of Meta (Facebook), which is a DSL embedded in Python (so it is dynamically-typed) to generate data pipeline description code that is statically-typed. We propose gradual metaprogramming to (1) provide a migration path toward statically typed DSLs, (2) immediately provide earlier detection of code generation type errors, and (3) report the source code location responsible for the type error. Gradual metaprogramming accomplishes this by type checking code fragments and incrementally performing runtime checks as they are spliced together. We define MetaGTLC, a metaprogramming calculus in which a gradually-typed metalanguage manipulates a statically-typed object language, and give semantics to it by translation to the cast calculus MetaCC. We prove that successful metaevaluation always generates a well-typed object program and mechanize the proof in Agda."
  },
  {
    "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models",
    "url": "http://arxiv.org/abs/2506.09042v1",
    "arxiv_id": "2506.09042v1",
    "authors": [
      "Xuanchi Ren",
      "Yifan Lu",
      "Tianshi Cao",
      "Ruiyuan Gao",
      "Shengyu Huang",
      "Amirmojtaba Sabour",
      "Tianchang Shen",
      "Tobias Pfaff",
      "Jay Zhangjie Wu",
      "Runjian Chen",
      "Seung Wook Kim",
      "Jun Gao",
      "Laura Leal-Taixe",
      "Mike Chen",
      "Sanja Fidler",
      "Huan Ling"
    ],
    "published": "2025-06-10T17:58:17+00:00",
    "summary": "Collecting and annotating real-world data for safety-critical physical AI systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is especially challenging to capture rare edge cases, which play a critical role in training and testing of an AV system. To address this challenge, we introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline that aims to generate challenging scenarios to facilitate downstream tasks such as perception and driving policy training. Powering this pipeline is Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation model for the driving domain and are capable of controllable, high-fidelity, multi-view, and spatiotemporally consistent driving video generation. We showcase the utility of these models by applying Cosmos-Drive-Dreams to scale the quantity and diversity of driving datasets with high-fidelity and challenging scenarios. Experimentally, we demonstrate that our generated data helps in mitigating long-tail distribution problems and enhances generalization in downstream tasks such as 3D lane detection, 3D object detection and driving policy learning. We open source our pipeline toolkit, dataset and model weights through the NVIDIA's Cosmos platform.   Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams"
  },
  {
    "title": "Deep Reinforcement Learning-Based RAN Slicing with Efficient Inter-Slice Isolation in Tactical Wireless Networks",
    "url": "http://arxiv.org/abs/2506.09039v1",
    "arxiv_id": "2506.09039v1",
    "authors": [
      "Abderrahime Filali",
      "Diala Naboulsi",
      "Georges Kaddoum"
    ],
    "published": "2025-06-10T17:57:33+00:00",
    "summary": "The next generation of tactical networks (TNs) is poised to further leverage the key enablers of 5G and beyond 5G (B5G) technology, such as radio access network (RAN) slicing and the open RAN (O-RAN) paradigm, to unlock multiple architectural options and opportunities for a wide range of innovative applications. RAN slicing and the O-RAN paradigm are considered game changers in TNs, where the former makes it possible to tailor user services to users requirements, and the latter brings openness and intelligence to the management of the RAN. In TNs, bandwidth scarcity requires a dynamic bandwidth slicing strategy. Although this type of strategy ensures efficient bandwidth utilization, it compromises RAN slicing isolation in terms of quality of service (QoS) performance. To deal with this challenge, we propose a deep reinforcement learning (DRL)-based RAN slicing mechanism that achieves a trade-off between efficient RAN bandwidth sharing and appropriate inter- and intra-slice isolation. The proposed mechanism performs bandwidth allocation in two stages. In the first stage, the bandwidth is allocated to the RAN slices. In the second stage, each slice partitions its bandwidth among its associated users. In both stages, the slicing operation is constrained by several considerations related to improving the QoS of slices and users that in turn foster inter- and intra-slice isolation. The proposed RAN slicing mechanism is based on DRL algorithms to perform the bandwidth sharing operation in each stage. We propose to deploy the mechanism in an O-RAN architecture and describe the O-RAN functional blocks and the main DRL model lifecycle management phases involved. We also develop three different implementations of the proposed mechanism, each based on a different DRL algorithm, and evaluate their performance against multiple baselines across various parameters."
  },
  {
    "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.09033v1",
    "arxiv_id": "2506.09033v1",
    "authors": [
      "Haozhen Zhang",
      "Tao Feng",
      "Jiaxuan You"
    ],
    "published": "2025-06-10T17:56:45+00:00",
    "summary": "The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \\textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave \"think\" actions (internal deliberation) with \"route\" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1."
  },
  {
    "title": "Play to Generalize: Learning to Reason Through Game Play",
    "url": "http://arxiv.org/abs/2506.08011v1",
    "arxiv_id": "2506.08011v1",
    "authors": [
      "Yunfei Xie",
      "Yinsong Ma",
      "Shiyi Lan",
      "Alan Yuille",
      "Junfei Xiao",
      "Chen Wei"
    ],
    "published": "2025-06-09T17:59:57+00:00",
    "summary": "Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base model's performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs."
  },
  {
    "title": "Reinforcement Pre-Training",
    "url": "http://arxiv.org/abs/2506.08007v1",
    "arxiv_id": "2506.08007v1",
    "authors": [
      "Qingxiu Dong",
      "Li Dong",
      "Yao Tang",
      "Tianzhu Ye",
      "Yutao Sun",
      "Zhifang Sui",
      "Furu Wei"
    ],
    "published": "2025-06-09T17:59:53+00:00",
    "summary": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training."
  },
  {
    "title": "Dynamic View Synthesis as an Inverse Problem",
    "url": "http://arxiv.org/abs/2506.08004v1",
    "arxiv_id": "2506.08004v1",
    "authors": [
      "Hidir Yesiltepe",
      "Pinar Yanardag"
    ],
    "published": "2025-06-09T17:59:47+00:00",
    "summary": "In this work, we address dynamic view synthesis from monocular videos as an inverse problem in a training-free setting. By redesigning the noise initialization phase of a pre-trained video diffusion model, we enable high-fidelity dynamic view synthesis without any weight updates or auxiliary modules. We begin by identifying a fundamental obstacle to deterministic inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and resolve it by introducing a novel noise representation, termed K-order Recursive Noise Representation. We derive a closed form expression for this representation, enabling precise and efficient alignment between the VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions resulting from camera motion, we introduce Stochastic Latent Modulation, which performs visibility aware sampling over the latent space to complete occluded regions. Comprehensive experiments demonstrate that dynamic view synthesis can be effectively performed through structured latent manipulation in the noise initialization phase."
  },
  {
    "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
    "url": "http://arxiv.org/abs/2506.08002v1",
    "arxiv_id": "2506.08002v1",
    "authors": [
      "Aadarsh Sahoo",
      "Vansh Tibrewal",
      "Georgia Gkioxari"
    ],
    "published": "2025-06-09T17:59:37+00:00",
    "summary": "Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/"
  },
  {
    "title": "UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References",
    "url": "http://arxiv.org/abs/2506.07996v1",
    "arxiv_id": "2506.07996v1",
    "authors": [
      "Ming-Feng Li",
      "Xin Yang",
      "Fu-En Wang",
      "Hritam Basak",
      "Yuyin Sun",
      "Shreekant Gayaka",
      "Min Sun",
      "Cheng-Hao Kuo"
    ],
    "published": "2025-06-09T17:58:12+00:00",
    "summary": "6D object pose estimation has shown strong generalizability to novel objects. However, existing methods often require either a complete, well-reconstructed 3D model or numerous reference images that fully cover the object. Estimating 6D poses from partial references, which capture only fragments of an object's appearance and geometry, remains challenging. To address this, we propose UA-Pose, an uncertainty-aware approach for 6D object pose estimation and online object completion specifically designed for partial references. We assume access to either (1) a limited set of RGBD images with known poses or (2) a single 2D image. For the first case, we initialize a partial object 3D model based on the provided images and poses, while for the second, we use image-to-3D techniques to generate an initial object 3D model. Our method integrates uncertainty into the incomplete 3D model, distinguishing between seen and unseen regions. This uncertainty enables confidence assessment in pose estimation and guides an uncertainty-aware sampling strategy for online object completion, enhancing robustness in pose estimation accuracy and improving object completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and HO3D datasets, including RGBD sequences of YCB objects manipulated by robots and human hands. Experimental results demonstrate significant performance improvements over existing methods, particularly when object observations are incomplete or partially captured. Project page: https://minfenli.github.io/UA-Pose/"
  },
  {
    "title": "PyGemini: Unified Software Development towards Maritime Autonomy Systems",
    "url": "http://arxiv.org/abs/2506.06262v1",
    "arxiv_id": "2506.06262v1",
    "authors": [
      "Kjetil Vasstein",
      "Christian Le",
      "Simon Lerv\u00e5g Breivik",
      "Trygve Maukon Myhr",
      "Annette Stahl",
      "Edmund F\u00f8rland Brekke"
    ],
    "published": "2025-06-06T17:43:00+00:00",
    "summary": "Ensuring the safety and certifiability of autonomous surface vessels (ASVs) requires robust decision-making systems, supported by extensive simulation, testing, and validation across a broad range of scenarios. However, the current landscape of maritime autonomy development is fragmented -- relying on disparate tools for communication, simulation, monitoring, and system integration -- which hampers interdisciplinary collaboration and inhibits the creation of compelling assurance cases, demanded by insurers and regulatory bodies. Furthermore, these disjointed tools often suffer from performance bottlenecks, vendor lock-in, and limited support for continuous integration workflows. To address these challenges, we introduce PyGemini, a permissively licensed, Python-native framework that builds on the legacy of Autoferry Gemini to unify maritime autonomy development. PyGemini introduces a novel Configuration-Driven Development (CDD) process that fuses Behavior-Driven Development (BDD), data-oriented design, and containerization to support modular, maintainable, and scalable software architectures. The framework functions as a stand-alone application, cloud-based service, or embedded library -- ensuring flexibility across research and operational contexts. We demonstrate its versatility through a suite of maritime tools -- including 3D content generation for simulation and monitoring, scenario generation for autonomy validation and training, and generative artificial intelligence pipelines for augmenting imagery -- thereby offering a scalable, maintainable, and performance-oriented foundation for future maritime robotics and autonomy research."
  },
  {
    "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens",
    "url": "http://arxiv.org/abs/2506.06261v1",
    "arxiv_id": "2506.06261v1",
    "authors": [
      "Jihwan Jeong",
      "Xiaoyu Wang",
      "Jingmin Wang",
      "Scott Sanner",
      "Pascal Poupart"
    ],
    "published": "2025-06-06T17:40:12+00:00",
    "summary": "Offline reinforcement learning (RL) is crucial when online exploration is costly or unsafe but often struggles with high epistemic uncertainty due to limited data. Existing methods rely on fixed conservative policies, restricting adaptivity and generalization. To address this, we propose Reflect-then-Plan (RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach. RefPlan unifies uncertainty modeling and MB planning by recasting planning as Bayesian posterior estimation. At deployment, it updates a belief over environment dynamics using real-time observations, incorporating uncertainty into MB planning via marginalization. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies."
  },
  {
    "title": "From NLVO to NAO: Reactive Robot Navigation using Velocity and Acceleration Obstacles",
    "url": "http://arxiv.org/abs/2506.06255v1",
    "arxiv_id": "2506.06255v1",
    "authors": [
      "Asher Stern",
      "Zvi Shiller"
    ],
    "published": "2025-06-06T17:35:37+00:00",
    "summary": "This paper introduces a novel approach for robot navigation in challenging dynamic environments. The proposed method builds upon the concept of Velocity Obstacles (VO) that was later extended to Nonlinear Velocity Obstacles (NLVO) to account for obstacles moving along nonlinear trajectories. The NLVO is extended in this paper to Acceleration Obstacles (AO) and Nonlinear Acceleration Obstacles (NAO) that account for velocity and acceleration constraints. Multi-robot navigation is achieved by using the same avoidance algorithm by all robots. At each time step, the trajectories of all robots are predicted based on their current velocity and acceleration to allow the computation of their respective NLVO, AO and NAO.   The introduction of AO and NAO allows the generation of safe avoidance maneuvers that account for the robot dynamic constraints better than could be done with the NLVO alone. This paper demonstrates the use of AO and NAO for robot navigation in challenging environments. It is shown that using AO and NAO enables simultaneous real-time collision avoidance while accounting for robot kinematics and a direct consideration of its dynamic constraints. The presented approach enables reactive and efficient navigation, with potential application for autonomous vehicles operating in complex dynamic environments."
  },
  {
    "title": "Fermion parity switches imprinted in the photonic field of cavity embedded Kitaev chains",
    "url": "http://arxiv.org/abs/2506.06237v1",
    "arxiv_id": "2506.06237v1",
    "authors": [
      "Victor Fernandez Becerra",
      "Olesia Dmytruk"
    ],
    "published": "2025-06-06T16:55:33+00:00",
    "summary": "The entanglement of electronic states with quantum light in cavity embedded systems has opened new avenues to manipulate quantum materials. In this work we investigate the Kitaev chain coupled to a single mode photonic cavity. Using exact diagonalization we calculate the many-body energy spectrum of the electron-photon Hamiltonian in finite-length chains. We find two distinct types of ground states, one with a well defined parity and another with an alternating parity where a doubly degenerate ground state takes place at exceptional points, known as parity switching points. The double ground state hosts edge states weakly affected by the cavity coupling, even in the low frequency regime, in contrast with higher excited states showing strong dependence with the cavity coupling. Besides the electronic quantities, we also find that the photon number peaks at values of the chemical potential corresponding to parity switching points. Therefore, we suggest that quantum optics experiments could be employed to detect the double ground state hosting edge states weakly hybridized with light. Finally, calculations of photonic quadratures reveal squeezed states that are both captured by the exact diagonalization technique and mean field decoupling. However, within these two approaches differences in the photon probability in odd numbers of photons are reported."
  },
  {
    "title": "Statistical Guarantees in Data-Driven Nonlinear Control: Conformal Robustness for Stability and Safety",
    "url": "http://arxiv.org/abs/2506.06228v1",
    "arxiv_id": "2506.06228v1",
    "authors": [
      "Ting-Wei Hsu",
      "Hiroyasu Tsukamoto"
    ],
    "published": "2025-06-06T16:42:16+00:00",
    "summary": "We present a true-dynamics-agnostic, statistically rigorous framework for establishing exponential stability and safety guarantees of closed-loop, data-driven nonlinear control. Central to our approach is the novel concept of conformal robustness, which robustifies the Lyapunov and zeroing barrier certificates of data-driven dynamical systems against model prediction uncertainties using conformal prediction. It quantifies these uncertainties by leveraging rank statistics of prediction scores over system trajectories, without assuming any specific underlying structure of the prediction model or distribution of the uncertainties. With the quantified uncertainty information, we further construct the conformally robust control Lyapunov function (CR-CLF) and control barrier function (CR-CBF), data-driven counterparts of the CLF and CBF, for fully data-driven control with statistical guarantees of finite-horizon exponential stability and safety. The performance of the proposed concept is validated in numerical simulations with four benchmark nonlinear control problems."
  },
  {
    "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets",
    "url": "http://arxiv.org/abs/2506.05346v1",
    "arxiv_id": "2506.05346v1",
    "authors": [
      "Lei Hsiung",
      "Tianyu Pang",
      "Yung-Chen Tang",
      "Linyue Song",
      "Tsung-Yi Ho",
      "Pin-Yu Chen",
      "Yaoqing Yang"
    ],
    "published": "2025-06-05T17:59:55+00:00",
    "summary": "Recent advancements in large language models (LLMs) have underscored their vulnerability to safety alignment jailbreaks, particularly when subjected to downstream fine-tuning. However, existing mitigation strategies primarily focus on reactively addressing jailbreak incidents after safety guardrails have been compromised, removing harmful gradients during fine-tuning, or continuously reinforcing safety alignment throughout fine-tuning. As such, they tend to overlook a critical upstream factor: the role of the original safety-alignment data. This paper therefore investigates the degradation of safety guardrails through the lens of representation similarity between upstream alignment datasets and downstream fine-tuning tasks. Our experiments demonstrate that high similarity between these datasets significantly weakens safety guardrails, making models more susceptible to jailbreaks. Conversely, low similarity between these two types of datasets yields substantially more robust models and thus reduces harmfulness score by up to 10.33%. By highlighting the importance of upstream dataset design in the building of durable safety guardrails and reducing real-world vulnerability to jailbreak attacks, these findings offer actionable insights for fine-tuning service providers."
  },
  {
    "title": "ContentV: Efficient Training of Video Generation Models with Limited Compute",
    "url": "http://arxiv.org/abs/2506.05343v1",
    "arxiv_id": "2506.05343v1",
    "authors": [
      "Wenfeng Lin",
      "Renjie Chen",
      "Boyuan Liu",
      "Shiyue Yan",
      "Ruoyu Feng",
      "Jiangchuan Wei",
      "Yichen Zhang",
      "Yimeng Zhou",
      "Chao Feng",
      "Jiao Ran",
      "Qi Wu",
      "Zuotao Liu",
      "Mingyu Guo"
    ],
    "published": "2025-06-05T17:59:54+00:00",
    "summary": "Recent advances in video generation demand increasingly efficient training recipes to mitigate escalating computational costs. In this report, we present ContentV, an 8B-parameter text-to-video model that achieves state-of-the-art performance (85.14 on VBench) after training on 256 x 64GB Neural Processing Units (NPUs) for merely four weeks. ContentV generates diverse, high-quality videos across multiple resolutions and durations from text prompts, enabled by three key innovations: (1) A minimalist architecture that maximizes reuse of pre-trained image generation models for video generation; (2) A systematic multi-stage training strategy leveraging flow matching for enhanced efficiency; and (3) A cost-effective reinforcement learning with human feedback framework that improves generation quality without requiring additional human annotations. All the code and models are available at: https://contentv.github.io."
  },
  {
    "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
    "url": "http://arxiv.org/abs/2506.05336v1",
    "arxiv_id": "2506.05336v1",
    "authors": [
      "Ghazi Shazan Ahmad",
      "Ahmed Heakl",
      "Hanan Gani",
      "Abdelrahman Shaker",
      "Zhiqiang Shen",
      "Ranjay Krishna",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ],
    "published": "2025-06-05T17:59:29+00:00",
    "summary": "Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at https://github.com/mbzuai-oryx/VideoMolmo."
  },
  {
    "title": "AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual Counting for MLLMs",
    "url": "http://arxiv.org/abs/2506.05328v1",
    "arxiv_id": "2506.05328v1",
    "authors": [
      "Lidong Lu",
      "Guo Chen",
      "Zhiqi Li",
      "Yicheng Liu",
      "Tong Lu"
    ],
    "published": "2025-06-05T17:58:33+00:00",
    "summary": "Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io."
  },
  {
    "title": "ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics Estimation",
    "url": "http://arxiv.org/abs/2506.05317v1",
    "arxiv_id": "2506.05317v1",
    "authors": [
      "Daniel Rho",
      "Jun Myeong Choi",
      "Biswadip Dey",
      "Roni Sengupta"
    ],
    "published": "2025-06-05T17:55:56+00:00",
    "summary": "Neural rendering has made significant strides in 3D reconstruction and novel view synthesis. With the integration with physics, it opens up new applications. The inverse problem of estimating physics from visual data, however, still remains challenging, limiting its effectiveness for applications like physically accurate digital twin creation in robotics and XR. Existing methods that incorporate physics into neural rendering frameworks typically require dense multi-view videos as input, making them impractical for scalable, real-world use. When presented with sparse multi-view videos, the sequential optimization strategy used by existing approaches introduces significant error accumulation, e.g., poor initial 3D reconstruction leads to bad material parameter estimation in subsequent stages. Instead of sequential optimization, directly optimizing all parameters at the same time also fails due to the highly non-convex and often non-differentiable nature of the problem. We propose ProJo4D, a progressive joint optimization framework that gradually increases the set of jointly optimized parameters guided by their sensitivity, leading to fully joint optimization over geometry, appearance, physical state, and material property. Evaluations on PAC-NeRF and Spring-Gaus datasets show that ProJo4D outperforms prior work in 4D future state prediction, novel view rendering of future state, and material parameter estimation, demonstrating its effectiveness in physically grounded 4D scene understanding. For demos, please visit the project webpage: https://daniel03c1.github.io/ProJo4D/"
  },
  {
    "title": "Object-centric 3D Motion Field for Robot Learning from Human Videos",
    "url": "http://arxiv.org/abs/2506.04227v1",
    "arxiv_id": "2506.04227v1",
    "authors": [
      "Zhao-Heng Yin",
      "Sherry Yang",
      "Pieter Abbeel"
    ],
    "published": "2025-06-04T17:59:06+00:00",
    "summary": "Learning robot control policies from human videos is a promising direction for scaling up robot learning. However, how to extract action knowledge (or action representations) from videos for policy learning remains a key challenge. Existing action representations such as video frames, pixelflow, and pointcloud flow have inherent limitations such as modeling complexity or loss of information. In this paper, we propose to use object-centric 3D motion field to represent actions for robot learning from human videos, and present a novel framework for extracting this representation from videos for zero-shot control. We introduce two novel components in its implementation. First, a novel training pipeline for training a ''denoising'' 3D motion field estimator to extract fine object 3D motions from human videos with noisy depth robustly. Second, a dense object-centric 3D motion field prediction architecture that favors both cross-embodiment transfer and policy generalization to background. We evaluate the system in real world setups. Experiments show that our method reduces 3D motion estimation error by over 50% compared to the latest method, achieve 55% average success rate in diverse tasks where prior approaches fail~($\\lesssim 10$\\%), and can even acquire fine-grained manipulation skills like insertion."
  },
  {
    "title": "Pseudo-Simulation for Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.04218v1",
    "arxiv_id": "2506.04218v1",
    "authors": [
      "Wei Cao",
      "Marcel Hallgarten",
      "Tianyu Li",
      "Daniel Dauner",
      "Xunjiang Gu",
      "Caojun Wang",
      "Yakov Miron",
      "Marco Aiello",
      "Hongyang Li",
      "Igor Gilitschenski",
      "Boris Ivanovic",
      "Marco Pavone",
      "Andreas Geiger",
      "Kashyap Chitta"
    ],
    "published": "2025-06-04T17:57:53+00:00",
    "summary": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical limitations. Real-world evaluation is often challenging due to safety concerns and a lack of reproducibility, whereas closed-loop simulation can face insufficient realism or high computational costs. Open-loop evaluation, while being efficient and data-driven, relies on metrics that generally overlook compounding errors. In this paper, we propose pseudo-simulation, a novel paradigm that addresses these limitations. Pseudo-simulation operates on real datasets, similar to open-loop evaluation, but augments them with synthetic observations generated prior to evaluation using 3D Gaussian Splatting. Our key idea is to approximate potential future states the AV might encounter by generating a diverse set of observations that vary in position, heading, and speed. Our method then assigns a higher importance to synthetic observations that best match the AV's likely behavior using a novel proximity-based weighting scheme. This enables evaluating error recovery and the mitigation of causal confusion, as in closed-loop benchmarks, without requiring sequential interactive simulation. We show that pseudo-simulation is better correlated with closed-loop simulations (R^2=0.8) than the best existing open-loop approach (R^2=0.7). We also establish a public leaderboard for the community to benchmark new methodologies with pseudo-simulation. Our code is available at https://github.com/autonomousvision/navsim."
  },
  {
    "title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis",
    "url": "http://arxiv.org/abs/2506.04217v1",
    "arxiv_id": "2506.04217v1",
    "authors": [
      "Junting Chen",
      "Haotian Liang",
      "Lingxiao Du",
      "Weiyun Wang",
      "Mengkang Hu",
      "Yao Mu",
      "Wenhai Wang",
      "Jifeng Dai",
      "Ping Luo",
      "Wenqi Shao",
      "Lin Shao"
    ],
    "published": "2025-06-04T17:57:44+00:00",
    "summary": "The rapid progress of navigation, manipulation, and vision models has made mobile manipulators capable in many specialized tasks. However, the open-world mobile manipulation (OWMM) task remains a challenge due to the need for generalization to open-ended instructions and environments, as well as the systematic complexity to integrate high-level decision making with low-level robot control based on both global scene understanding and current agent state. To address this complexity, we propose a novel multi-modal agent architecture that maintains multi-view scene frames and agent states for decision-making and controls the robot by function calling. A second challenge is the hallucination from domain shift. To enhance the agent performance, we further introduce an agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our task domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM as the first dedicated foundation model for mobile manipulators with global scene understanding, robot state tracking, and multi-modal action generation in a unified model. Through experiments, we demonstrate that our model achieves SOTA performance compared to other foundation models including GPT-4o and strong zero-shot generalization in real world. The project page is at https://github.com/HHYHRHY/OWMM-Agent"
  },
  {
    "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04207v1",
    "arxiv_id": "2506.04207v1",
    "authors": [
      "Shuang Chen",
      "Yue Guo",
      "Zhaochen Su",
      "Yafu Li",
      "Yulun Wu",
      "Jiacheng Chen",
      "Jiayu Chen",
      "Weijie Wang",
      "Xiaoye Qu",
      "Yu Cheng"
    ],
    "published": "2025-06-04T17:51:08+00:00",
    "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025."
  },
  {
    "title": "MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures",
    "url": "http://arxiv.org/abs/2506.04195v1",
    "arxiv_id": "2506.04195v1",
    "authors": [
      "Elena Zamaraeva",
      "Christopher M. Collins",
      "George R. Darling",
      "Matthew S. Dyer",
      "Bei Peng",
      "Rahul Savani",
      "Dmytro Antypov",
      "Vladimir V. Gusev",
      "Judith Clymo",
      "Paul G. Spirakis",
      "Matthew J. Rosseinsky"
    ],
    "published": "2025-06-04T17:40:57+00:00",
    "summary": "Geometry optimization of atomic structures is a common and crucial task in computational chemistry and materials design. Following the learning to optimize paradigm, we propose a new multi-agent reinforcement learning method called Multi-Agent Crystal Structure optimization (MACS) to address periodic crystal structure optimization. MACS treats geometry optimization as a partially observable Markov game in which atoms are agents that adjust their positions to collectively discover a stable configuration. We train MACS across various compositions of reported crystalline materials to obtain a policy that successfully optimizes structures from the training compositions as well as structures of larger sizes and unseen compositions, confirming its excellent scalability and zero-shot transferability. We benchmark our approach against a broad range of state-of-the-art optimization methods and demonstrate that MACS optimizes periodic crystal structures significantly faster, with fewer energy calculations, and the lowest failure rate."
  },
  {
    "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation",
    "url": "http://arxiv.org/abs/2506.03147v1",
    "arxiv_id": "2506.03147v1",
    "authors": [
      "Bin Lin",
      "Zongjian Li",
      "Xinhua Cheng",
      "Yuwei Niu",
      "Yang Ye",
      "Xianyi He",
      "Shenghai Yuan",
      "Wangbo Yu",
      "Shaodong Wang",
      "Yunyang Ge",
      "Yatian Pang",
      "Li Yuan"
    ],
    "published": "2025-06-03T17:59:33+00:00",
    "summary": "Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4o-Image leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As a result, we build a strong unified model using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training and evaluation scripts, and datasets."
  },
  {
    "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.03136v1",
    "arxiv_id": "2506.03136v1",
    "authors": [
      "Yinjie Wang",
      "Ling Yang",
      "Ye Tian",
      "Ke Shen",
      "Mengdi Wang"
    ],
    "published": "2025-06-03T17:58:42+00:00",
    "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE"
  },
  {
    "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation",
    "url": "http://arxiv.org/abs/2506.03122v1",
    "arxiv_id": "2506.03122v1",
    "authors": [
      "Prashanth Vijayaraghavan",
      "Luyao Shi",
      "Ehsan Degan",
      "Vandana Mukherjee",
      "Xin Zhang"
    ],
    "published": "2025-06-03T17:54:30+00:00",
    "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements. However, the vast design search space and strict constraint adherence make efficient synthesis challenging. Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis. The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage. The refined model is then used directly to generate topologies that satisfy the design constraints. Empirical results show that AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by ~14% compared to the best baselines, while reducing duplicate generation rates by ~38%. It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization. These findings highlight the framework's effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design."
  },
  {
    "title": "Assessing Workers Neuro-physiological Stress Responses to Augmented Reality Safety Warnings in Immersive Virtual Roadway Work Zones",
    "url": "http://arxiv.org/abs/2506.03113v1",
    "arxiv_id": "2506.03113v1",
    "authors": [
      "Fatemeh Banani Ardecani",
      "Omidreza Shoghli"
    ],
    "published": "2025-06-03T17:40:49+00:00",
    "summary": "This paper presents a multi-stage experimental framework that integrates immersive Virtual Reality (VR) simulations, wearable sensors, and advanced signal processing to investigate construction workers neuro-physiological stress responses to multi-sensory AR-enabled warnings. Participants performed light- and moderate-intensity roadway maintenance tasks within a high-fidelity VR roadway work zone, while key stress markers of electrodermal activity (EDA), heart rate variability (HRV), and electroencephalography (EEG) were continuously measured. Statistical analyses revealed that task intensity significantly influenced physiological and neurological stress indicators. Moderate-intensity tasks elicited greater autonomic arousal, evidenced by elevated heart rate measures (mean-HR, std-HR, max-HR) and stronger electrodermal responses, while EEG data indicated distinct stress-related alpha suppression and beta enhancement. Feature-importance analysis further identified mean EDR and short-term HR metrics as discriminative for classifying task intensity. Correlation results highlighted a temporal lag between immediate neural changes and subsequent physiological stress reactions, emphasizing the interplay between cognition and autonomic regulation during hazardous tasks."
  },
  {
    "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback",
    "url": "http://arxiv.org/abs/2506.03106v1",
    "arxiv_id": "2506.03106v1",
    "authors": [
      "Xiaoying Zhang",
      "Hao Sun",
      "Yipeng Zhang",
      "Kaituo Feng",
      "Chao Yang",
      "Helen Meng"
    ],
    "published": "2025-06-03T17:39:02+00:00",
    "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration."
  },
  {
    "title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL",
    "url": "http://arxiv.org/abs/2505.24875v1",
    "arxiv_id": "2505.24875v1",
    "authors": [
      "Yu Zhang",
      "Yunqi Li",
      "Yifan Yang",
      "Rui Wang",
      "Yuqing Yang",
      "Dai Qi",
      "Jianmin Bao",
      "Dongdong Chen",
      "Chong Luo",
      "Lili Qiu"
    ],
    "published": "2025-05-30T17:59:48+00:00",
    "summary": "Although chain-of-thought reasoning and reinforcement learning (RL) have driven breakthroughs in NLP, their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based \"thinking\" skills via supervised fine-tuning on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization. To enable the model to reason through text before generating images, We automatically generate and release a corpus of model crafted rationales paired with visual prompts, enabling controlled planning of object layouts, styles, and scene compositions. Our GRPO algorithm uses reward signals from a pretrained vision language model to assess overall visual quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models. More: aka.ms/reasongen."
  },
  {
    "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners",
    "url": "http://arxiv.org/abs/2505.24872v1",
    "arxiv_id": "2505.24872v1",
    "authors": [
      "Zilin Xiao",
      "Jaywon Koo",
      "Siru Ouyang",
      "Jefferson Hernandez",
      "Yu Meng",
      "Vicente Ordonez"
    ],
    "published": "2025-05-30T17:59:43+00:00",
    "summary": "Recent advancements in reinforcement learning with verifiable rewards have pushed the boundaries of the visual reasoning capabilities in large vision-language models (LVLMs). However, training LVLMs with reinforcement fine-tuning (RFT) is computationally expensive, posing a significant challenge to scaling model size. In this work, we propose ProxyThinker, an inference-time technique that enables large models to inherit the visual reasoning capabilities from small, slow-thinking visual reasoners without any training. By subtracting the output distributions of base models from those of RFT reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors such as self-verification and self-correction. ProxyThinker consistently boosts performance on challenging visual benchmarks on spatial, mathematical, and multi-disciplinary reasoning, enabling untuned base models to compete with the performance of their full-scale RFT counterparts. Furthermore, our implementation efficiently coordinates multiple language models with parallelism techniques and achieves up to 38 $\\times$ faster inference compared to previous decoding-time methods, paving the way for the practical deployment of ProxyThinker. Code is available at https://github.com/MrZilinXiao/ProxyThinker."
  },
  {
    "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24871v1",
    "arxiv_id": "2505.24871v1",
    "authors": [
      "Yiqing Liang",
      "Jielin Qiu",
      "Wenhao Ding",
      "Zuxin Liu",
      "James Tompkin",
      "Mengdi Xu",
      "Mengzhou Xia",
      "Zhengzhong Tu",
      "Laixi Shi",
      "Jiacheng Zhu"
    ],
    "published": "2025-05-30T17:59:38+00:00",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model's accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline."
  },
  {
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models",
    "url": "http://arxiv.org/abs/2505.24864v1",
    "arxiv_id": "2505.24864v1",
    "authors": [
      "Mingjie Liu",
      "Shizhe Diao",
      "Ximing Lu",
      "Jian Hu",
      "Xin Dong",
      "Yejin Choi",
      "Jan Kautz",
      "Yi Dong"
    ],
    "published": "2025-05-30T17:59:01+00:00",
    "summary": "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B"
  },
  {
    "title": "PB&J: Peanut Butter and Joints for Damped Articulation",
    "url": "http://arxiv.org/abs/2505.24860v1",
    "arxiv_id": "2505.24860v1",
    "authors": [
      "Avery S. Williamson",
      "Michael J. Bennington",
      "Ravesh Sukhnandan",
      "Mrinali Nakhre",
      "Yuemin Mao",
      "Victoria A. Webster-Wood"
    ],
    "published": "2025-05-30T17:57:21+00:00",
    "summary": "Many bioinspired robots mimic the rigid articulated joint structure of the human hand for grasping tasks, but experience high-frequency mechanical perturbations that can destabilize the system and negatively affect precision without a high-frequency controller. Despite having bandwidth-limited controllers that experience time delays between sensing and actuation, biological systems can respond successfully to and mitigate these high-frequency perturbations. Human joints include damping and stiffness that many rigid articulated bioinspired hand robots lack. To enable researchers to explore the effects of joint viscoelasticity in joint control, we developed a human-hand-inspired grasping robot with viscoelastic structures that utilizes accessible and bioderived materials to reduce the economic and environmental impact of prototyping novel robotic systems. We demonstrate that an elastic element at the finger joints is necessary to achieve concurrent flexion, which enables secure grasping of spherical objects. To significantly damp the manufactured finger joints, we modeled, manufactured, and characterized rotary dampers using peanut butter as an organic analog joint working fluid. Finally, we demonstrated that a real-time position-based controller could be used to successfully catch a lightweight falling ball. We developed this open-source, low-cost grasping platform that abstracts the morphological and mechanical properties of the human hand to enable researchers to explore questions about biomechanics in roboto that would otherwise be difficult to test in simulation or modeling."
  },
  {
    "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
    "url": "http://arxiv.org/abs/2505.23762v1",
    "arxiv_id": "2505.23762v1",
    "authors": [
      "Chenyu Yang",
      "Shiqian Su",
      "Shi Liu",
      "Xuan Dong",
      "Yue Yu",
      "Weijie Su",
      "Xuehui Wang",
      "Zhaoyang Liu",
      "Jinguo Zhu",
      "Hao Li",
      "Wenhai Wang",
      "Yu Qiao",
      "Xizhou Zhu",
      "Jifeng Dai"
    ],
    "published": "2025-05-29T17:59:51+00:00",
    "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI."
  },
  {
    "title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models",
    "url": "http://arxiv.org/abs/2505.23757v1",
    "arxiv_id": "2505.23757v1",
    "authors": [
      "Haohan Chi",
      "Huan-ang Gao",
      "Ziming Liu",
      "Jianing Liu",
      "Chenyu Liu",
      "Jinwei Li",
      "Kaisen Yang",
      "Yangcheng Yu",
      "Zeda Wang",
      "Wenyi Li",
      "Leichen Wang",
      "Xingtao Hu",
      "Hao Sun",
      "Hang Zhao",
      "Hao Zhao"
    ],
    "published": "2025-05-29T17:59:46+00:00",
    "summary": "Vision-Language-Action (VLA) models for autonomous driving show promise but falter in unstructured corner case scenarios, largely due to a scarcity of targeted benchmarks. To address this, we introduce Impromptu VLA. Our core contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated video clips, distilled from over 2M source clips sourced from 8 open-source large-scale datasets. This dataset is built upon our novel taxonomy of four challenging unstructured categories and features rich, planning-oriented question-answering annotations and action trajectories. Crucially, experiments demonstrate that VLAs trained with our dataset achieve substantial performance gains on established benchmarks--improving closed-loop NeuroNCAP scores and collision rates, and reaching near state-of-the-art L2 accuracy in open-loop nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an effective diagnostic, revealing clear VLM improvements in perception, prediction, and planning. Our code, data and models are available at https://github.com/ahydchh/Impromptu-VLA."
  },
  {
    "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.23754v1",
    "arxiv_id": "2505.23754v1",
    "authors": [
      "Ziyin Zhang",
      "Jiahao Xu",
      "Zhiwei He",
      "Tian Liang",
      "Qiuzhi Liu",
      "Yansi Li",
      "Linfeng Song",
      "Zhengwen Liang",
      "Zhuosheng Zhang",
      "Rui Wang",
      "Zhaopeng Tu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "published": "2025-05-29T17:59:39+00:00",
    "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration."
  },
  {
    "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
    "url": "http://arxiv.org/abs/2505.23745v1",
    "arxiv_id": "2505.23745v1",
    "authors": [
      "Hao Dong",
      "Moru Liu",
      "Jian Liang",
      "Eleni Chatzi",
      "Olga Fink"
    ],
    "published": "2025-05-29T17:59:01+00:00",
    "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM."
  },
  {
    "title": "Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side",
    "url": "http://arxiv.org/abs/2505.23733v1",
    "arxiv_id": "2505.23733v1",
    "authors": [
      "Truong",
      "Luu",
      "Binny M. Samuel"
    ],
    "published": "2025-05-29T17:57:01+00:00",
    "summary": "In recent years, the rapid advancement and democratization of generative AI models have sparked significant debate over safety, ethical risks, and dual-use concerns, particularly in the context of cybersecurity. While anecdotally known, this paper provides empirical evidence regarding generative AI's association with malicious internet-related activities and cybercrime by examining the phenomenon through psychological frameworks of technological amplification and affordance theory. Using a quasi-experimental design with interrupted time series analysis, we analyze two datasets, one general and one cryptocurrency-focused, to empirically assess generative AI's role in cybercrime. The findings contribute to ongoing discussions about AI governance by balancing control and fostering innovation, underscoring the need for strategies to guide policymakers, inform AI developers and cybersecurity professionals, and educate the public to maximize AI's benefits while mitigating its risks."
  },
  {
    "title": "Maximizing Confidence Alone Improves Reasoning",
    "url": "http://arxiv.org/abs/2505.22660v1",
    "arxiv_id": "2505.22660v1",
    "authors": [
      "Mihir Prabhudesai",
      "Lili Chen",
      "Alex Ippoliti",
      "Katerina Fragkiadaki",
      "Hao Liu",
      "Deepak Pathak"
    ],
    "published": "2025-05-28T17:59:37+00:00",
    "summary": "Reinforcement learning (RL) has enabled machine learning models to achieve significant advances in many fields. Most recently, RL has empowered frontier language models to solve challenging math, science, and coding problems. However, central to any RL algorithm is the reward function, and reward engineering is a notoriously difficult problem in any domain. In this paper, we propose RENT: Reinforcement Learning via Entropy Minimization -- a fully unsupervised RL method that requires no external reward or ground-truth answers, and instead uses the model's entropy of its underlying distribution as an intrinsic reward. We find that by reinforcing the chains of thought that yield high model confidence on its generated answers, the model improves its reasoning ability. In our experiments, we showcase these improvements on an extensive suite of commonly-used reasoning benchmarks, including GSM8K, MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and Mistral families. The generality of our unsupervised learning method lends itself to applicability in a wide range of domains where external supervision is limited or unavailable."
  },
  {
    "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason",
    "url": "http://arxiv.org/abs/2505.22653v1",
    "arxiv_id": "2505.22653v1",
    "authors": [
      "Ang Lv",
      "Ruobing Xie",
      "Xingwu Sun",
      "Zhanhui Kang",
      "Rui Yan"
    ],
    "published": "2025-05-28T17:59:03+00:00",
    "summary": "Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as ``first, I need to''-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLM's performance on open-ended tasks. These findings suggest the importance of improving models' foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason."
  },
  {
    "title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "url": "http://arxiv.org/abs/2505.22648v1",
    "arxiv_id": "2505.22648v1",
    "authors": [
      "Jialong Wu",
      "Baixuan Li",
      "Runnan Fang",
      "Wenbiao Yin",
      "Liwen Zhang",
      "Zhengwei Tao",
      "Dingchu Zhang",
      "Zekun Xi",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Jingren Zhou"
    ],
    "published": "2025-05-28T17:57:07+00:00",
    "summary": "Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent."
  },
  {
    "title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control",
    "url": "http://arxiv.org/abs/2505.22642v1",
    "arxiv_id": "2505.22642v1",
    "authors": [
      "Younggyo Seo",
      "Carmelo Sferrazza",
      "Haoran Geng",
      "Michal Nauman",
      "Zhao-Heng Yin",
      "Pieter Abbeel"
    ],
    "published": "2025-05-28T17:55:26+00:00",
    "summary": "Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics."
  },
  {
    "title": "SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes",
    "url": "http://arxiv.org/abs/2505.22638v1",
    "arxiv_id": "2505.22638v1",
    "authors": [
      "Denis Donadel",
      "Gabriele Crestanello",
      "Giulio Morandini",
      "Daniele Antonioli",
      "Mauro Conti",
      "Massimo Merro"
    ],
    "published": "2025-05-28T17:54:23+00:00",
    "summary": "Industrial Control Systems (ICS) manage critical infrastructures like power grids and water treatment plants. Cyberattacks on ICSs can disrupt operations, causing severe economic, environmental, and safety issues. For example, undetected pollution in a water plant can put the lives of thousands at stake. ICS researchers have increasingly turned to honeypots -- decoy systems designed to attract attackers, study their behaviors, and eventually improve defensive mechanisms. However, existing ICS honeypots struggle to replicate the ICS physical process, making them susceptible to detection. Accurately simulating the noise in ICS physical processes is challenging because different factors produce it, including sensor imperfections and external interferences.   In this paper, we propose SimProcess, a novel framework to rank the fidelity of ICS simulations by evaluating how closely they resemble real-world and noisy physical processes. It measures the simulation distance from a target system by estimating the noise distribution with machine learning models like Random Forest. Unlike existing solutions that require detailed mathematical models or are limited to simple systems, SimProcess operates with only a timeseries of measurements from the real system, making it applicable to a broader range of complex dynamic systems. We demonstrate the framework's effectiveness through a case study using real-world power grid data from the EPIC testbed. We compare the performance of various simulation methods, including static and generative noise techniques. Our model correctly classifies real samples with a recall of up to 1.0. It also identifies Gaussian and Gaussian Mixture as the best distribution to simulate our power systems, together with a generative solution provided by an autoencoder, thereby helping developers to improve honeypot fidelity. Additionally, we make our code publicly available."
  },
  {
    "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery",
    "url": "http://arxiv.org/abs/2505.21499v1",
    "arxiv_id": "2505.21499v1",
    "authors": [
      "Haowei Wang",
      "Junjie Wang",
      "Xiaojun Jia",
      "Rupeng Zhang",
      "Mingyang Li",
      "Zhe Liu",
      "Yang Liu",
      "Qing Wang"
    ],
    "published": "2025-05-27T17:59:05+00:00",
    "summary": "Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject."
  },
  {
    "title": "CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception",
    "url": "http://arxiv.org/abs/2505.21495v1",
    "arxiv_id": "2505.21495v1",
    "authors": [
      "Pranav N. Thakkar",
      "Shubhangi Sinha",
      "Karan Baijal",
      "Yuhan",
      "Bian",
      "Leah Lackey",
      "Ben Dodson",
      "Heisen Kong",
      "Jueun Kwon",
      "Amber Li",
      "Yifei Hu",
      "Alexios Rekoutis",
      "Tom Silver",
      "Tapomayukh Bhattacharjee"
    ],
    "published": "2025-05-27T17:58:00+00:00",
    "summary": "Robust robot manipulation in unstructured environments often requires understanding object properties that extend beyond geometry, such as material or compliance-properties that can be challenging to infer using vision alone. Multimodal haptic sensing provides a promising avenue for inferring such properties, yet progress has been constrained by the lack of large, diverse, and realistic haptic datasets. In this work, we introduce the CLAMP device, a low-cost (<\\$200) sensorized reacher-grabber designed to collect large-scale, in-the-wild multimodal haptic data from non-expert users in everyday settings. We deployed 16 CLAMP devices to 41 participants, resulting in the CLAMP dataset, the largest open-source multimodal haptic dataset to date, comprising 12.3 million datapoints across 5357 household objects. Using this dataset, we train a haptic encoder that can infer material and compliance object properties from multimodal haptic data. We leverage this encoder to create the CLAMP model, a visuo-haptic perception model for material recognition that generalizes to novel objects and three robot embodiments with minimal finetuning. We also demonstrate the effectiveness of our model in three real-world robot manipulation tasks: sorting recyclable and non-recyclable waste, retrieving objects from a cluttered bag, and distinguishing overripe from ripe bananas. Our results show that large-scale, in-the-wild haptic data collection can unlock new capabilities for generalizable robot manipulation. Website: https://emprise.cs.cornell.edu/clamp/"
  },
  {
    "title": "Reinforcing General Reasoning without Verifiers",
    "url": "http://arxiv.org/abs/2505.21493v1",
    "arxiv_id": "2505.21493v1",
    "authors": [
      "Xiangxin Zhou",
      "Zichen Liu",
      "Anya Sims",
      "Haonan Wang",
      "Tianyu Pang",
      "Chongxuan Li",
      "Liang Wang",
      "Min Lin",
      "Chao Du"
    ],
    "published": "2025-05-27T17:56:27+00:00",
    "summary": "The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree."
  },
  {
    "title": "Policy Optimized Text-to-Image Pipeline Design",
    "url": "http://arxiv.org/abs/2505.21478v1",
    "arxiv_id": "2505.21478v1",
    "authors": [
      "Uri Gadot",
      "Rinon Gal",
      "Yftah Ziser",
      "Gal Chechik",
      "Shie Mannor"
    ],
    "published": "2025-05-27T17:50:47+00:00",
    "summary": "Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines."
  },
  {
    "title": "Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO",
    "url": "http://arxiv.org/abs/2505.21457v1",
    "arxiv_id": "2505.21457v1",
    "authors": [
      "Muzhi Zhu",
      "Hao Zhong",
      "Canyu Zhao",
      "Zongze Du",
      "Zheng Huang",
      "Mingyu Liu",
      "Hao Chen",
      "Cheng Zou",
      "Jingdong Chen",
      "Ming Yang",
      "Chunhua Shen"
    ],
    "published": "2025-05-27T17:29:31+00:00",
    "summary": "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs."
  },
  {
    "title": "Variational Deep Learning via Implicit Regularization",
    "url": "http://arxiv.org/abs/2505.20235v1",
    "arxiv_id": "2505.20235v1",
    "authors": [
      "Jonathan Wenger",
      "Beau Coker",
      "Juraj Marusic",
      "John P. Cunningham"
    ],
    "published": "2025-05-26T17:15:57+00:00",
    "summary": "Modern deep learning models generalize remarkably well in-distribution, despite being overparametrized and trained with little to no explicit regularization. Instead, current theory credits implicit regularization imposed by the choice of architecture, hyperparameters and optimization procedure. However, deploying deep learning models out-of-distribution, in sequential decision-making tasks, or in safety-critical domains, necessitates reliable uncertainty quantification, not just a point estimate. The machinery of modern approximate inference -- Bayesian deep learning -- should answer the need for uncertainty quantification, but its effectiveness has been challenged by our inability to define useful explicit inductive biases through priors, as well as the associated computational burden. Instead, in this work we demonstrate, both theoretically and empirically, how to regularize a variational deep network implicitly via the optimization procedure, just as for standard deep learning. We fully characterize the inductive bias of (stochastic) gradient descent in the case of an overparametrized linear model as generalized variational inference and demonstrate the importance of the choice of parametrization. Finally, we show empirically that our approach achieves strong in- and out-of-distribution performance without tuning of additional hyperparameters and with minimal time and memory overhead over standard deep learning."
  },
  {
    "title": "Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects",
    "url": "http://arxiv.org/abs/2505.20223v1",
    "arxiv_id": "2505.20223v1",
    "authors": [
      "Yixin Cui",
      "Haotian Lin",
      "Shuo Yang",
      "Yixiao Wang",
      "Yanjun Huang",
      "Hong Chen"
    ],
    "published": "2025-05-26T17:06:00+00:00",
    "summary": "The rapid evolution of large language models in natural language processing has substantially elevated their semantic understanding and logical reasoning capabilities. Such proficiencies have been leveraged in autonomous driving systems, contributing to significant improvements in system performance. Models such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning, an advanced cognitive method that simulates human thinking processes, demonstrating remarkable reasoning capabilities in complex tasks. By structuring complex driving scenarios within a systematic reasoning framework, this approach has emerged as a prominent research focus in autonomous driving, substantially improving the system's ability to handle challenging cases. This paper investigates how CoT methods improve the reasoning abilities of autonomous driving models. Based on a comprehensive literature review, we present a systematic analysis of the motivations, methodologies, challenges, and future research directions of CoT in autonomous driving. Furthermore, we propose the insight of combining CoT with self-learning to facilitate self-evolution in driving systems. To ensure the relevance and timeliness of this study, we have compiled a dynamic repository of literature and open-source projects, diligently updated to incorporate forefront developments. The repository is publicly available at https://github.com/cuiyx1720/Awesome-CoT4AD."
  },
  {
    "title": "Fine-grained List-wise Alignment for Generative Medication Recommendation",
    "url": "http://arxiv.org/abs/2505.20218v1",
    "arxiv_id": "2505.20218v1",
    "authors": [
      "Chenxiao Fan",
      "Chongming Gao",
      "Wentao Shi",
      "Yaxin Gong",
      "Zihao Zhao",
      "Fuli Feng"
    ],
    "published": "2025-05-26T16:59:23+00:00",
    "summary": "Accurate and safe medication recommendations are critical for effective clinical decision-making, especially in multimorbidity cases. However, existing systems rely on point-wise prediction paradigms that overlook synergistic drug effects and potential adverse drug-drug interactions (DDIs). We propose FLAME, a fine-grained list-wise alignment framework for large language models (LLMs), enabling drug-by-drug generation of drug lists. FLAME formulates recommendation as a sequential decision process, where each step adds or removes a single drug. To provide fine-grained learning signals, we devise step-wise Group Relative Policy Optimization (GRPO) with potential-based reward shaping, which explicitly models DDIs and optimizes the contribution of each drug to the overall prescription. Furthermore, FLAME enhances patient modeling by integrating structured clinical knowledge and collaborative information into the representation space of LLMs. Experiments on benchmark datasets demonstrate that FLAME achieves state-of-the-art performance, delivering superior accuracy, controllable safety-accuracy trade-offs, and strong generalization across diverse clinical scenarios. Our code is available at https://github.com/cxfann/Flame."
  },
  {
    "title": "Temporal Sampling for Forgotten Reasoning in LLMs",
    "url": "http://arxiv.org/abs/2505.20196v1",
    "arxiv_id": "2505.20196v1",
    "authors": [
      "Yuetai Li",
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Xiang Yue",
      "Radha Poovendran"
    ],
    "published": "2025-05-26T16:39:52+00:00",
    "summary": "Fine-tuning large language models (LLMs) is intended to improve their reasoning capabilities, yet we uncover a counterintuitive effect: models often forget how to solve problems they previously answered correctly during training. We term this phenomenon temporal forgetting and show that it is widespread across model sizes, fine-tuning methods (both Reinforcement Learning and Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this gap, we introduce Temporal Sampling, a simple decoding strategy that draws outputs from multiple checkpoints along the training trajectory. This approach recovers forgotten solutions without retraining or ensembling, and leads to substantial improvements in reasoning performance, gains from 4 to 19 points in Pass@k and consistent gains in Majority@k across several benchmarks. We further extend our method to LoRA-adapted models, demonstrating that storing only adapter weights across checkpoints achieves similar benefits with minimal storage cost. By leveraging the temporal diversity inherent in training, Temporal Sampling offers a practical, compute-efficient way to surface hidden reasoning ability and rethink how we evaluate LLMs."
  },
  {
    "title": "URPlanner: A Universal Paradigm For Collision-Free Robotic Motion Planning Based on Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.20175v1",
    "arxiv_id": "2505.20175v1",
    "authors": [
      "Fengkang Ying",
      "Hanwen Zhang",
      "Haozhe Wang",
      "Huishi Huang",
      "Marcelo H. Ang Jr"
    ],
    "published": "2025-05-26T16:15:42+00:00",
    "summary": "Collision-free motion planning for redundant robot manipulators in complex environments is yet to be explored. Although recent advancements at the intersection of deep reinforcement learning (DRL) and robotics have highlighted its potential to handle versatile robotic tasks, current DRL-based collision-free motion planners for manipulators are highly costly, hindering their deployment and application. This is due to an overreliance on the minimum distance between the manipulator and obstacles, inadequate exploration and decision-making by DRL, and inefficient data acquisition and utilization. In this article, we propose URPlanner, a universal paradigm for collision-free robotic motion planning based on DRL. URPlanner offers several advantages over existing approaches: it is platform-agnostic, cost-effective in both training and deployment, and applicable to arbitrary manipulators without solving inverse kinematics. To achieve this, we first develop a parameterized task space and a universal obstacle avoidance reward that is independent of minimum distance. Second, we introduce an augmented policy exploration and evaluation algorithm that can be applied to various DRL algorithms to enhance their performance. Third, we propose an expert data diffusion strategy for efficient policy learning, which can produce a large-scale trajectory dataset from only a few expert demonstrations. Finally, the superiority of the proposed methods is comprehensively verified through experiments."
  },
  {
    "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents",
    "url": "http://arxiv.org/abs/2505.20148v1",
    "arxiv_id": "2505.20148v1",
    "authors": [
      "Ziming Wei",
      "Bingqian Lin",
      "Zijian Jiao",
      "Yunshuang Nie",
      "Liang Ma",
      "Yuecheng Liu",
      "Yuzheng Zhuang",
      "Xiaodan Liang"
    ],
    "published": "2025-05-26T15:48:14+00:00",
    "summary": "Spatial Planning is a crucial part in the field of spatial intelligence, which requires the understanding and planning about object arrangements in space perspective. AI agents with the spatial planning ability can better adapt to various real-world applications, including robotic manipulation, automatic assembly, urban planning etc. Recent works have attempted to construct benchmarks for evaluating the spatial intelligence of Multimodal Large Language Models (MLLMs). Nevertheless, these benchmarks primarily focus on spatial reasoning based on typical Visual Question-Answering (VQA) forms, which suffers from the gap between abstract spatial understanding and concrete task execution. In this work, we take a step further to build a comprehensive benchmark called MineAnyBuild, aiming to evaluate the spatial planning ability of open-world AI agents in the Minecraft game. Specifically, MineAnyBuild requires an agent to generate executable architecture building plans based on the given multi-modal human instructions. It involves 4,000 curated spatial planning tasks and also provides a paradigm for infinitely expandable data collection by utilizing rich player-generated content. MineAnyBuild evaluates spatial planning through four core supporting dimensions: spatial understanding, spatial reasoning, creativity, and spatial commonsense. Based on MineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based agents, revealing the severe limitations but enormous potential in their spatial planning abilities. We believe our MineAnyBuild will open new avenues for the evaluation of spatial intelligence and help promote further development for open-world AI agents capable of spatial planning."
  },
  {
    "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities",
    "url": "http://arxiv.org/abs/2505.20147v1",
    "arxiv_id": "2505.20147v1",
    "authors": [
      "Jin Wang",
      "Yao Lai",
      "Aoxue Li",
      "Shifeng Zhang",
      "Jiacheng Sun",
      "Ning Kang",
      "Chengyue Wu",
      "Zhenguo Li",
      "Ping Luo"
    ],
    "published": "2025-05-26T15:46:53+00:00",
    "summary": "The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning."
  },
  {
    "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.20131v1",
    "arxiv_id": "2505.20131v1",
    "authors": [
      "Yuanxin Zhuang",
      "Dazhong Shen",
      "Ying Sun"
    ],
    "published": "2025-05-26T15:29:08+00:00",
    "summary": "Molecular editing aims to modify a given molecule to optimize desired chemical properties while preserving structural similarity. However, current approaches typically rely on string-based or continuous representations, which fail to adequately capture the discrete, graph-structured nature of molecules, resulting in limited structural fidelity and poor controllability. In this paper, we propose MolEditRL, a molecular editing framework that explicitly integrates structural constraints with precise property optimization. Specifically, MolEditRL consists of two stages: (1) a discrete graph diffusion model pretrained to reconstruct target molecules conditioned on source structures and natural language instructions; (2) an editing-aware reinforcement learning fine-tuning stage that further enhances property alignment and structural preservation by explicitly optimizing editing decisions under graph constraints. For comprehensive evaluation, we construct MolEdit-Instruct, the largest and most property-rich molecular editing dataset, comprising 3 million diverse examples spanning single- and multi-property tasks across 10 chemical attributes. Experimental results demonstrate that MolEditRL significantly outperforms state-of-the-art methods in both property optimization accuracy and structural fidelity, achieving a 74\\% improvement in editing success rate while using 98\\% fewer parameters."
  },
  {
    "title": "Proxy-Free GFlowNet",
    "url": "http://arxiv.org/abs/2505.20110v1",
    "arxiv_id": "2505.20110v1",
    "authors": [
      "Ruishuo Chen",
      "Xun Wang",
      "Rui Hu",
      "Zhuoran Li",
      "Longbo Huang"
    ],
    "published": "2025-05-26T15:12:22+00:00",
    "summary": "Generative Flow Networks (GFlowNets) are a promising class of generative models designed to sample diverse, high-reward structures by modeling distributions over compositional objects. In many real-world applications, obtaining the reward function for such objects is expensive, time-consuming, or requires human input, making it necessary to train GFlowNets from historical datasets. Most existing methods adopt a model-based approach, learning a proxy model from the dataset to approximate the reward function. However, this strategy inherently ties the quality of the learned policy to the accuracy of the proxy, introducing additional complexity and uncertainty into the training process. To overcome these limitations, we propose \\textbf{Trajectory-Distilled GFlowNet (TD-GFN)}, a \\emph{proxy-free} training framework that eliminates the need for out-of-dataset reward queries. Our method is motivated by the key observation that different edges in the associated directed acyclic graph (DAG) contribute unequally to effective policy learning. TD-GFN leverages inverse reinforcement learning to estimate edge-level rewards from the offline dataset, which are then used to ingeniously prune the DAG and guide backward trajectory sampling during training. This approach directs the policy toward high-reward regions while reducing the complexity of model fitting. Empirical results across multiple tasks show that TD-GFN trains both efficiently and reliably, significantly outperforming existing baselines in convergence speed and sample quality."
  },
  {
    "title": "Classical-to-quantum transfer of geometric phase for non-interferometric phase measurement and manipulation of quantum state",
    "url": "http://arxiv.org/abs/2505.20108v1",
    "arxiv_id": "2505.20108v1",
    "authors": [
      "Vimlesh Kumar",
      "Chahat Kaushik",
      "M. Ebrahim-Zadeh",
      "C. M. Chandrashekar",
      "G. K. Samanta"
    ],
    "published": "2025-05-26T15:12:04+00:00",
    "summary": "The geometric phase, originating from the cyclic evolution of a state, such as polarization on the Poincar\\'e sphere, is typically measured through interferometric approaches that often include unwanted contributions from the dynamic phase. Here, we present a non-interferometric technique based on quantum correlation of pair photons to measure the geometric phase of a classical beam. The transfer of geometric phase of the classical pump beam arising from the cyclic evolution of its polarization state on the Poincar\\'e sphere onto the polarization-entangled pair photons generated via spontaneous parametric down-conversion in a Sagnac interferometer enables easy control over the quantum state. Characterization of the generated quantum states reveals that the geometric phase of the pump beam controls the coincidence counts, entanglement visibility, Bell's parameter, quantum state tomography, and fidelity in close agreement with theoretical predictions. We observe sinusoidal modulation of the Bell's parameter and state fidelity with changes in the geometric phase, resulting in transitions between orthogonal Bell states and Bell-like maximally entangled states. Our results establish the geometric phase of the classical pump as a tunable parameter for quantum state control, offering a compact, passive platform for phase manipulation in quantum photonic systems, enabling geometric phase-based quantum gates, and compensating unwanted phase acquired by the quantum state on propagation."
  },
  {
    "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.18129v1",
    "arxiv_id": "2505.18129v1",
    "authors": [
      "Yan Ma",
      "Linge Du",
      "Xuyang Shen",
      "Shaoxiang Chen",
      "Pengfei Li",
      "Qibing Ren",
      "Lizhuang Ma",
      "Yuchao Dai",
      "Pengfei Liu",
      "Junjie Yan"
    ],
    "published": "2025-05-23T17:41:14+00:00",
    "summary": "Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at https://github.com/MiniMax-AI."
  },
  {
    "title": "Tuning Thermal Conductivity and Electron-Phonon Interactions in Carbon and Boron Nitride Moir\u00e9 Diamanes via Twist Angle Manipulation",
    "url": "http://arxiv.org/abs/2505.18127v1",
    "arxiv_id": "2505.18127v1",
    "authors": [
      "Rustam Arabov",
      "Nikita Rybin",
      "Victor Demin",
      "Mikhail Polovinkin",
      "Alexander Kvashnin",
      "Leonid Chernozatonskii",
      "Alexander Shapeev"
    ],
    "published": "2025-05-23T17:36:35+00:00",
    "summary": "We have investigated the effect of interlayer twist angle on the in-plane lattice thermal conductivity and the band gap renormalization in diamane-like hydrogenated bilayer boron nitride (BN) and graphene Moir\\'e lattices. Machine learning moment tensor potentials were used for calculating energies and forces of interatomic interactions. The methods based on the solution of the Boltzmann transport equation (BTE) for phonons and the Green-Kubo (GK) formula were utilized to obtain LTC values. The 20-40\\% difference in LTC values obtained with GK and BTE-based methods showed the importance of high-order anharmonic contributions to LTC in the BN-based lattice with $\\theta=21.8^\\circ$ and all considered graphene-based structures. Significant reduction (by 4.5 - 9 times) of the in-plane LTC with the increase in the twist angle was observed in the Moir\\'e lattices. This LTC reduction is caused by the decrease of phonon lifetimes. The phonon lifetimes decrease due to the growth of structural disorder in the Moir\\'e lattices with the twist angle increase. We also show that the growth of disorder with increasing twist angle affects the electron-phonon interactions. This leads to higher band gap renormalization (induced by classical nuclei motion) at higher twist angles. High band gap renormalization (even at T = 0 K) values obtained considering the quantum nuclear effects are caused by the high frequencies of lattice vibrations in the Moir\\'e lattices. These high frequencies are caused by the presence of light hydrogen atoms on the surfaces of the structures. Understanding of the twist-angle-induced disorder effect on phonon properties, LTC and electron-phonon coupling in the Moir\\'e lattices provides a fundamental basis for manipulating the thermal and electronic properties of these structures, making them promising for applications in thermoelectrics, microelectronics and optoelectronics."
  },
  {
    "title": "Reward Model Overoptimisation in Iterated RLHF",
    "url": "http://arxiv.org/abs/2505.18126v1",
    "arxiv_id": "2505.18126v1",
    "authors": [
      "Lorenz Wolf",
      "Robert Kirk",
      "Mirco Musolesi"
    ],
    "published": "2025-05-23T17:36:13+00:00",
    "summary": "Reinforcement learning from human feedback (RLHF) is a widely used method for aligning large language models with human preferences. However, RLHF often suffers from reward model overoptimisation, in which models overfit to the reward function, resulting in non-generalisable policies that exploit the idiosyncrasies and peculiarities of the reward function. A common mitigation is iterated RLHF, in which reward models are repeatedly retrained with updated human feedback and policies are re-optimised. Despite its increasing adoption, the dynamics of overoptimisation in this setting remain poorly understood. In this work, we present the first comprehensive study of overoptimisation in iterated RLHF. We systematically analyse key design choices - how reward model training data is transferred across iterations, which reward function is used for optimisation, and how policies are initialised. Using the controlled AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over successive iterations, as reward models increasingly approximate ground-truth preferences. However, performance gains diminish over time, and while reinitialising from the base policy is robust, it limits optimisation flexibility. Other initialisation strategies often fail to recover from early overoptimisation. These findings offer actionable insights for building more stable and generalisable RLHF pipelines."
  },
  {
    "title": "ProgRM: Build Better GUI Agents with Progress Rewards",
    "url": "http://arxiv.org/abs/2505.18121v1",
    "arxiv_id": "2505.18121v1",
    "authors": [
      "Danyang Zhang",
      "Situo Zhang",
      "Ziyue Yang",
      "Zichen Zhu",
      "Zihan Zhao",
      "Ruisheng Cao",
      "Lu Chen",
      "Kai Yu"
    ],
    "published": "2025-05-23T17:23:11+00:00",
    "summary": "LLM-based (Large Language Model) GUI (Graphical User Interface) agents can potentially reshape our daily lives significantly. However, current LLM-based GUI agents suffer from the scarcity of high-quality training data owing to the difficulties of trajectory collection and reward annotation. Existing works have been exploring LLMs to collect trajectories for imitation learning or to offer reward signals for online RL training. However, the Outcome Reward Model (ORM) used in existing works cannot provide finegrained feedback and can over-penalize the valuable steps in finally failed trajectories. To this end, we propose Progress Reward Model (ProgRM) to provide dense informative intermediate rewards by predicting a task completion progress for each step in online training. To handle the challenge of progress reward label annotation, we further design an efficient LCS-based (Longest Common Subsequence) self-annotation algorithm to discover the key steps in trajectories and assign progress labels accordingly. ProgRM is evaluated with extensive experiments and analyses. Actors trained with ProgRM outperform leading proprietary LLMs and ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for experiments will be made publicly available upon acceptance."
  },
  {
    "title": "Bridging Supervised Learning and Reinforcement Learning in Math Reasoning",
    "url": "http://arxiv.org/abs/2505.18116v1",
    "arxiv_id": "2505.18116v1",
    "authors": [
      "Huayu Chen",
      "Kaiwen Zheng",
      "Qinsheng Zhang",
      "Ganqu Cui",
      "Yin Cui",
      "Haotian Ye",
      "Tsung-Yi Lin",
      "Ming-Yu Liu",
      "Jun Zhu",
      "Haoxiang Wang"
    ],
    "published": "2025-05-23T17:17:40+00:00",
    "summary": "Reinforcement Learning (RL) has played a central role in the recent surge of LLMs' math abilities by enabling self-improvement through binary verifier signals. In contrast, Supervised Learning (SL) is rarely considered for such verification-driven training, largely due to its heavy reliance on reference answers and inability to reflect on mistakes. In this work, we challenge the prevailing notion that self-improvement is exclusive to RL and propose Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to reflect on their failures and improve autonomously with no external teachers. In online training, instead of throwing away self-generated negative answers, NFT constructs an implicit negative policy to model them. This implicit policy is parameterized with the same positive LLM we target to optimize on positive data, enabling direct policy optimization on all LLMs' generations. We conduct experiments on 7B and 32B models in math reasoning tasks. Results consistently show that through the additional leverage of negative feedback, NFT significantly improves over SL baselines like Rejection sampling Fine-Tuning, matching or even surpassing leading RL algorithms like GRPO and DAPO. Furthermore, we demonstrate that NFT and GRPO are actually equivalent in strict-on-policy training, even though they originate from entirely different theoretical foundations. Our experiments and theoretical findings bridge the gap between SL and RL methods in binary-feedback learning systems."
  },
  {
    "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.17022v1",
    "arxiv_id": "2505.17022v1",
    "authors": [
      "Chengqi Duan",
      "Rongyao Fang",
      "Yuqing Wang",
      "Kun Wang",
      "Linjiang Huang",
      "Xingyu Zeng",
      "Hongsheng Li",
      "Xihui Liu"
    ],
    "published": "2025-05-22T17:59:58+00:00",
    "summary": "Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1."
  },
  {
    "title": "SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward",
    "url": "http://arxiv.org/abs/2505.17018v1",
    "arxiv_id": "2505.17018v1",
    "authors": [
      "Kaixuan Fan",
      "Kaituo Feng",
      "Haoming Lyu",
      "Dongzhan Zhou",
      "Xiangyu Yue"
    ],
    "published": "2025-05-22T17:59:53+00:00",
    "summary": "Recent advances have shown success in eliciting strong reasoning abilities in multimodal large language models (MLLMs) through rule-based reinforcement learning (RL) with outcome rewards. However, this paradigm typically lacks supervision over the thinking process leading to the final outcome.As a result, the model may learn sub-optimal reasoning strategies, which can hinder its generalization ability. In light of this, we propose SophiaVL-R1, as an attempt to add reward signals for the thinking process in this paradigm. To achieve this, we first train a thinking reward model that evaluates the quality of the entire thinking process. Given that the thinking reward may be unreliable for certain samples due to reward hacking, we propose the Trust-GRPO method, which assigns a trustworthiness weight to the thinking reward during training. This weight is computed based on the thinking reward comparison of responses leading to correct answers versus incorrect answers, helping to mitigate the impact of potentially unreliable thinking rewards. Moreover, we design an annealing training strategy that gradually reduces the thinking reward over time, allowing the model to rely more on the accurate rule-based outcome reward in later training stages. Experiments show that our SophiaVL-R1 surpasses a series of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU), demonstrating strong reasoning and generalization capabilities. Notably, our SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite the latter having 10 times more parameters. All code, models, and datasets are made publicly available at https://github.com/kxfan2002/SophiaVL-R1."
  },
  {
    "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
    "url": "http://arxiv.org/abs/2505.17017v1",
    "arxiv_id": "2505.17017v1",
    "authors": [
      "Chengzhuo Tong",
      "Ziyu Guo",
      "Renrui Zhang",
      "Wenyu Shan",
      "Xinyu Wei",
      "Zhenghao Xing",
      "Hongsheng Li",
      "Pheng-Ann Heng"
    ],
    "published": "2025-05-22T17:59:49+00:00",
    "summary": "Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT"
  },
  {
    "title": "Interactive Post-Training for Vision-Language-Action Models",
    "url": "http://arxiv.org/abs/2505.17016v1",
    "arxiv_id": "2505.17016v1",
    "authors": [
      "Shuhan Tan",
      "Kairan Dou",
      "Yue Zhao",
      "Philipp Kr\u00e4henb\u00fchl"
    ],
    "published": "2025-05-22T17:59:45+00:00",
    "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.   RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision."
  },
  {
    "title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models",
    "url": "http://arxiv.org/abs/2505.17015v1",
    "arxiv_id": "2505.17015v1",
    "authors": [
      "Runsen Xu",
      "Weiyao Wang",
      "Hao Tang",
      "Xingyu Chen",
      "Xiaodong Wang",
      "Fu-Jen Chu",
      "Dahua Lin",
      "Matt Feiszli",
      "Kevin J. Liang"
    ],
    "published": "2025-05-22T17:59:39+00:00",
    "summary": "Multi-modal large language models (MLLMs) have rapidly advanced in visual tasks, yet their spatial understanding remains limited to single images, leaving them ill-suited for robotics and other real-world applications that require multi-frame reasoning. In this paper, we propose a framework to equip MLLMs with robust multi-frame spatial understanding by integrating depth perception, visual correspondence, and dynamic perception. Central to our approach is the MultiSPA dataset, a novel, large-scale collection of more than 27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves significant gains over baselines and proprietary systems, demonstrating scalable, generalizable multi-frame reasoning. We further observe multi-task benefits and early indications of emergent capabilities in challenging scenarios, and showcase how our model can serve as a multi-frame reward annotator for robotics."
  },
  {
    "title": "On the creation of narrow AI: hierarchy and nonlocality of neural network skills",
    "url": "http://arxiv.org/abs/2505.15811v1",
    "arxiv_id": "2505.15811v1",
    "authors": [
      "Eric J. Michaud",
      "Asher Parker-Sartori",
      "Max Tegmark"
    ],
    "published": "2025-05-21T17:59:21+00:00",
    "summary": "We study the problem of creating strong, yet narrow, AI systems. While recent AI progress has been driven by the training of large general-purpose foundation models, the creation of smaller models specialized for narrow domains could be valuable for both efficiency and safety. In this work, we explore two challenges involved in creating such systems, having to do with basic properties of how neural networks learn and structure their representations. The first challenge regards when it is possible to train narrow models from scratch. Through experiments on a synthetic task, we find that it is sometimes necessary to train networks on a wide distribution of data to learn certain narrow skills within that distribution. This effect arises when skills depend on each other hierarchically, and training on a broad distribution introduces a curriculum which substantially accelerates learning. The second challenge regards how to transfer particular skills from large general models into small specialized models. We find that model skills are often not perfectly localized to a particular set of prunable components. However, we find that methods based on pruning can still outperform distillation. We investigate the use of a regularization objective to align desired skills with prunable components while unlearning unnecessary skills."
  },
  {
    "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents",
    "url": "http://arxiv.org/abs/2505.15810v1",
    "arxiv_id": "2505.15810v1",
    "authors": [
      "Yuqi Zhou",
      "Sunhao Dai",
      "Shuai Wang",
      "Kaiwen Zhou",
      "Qinqlin Jia",
      "Junxu"
    ],
    "published": "2025-05-21T17:59:09+00:00",
    "summary": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm, coupling online Reinforcement Learning (RL) with explicit chain-of-thought reasoning prior to object grounding and thereby achieving substantial performance gains. In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update-each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks. Input design: Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance. Output evaluation: Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality. Policy update: Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases. To address these issues, we propose three targeted solutions. First, we adopt a Fast Thinking Template that encourages direct answer generation, reducing excessive reasoning during training. Second, we incorporate a box size constraint into the reward function to mitigate reward hacking. Third, we revise the RL objective by adjusting length normalization and adding a difficulty-aware scaling factor, enabling better optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro. This surpasses all prior models of similar size and even outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI agent grounding. The project repository is available at https://github.com/Yuqi-Zhou/GUI-G1."
  },
  {
    "title": "MMaDA: Multimodal Large Diffusion Language Models",
    "url": "http://arxiv.org/abs/2505.15809v1",
    "arxiv_id": "2505.15809v1",
    "authors": [
      "Ling Yang",
      "Ye Tian",
      "Bowen Li",
      "Xinchen Zhang",
      "Ke Shen",
      "Yunhai Tong",
      "Mengdi Wang"
    ],
    "published": "2025-05-21T17:59:05+00:00",
    "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at: https://github.com/Gen-Verse/MMaDA"
  },
  {
    "title": "Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering",
    "url": "http://arxiv.org/abs/2505.15805v1",
    "arxiv_id": "2505.15805v1",
    "authors": [
      "Hwan Chang",
      "Yumin Kim",
      "Yonghyun Jun",
      "Hwanhee Lee"
    ],
    "published": "2025-05-21T17:58:11+00:00",
    "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as enterprise and government, ensuring that they adhere to user-defined security policies within context is critical-especially with respect to information non-disclosure. While prior LLM studies have focused on general safety and socially sensitive data, large-scale benchmarks for contextual security preservation against attacks remain lacking. To address this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating LLM adherence to contextual non-disclosure policies in question answering. Derived from realistic contexts, our dataset includes explicit policies and queries designed as direct and challenging indirect attacks seeking prohibited information. We evaluate 10 LLMs on our benchmark and reveal a significant vulnerability: many models violate user-defined policies and leak sensitive information. This failure is particularly severe against indirect attacks, highlighting a critical gap in current LLM safety alignment for sensitive applications. Our analysis reveals that while models can often identify the correct answer to a query, they struggle to incorporate policy constraints during generation. In contrast, they exhibit a partial ability to revise outputs when explicitly prompted. Our findings underscore the urgent need for more robust methods to guarantee contextual security."
  },
  {
    "title": "STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs",
    "url": "http://arxiv.org/abs/2505.15804v1",
    "arxiv_id": "2505.15804v1",
    "authors": [
      "Zongzhao Li",
      "Zongyang Ma",
      "Mingze Li",
      "Songyou Li",
      "Yu Rong",
      "Tingyang Xu",
      "Ziqi Zhang",
      "Deli Zhao",
      "Wenbing Huang"
    ],
    "published": "2025-05-21T17:57:38+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at https://github.com/zongzhao23/STAR-R1."
  },
  {
    "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
    "url": "http://arxiv.org/abs/2505.14684v1",
    "arxiv_id": "2505.14684v1",
    "authors": [
      "Haolei Xu",
      "Yuchen Yan",
      "Yongliang Shen",
      "Wenqi Zhang",
      "Guiyang Hou",
      "Shengpei Jiang",
      "Kaitao Song",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "published": "2025-05-20T17:59:31+00:00",
    "summary": "Large language models (LLMs) have achieved remarkable progress on mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits."
  },
  {
    "title": "Emerging Properties in Unified Multimodal Pretraining",
    "url": "http://arxiv.org/abs/2505.14683v1",
    "arxiv_id": "2505.14683v1",
    "authors": [
      "Chaorui Deng",
      "Deyao Zhu",
      "Kunchang Li",
      "Chenhui Gou",
      "Feng Li",
      "Zeyu Wang",
      "Shu Zhong",
      "Weihao Yu",
      "Xiaonan Nie",
      "Ziang Song",
      "Guang Shi",
      "Haoqi Fan"
    ],
    "published": "2025-05-20T17:59:30+00:00",
    "summary": "Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/"
  },
  {
    "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14677v1",
    "arxiv_id": "2505.14677v1",
    "authors": [
      "Jiaer Xia",
      "Yuhang Zang",
      "Peng Gao",
      "Yixuan Li",
      "Kaiyang Zhou"
    ],
    "published": "2025-05-20T17:58:35+00:00",
    "summary": "Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM -- by prompting the model to produce a reasoning chain before providing an answer -- can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks."
  },
  {
    "title": "Reward Reasoning Model",
    "url": "http://arxiv.org/abs/2505.14674v1",
    "arxiv_id": "2505.14674v1",
    "authors": [
      "Jiaxin Guo",
      "Zewen Chi",
      "Li Dong",
      "Qingxiu Dong",
      "Xun Wu",
      "Shaohan Huang",
      "Furu Wei"
    ],
    "published": "2025-05-20T17:58:03+00:00",
    "summary": "Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning."
  },
  {
    "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment",
    "url": "http://arxiv.org/abs/2505.14667v1",
    "arxiv_id": "2505.14667v1",
    "authors": [
      "Wonje Jeung",
      "Sangyeon Yoon",
      "Minsuk Kahng",
      "Albert No"
    ],
    "published": "2025-05-20T17:54:54+00:00",
    "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem solving, but their structured reasoning pathways can lead to unsafe outputs when exposed to harmful prompts. Existing safety alignment methods reduce harmful outputs but can degrade reasoning depth, leading to significant trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at the start of their reasoning, in response to harmful prompts, while leaving the rest of the reasoning process unsupervised. Empirical results across multiple benchmarks indicate that SAFEPATH effectively reduces harmful outputs while maintaining reasoning performance. Specifically, SAFEPATH reduces harmful responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot variant that requires no fine-tuning. In addition, we provide a comprehensive analysis of how existing methods in LLMs generalize, or fail, when applied to reasoning-centric models, revealing critical gaps and new directions for safer AI."
  },
  {
    "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards",
    "url": "http://arxiv.org/abs/2505.13445v1",
    "arxiv_id": "2505.13445v1",
    "authors": [
      "Xiaoyuan Liu",
      "Tian Liang",
      "Zhiwei He",
      "Jiahao Xu",
      "Wenxuan Wang",
      "Pinjia He",
      "Zhaopeng Tu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "published": "2025-05-19T17:59:31+00:00",
    "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. Extensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners."
  },
  {
    "title": "GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation",
    "url": "http://arxiv.org/abs/2505.13441v1",
    "arxiv_id": "2505.13441v1",
    "authors": [
      "Abhay Deshpande",
      "Yuquan Deng",
      "Arijit Ray",
      "Jordi Salvador",
      "Winson Han",
      "Jiafei Duan",
      "Kuo-Hao Zeng",
      "Yuke Zhu",
      "Ranjay Krishna",
      "Rose Hendrix"
    ],
    "published": "2025-05-19T17:59:06+00:00",
    "summary": "We present GrasMolmo, a generalizable open-vocabulary task-oriented grasping (TOG) model. GraspMolmo predicts semantically appropriate, stable grasps conditioned on a natural language instruction and a single RGB-D frame. For instance, given \"pour me some tea\", GraspMolmo selects a grasp on a teapot handle rather than its body. Unlike prior TOG methods, which are limited by small datasets, simplistic language, and uncluttered scenes, GraspMolmo learns from PRISM, a novel large-scale synthetic dataset of 379k samples featuring cluttered environments and diverse, realistic task descriptions. We fine-tune the Molmo visual-language model on this data, enabling GraspMolmo to generalize to novel open-vocabulary instructions and objects. In challenging real-world evaluations, GraspMolmo achieves state-of-the-art results, with a 70% prediction success on complex tasks, compared to the 35% achieved by the next best alternative. GraspMolmo also successfully demonstrates the ability to predict semantically correct bimanual grasps zero-shot. We release our synthetic dataset, code, model, and benchmarks to accelerate research in task-semantic robotic manipulation, which, along with videos, are available at https://abhaybd.github.io/GraspMolmo/."
  },
  {
    "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
    "url": "http://arxiv.org/abs/2505.13438v1",
    "arxiv_id": "2505.13438v1",
    "authors": [
      "Penghui Qi",
      "Zichen Liu",
      "Tianyu Pang",
      "Chao Du",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "published": "2025-05-19T17:58:44+00:00",
    "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency."
  },
  {
    "title": "KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical Models Enables Precise Replication of Able-Bodied and Impaired Movement from Markerless Motion Capture",
    "url": "http://arxiv.org/abs/2505.13436v1",
    "arxiv_id": "2505.13436v1",
    "authors": [
      "R. James Cotton"
    ],
    "published": "2025-05-19T17:58:03+00:00",
    "summary": "Broader access to high-quality movement analysis could greatly benefit movement science and rehabilitation, such as allowing more detailed characterization of movement impairments and responses to interventions, or even enabling early detection of new neurological conditions or fall risk. While emerging technologies are making it easier to capture kinematics with biomechanical models, or how joint angles change over time, inferring the underlying physics that give rise to these movements, including ground reaction forces, joint torques, or even muscle activations, is still challenging. Here we explore whether imitation learning applied to a biomechanical model from a large dataset of movements from able-bodied and impaired individuals can learn to compute these inverse dynamics. Although imitation learning in human pose estimation has seen great interest in recent years, our work differences in several ways: we focus on using an accurate biomechanical model instead of models adopted for computer vision, we test it on a dataset that contains participants with impaired movements, we reported detailed tracking metrics relevant for the clinical measurement of movement including joint angles and ground contact events, and finally we apply imitation learning to a muscle-driven neuromusculoskeletal model. We show that our imitation learning policy, KinTwin, can accurately replicate the kinematics of a wide range of movements, including those with assistive devices or therapist assistance, and that it can infer clinically meaningful differences in joint torques and muscle activations. Our work demonstrates the potential for using imitation learning to enable high-quality movement analysis in clinical practice."
  },
  {
    "title": "A Practical Guide for Incorporating Symmetry in Diffusion Policy",
    "url": "http://arxiv.org/abs/2505.13431v1",
    "arxiv_id": "2505.13431v1",
    "authors": [
      "Dian Wang",
      "Boce Hu",
      "Shuran Song",
      "Robin Walters",
      "Robert Platt"
    ],
    "published": "2025-05-19T17:55:28+00:00",
    "summary": "Recently, equivariant neural networks for policy learning have shown promising improvements in sample efficiency and generalization, however, their wide adoption faces substantial barriers due to implementation complexity. Equivariant architectures typically require specialized mathematical formulations and custom network design, posing significant challenges when integrating with modern policy frameworks like diffusion-based models. In this paper, we explore a number of straightforward and practical approaches to incorporate symmetry benefits into diffusion policies without the overhead of full equivariant designs. Specifically, we investigate (i) invariant representations via relative trajectory actions and eye-in-hand perception, (ii) integrating equivariant vision encoders, and (iii) symmetric feature extraction with pretrained encoders using Frame Averaging. We first prove that combining eye-in-hand perception with relative or delta action parameterization yields inherent SE(3)-invariance, thus improving policy generalization. We then perform a systematic experimental study on those design choices for integrating symmetry in diffusion policies, and conclude that an invariant representation with equivariant feature extraction significantly improves the policy performance. Our method achieves performance on par with or exceeding fully equivariant architectures while greatly simplifying implementation."
  },
  {
    "title": "Bracing for Impact: Robust Humanoid Push Recovery and Locomotion with Reduced Order Models",
    "url": "http://arxiv.org/abs/2505.11495v1",
    "arxiv_id": "2505.11495v1",
    "authors": [
      "Lizhi Yang",
      "Blake Werner",
      "Adrian B. Ghansah",
      "Aaron D. Ames"
    ],
    "published": "2025-05-16T17:57:18+00:00",
    "summary": "Push recovery during locomotion will facilitate the deployment of humanoid robots in human-centered environments. In this paper, we present a unified framework for walking control and push recovery for humanoid robots, leveraging the arms for push recovery while dynamically walking. The key innovation is to use the environment, such as walls, to facilitate push recovery by combining Single Rigid Body model predictive control (SRB-MPC) with Hybrid Linear Inverted Pendulum (HLIP) dynamics to enable robust locomotion, push detection, and recovery by utilizing the robot's arms to brace against such walls and dynamically adjusting the desired contact forces and stepping patterns. Extensive simulation results on a humanoid robot demonstrate improved perturbation rejection and tracking performance compared to HLIP alone, with the robot able to recover from pushes up to 100N for 0.2s while walking at commanded speeds up to 0.5m/s. Robustness is further validated in scenarios with angled walls and multi-directional pushes."
  },
  {
    "title": "SHIELD: Safety on Humanoids via CBFs In Expectation on Learned Dynamics",
    "url": "http://arxiv.org/abs/2505.11494v1",
    "arxiv_id": "2505.11494v1",
    "authors": [
      "Lizhi Yang",
      "Blake Werner",
      "Ryan K. Cosner",
      "David Fridovich-Keil",
      "Preston Culbertson",
      "Aaron D. Ames"
    ],
    "published": "2025-05-16T17:57:03+00:00",
    "summary": "Robot learning has produced remarkably effective ``black-box'' controllers for complex tasks such as dynamic locomotion on humanoids. Yet ensuring dynamic safety, i.e., constraint satisfaction, remains challenging for such policies. Reinforcement learning (RL) embeds constraints heuristically through reward engineering, and adding or modifying constraints requires retraining. Model-based approaches, like control barrier functions (CBFs), enable runtime constraint specification with formal guarantees but require accurate dynamics models. This paper presents SHIELD, a layered safety framework that bridges this gap by: (1) training a generative, stochastic dynamics residual model using real-world data from hardware rollouts of the nominal controller, capturing system behavior and uncertainties; and (2) adding a safety layer on top of the nominal (learned locomotion) controller that leverages this model via a stochastic discrete-time CBF formulation enforcing safety constraints in probability. The result is a minimally-invasive safety layer that can be added to the existing autonomy stack to give probabilistic guarantees of safety that balance risk and performance. In hardware experiments on an Unitree G1 humanoid, SHIELD enables safe navigation (obstacle avoidance) through varied indoor and outdoor environments using a nominal (unknown) RL controller and onboard perception."
  },
  {
    "title": "Improving Assembly Code Performance with Large Language Models via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.11480v1",
    "arxiv_id": "2505.11480v1",
    "authors": [
      "Anjiang Wei",
      "Tarun Suresh",
      "Huanmi Tan",
      "Yinglun Xu",
      "Gagandeep Singh",
      "Ke Wang",
      "Alex Aiken"
    ],
    "published": "2025-05-16T17:40:45+00:00",
    "summary": "Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance."
  },
  {
    "title": "Automatic Reward Shaping from Confounded Offline Data",
    "url": "http://arxiv.org/abs/2505.11478v1",
    "arxiv_id": "2505.11478v1",
    "authors": [
      "Mingxuan Li",
      "Junzhe Zhang",
      "Elias Bareinboim"
    ],
    "published": "2025-05-16T17:40:01+00:00",
    "summary": "A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures. Off-policy learning methods, like Q-learning, allow learners to make optimal decisions based on past experiences. This paper studies off-policy learning from biased data in complex and high-dimensional domains where \\emph{unobserved confounding} cannot be ruled out a priori. Building on the well-celebrated Deep Q-Network (DQN), we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data. Specifically, our algorithm attempts to find a safe policy for the worst-case environment compatible with the observations. We apply our method to twelve confounded Atari games, and find that it consistently dominates the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist."
  },
  {
    "title": "UMArm: Untethered, Modular, Wearable, Soft Pneumatic Arm",
    "url": "http://arxiv.org/abs/2505.11476v1",
    "arxiv_id": "2505.11476v1",
    "authors": [
      "Runze Zuo",
      "Dong Heon Han",
      "Richard Li",
      "Saima Jamal",
      "Daniel Bruder"
    ],
    "published": "2025-05-16T17:31:20+00:00",
    "summary": "Robotic arms are essential to modern industries, however, their adaptability to unstructured environments remains limited. Soft robotic arms, particularly those actuated pneumatically, offer greater adaptability in unstructured environments and enhanced safety for human-robot interaction. However, current pneumatic soft arms are constrained by limited degrees of freedom, precision, payload capacity, and reliance on bulky external pressure regulators. In this work, a novel pneumatically driven rigid-soft hybrid arm, ``UMArm'', is presented. The shortcomings of pneumatically actuated soft arms are addressed by densely integrating high-force-to-weight-ratio, self-regulated McKibben actuators onto a lightweight rigid spine structure. The modified McKibben actuators incorporate valves and controllers directly inside, eliminating the need for individual pressure lines and external regulators, significantly reducing system weight and complexity. Full untethered operation, high payload capacity, precision, and directionally tunable compliance are achieved by the UMArm. Portability is demonstrated through a wearable assistive arm experiment, and versatility is showcased by reconfiguring the system into an inchworm robot. The results of this work show that the high-degree-of-freedom, external-regulator-free pneumatically driven arm systems like the UMArm possess great potential for real-world unstructured environments."
  },
  {
    "title": "3D-Fixup: Advancing Photo Editing with 3D Priors",
    "url": "http://arxiv.org/abs/2505.10566v1",
    "arxiv_id": "2505.10566v1",
    "authors": [
      "Yen-Chi Cheng",
      "Krishna Kumar Singh",
      "Jae Shin Yoon",
      "Alex Schwing",
      "Liangyan Gui",
      "Matheus Gadelha",
      "Paul Guerrero",
      "Nanxuan Zhao"
    ],
    "published": "2025-05-15T17:59:51+00:00",
    "summary": "Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design a data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3D-aware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at https://3dfixup.github.io/"
  },
  {
    "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models",
    "url": "http://arxiv.org/abs/2505.10554v1",
    "arxiv_id": "2505.10554v1",
    "authors": [
      "Zhiyuan Hu",
      "Yibo Wang",
      "Hanze Dong",
      "Yuhui Xu",
      "Amrita Saha",
      "Caiming Xiong",
      "Bryan Hooi",
      "Junnan Li"
    ],
    "published": "2025-05-15T17:58:33+00:00",
    "summary": "Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model's \"aha moment\". However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs' reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental \"aha moments\". Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10\\% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional 2\\% average gain in the performance ceiling across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment"
  },
  {
    "title": "Loop closure grasping: Topological transformations enable strong, gentle, and versatile grasps",
    "url": "http://arxiv.org/abs/2505.10552v1",
    "arxiv_id": "2505.10552v1",
    "authors": [
      "Kentaro Barhydt",
      "O. Godson Osele",
      "Sreela Kodali",
      "Cosima du Pasquier",
      "Chase M. Hartquist",
      "H. Harry Asada",
      "Allison M. Okamura"
    ],
    "published": "2025-05-15T17:58:01+00:00",
    "summary": "Grasping mechanisms must both create and subsequently hold grasps that permit safe and effective object manipulation. Existing mechanisms address the different functional requirements of grasp creation and grasp holding using a single morphology, but have yet to achieve the simultaneous strength, gentleness, and versatility needed for many applications. We present \"loop closure grasping\", a class of robotic grasping that addresses these different functional requirements through topological transformations between open-loop and closed-loop morphologies. We formalize these morphologies for grasping, formulate the loop closure grasping method, and present principles and a design architecture that we implement using soft growing inflated beams, winches, and clamps. The mechanisms' initial open-loop topology enables versatile grasp creation via unencumbered tip movement, and closing the loop enables strong and gentle holding with effectively infinite bending compliance. Loop closure grasping circumvents the tradeoffs of single-morphology designs, enabling grasps involving historically challenging objects, environments, and configurations."
  },
  {
    "title": "Observing Bethe strings in an attractive Bose gas far from equilibrium",
    "url": "http://arxiv.org/abs/2505.10550v1",
    "arxiv_id": "2505.10550v1",
    "authors": [
      "Milena Horvath",
      "Alvise Bastianello",
      "Sudipta Dhar",
      "Rebekka Koch",
      "Yanliang Guo",
      "Jean-S\u00e9bastien Caux",
      "Manuele Landini",
      "Hanns-Christoph N\u00e4gerl"
    ],
    "published": "2025-05-15T17:56:49+00:00",
    "summary": "Bethe strings are bound states of constituent particles in a variety of interacting many-body one-dimensional (1D) integrable quantum models relevant to magnetism, nanophysics, cold atoms and beyond. As emergent fundamental excitations, they are predicted to collectively reshape observable equilibrium and dynamical properties. Small individual Bethe strings have recently been observed in quantum magnets and superconducting qubits. However, creating states featuring intermixtures of many, including large, strings remains an outstanding experimental challenge. Here, using nearly integrable ultracold Bose gases, we realize such intermixtures of Bethe strings out of equilibrium, by dynamically tuning interactions from repulsive to attractive. We measure the average binding energy of the strings, revealing the presence of bound states of more than six particles. We find further evidence for them in the momentum distribution and in Tan's contact, connected to the correlated density. Our data quantitatively agree with predictions from generalized hydrodynamics (GHD). Manipulating intermixtures of Bethe strings opens new avenues for understanding quantum coherence, nonlinear dynamics and thermalization in strongly-interacting 1D systems."
  },
  {
    "title": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning",
    "url": "http://arxiv.org/abs/2505.10547v1",
    "arxiv_id": "2505.10547v1",
    "authors": [
      "Milan Ganai",
      "Rohan Sinha",
      "Christopher Agia",
      "Daniel Morton",
      "Marco Pavone"
    ],
    "published": "2025-05-15T17:55:28+00:00",
    "summary": "Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robot's training data, i.e. out-of-distribution (OOD) failures. However, due to the high inference latency of Large Vision and Language Models, current methods rely on manually defined intervention policies to enact fallbacks, thereby lacking the ability to plan generalizable, semantically safe motions. To overcome these challenges we present FORTRESS, a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures. At a low frequency in nominal operations, FORTRESS uses multi-modal reasoners to identify goals and anticipate failure modes. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time. By bridging open-world, multi-modal reasoning with dynamics-aware planning, we eliminate the need for hard-coded fallbacks and human safety interventions. FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation."
  },
  {
    "title": "DataMIL: Selecting Data for Robot Imitation Learning with Datamodels",
    "url": "http://arxiv.org/abs/2505.09603v1",
    "arxiv_id": "2505.09603v1",
    "authors": [
      "Shivin Dass",
      "Alaa Khaddaj",
      "Logan Engstrom",
      "Aleksander Madry",
      "Andrew Ilyas",
      "Roberto Mart\u00edn-Mart\u00edn"
    ],
    "published": "2025-05-14T17:55:10+00:00",
    "summary": "Recently, the robotics community has amassed ever larger and more diverse datasets to train generalist robot policies. However, while these policies achieve strong mean performance across a variety of tasks, they often underperform on individual, specialized tasks and require further tuning on newly acquired task-specific data. Combining task-specific data with carefully curated subsets of large prior datasets via co-training can produce better specialized policies, but selecting data naively may actually harm downstream performance. To address this, we introduce DataMIL, a policy-driven data selection framework built on the datamodels paradigm that reasons about data selection in an end-to-end manner, using the policy itself to identify which data points will most improve performance. Unlike standard practices that filter data using human notions of quality (e.g., based on semantic or visual similarity), DataMIL directly optimizes data selection for task success, allowing us to select data that enhance the policy while dropping data that degrade it. To avoid performing expensive rollouts in the environment during selection, we use a novel surrogate loss function on task-specific data, allowing us to use DataMIL in the real world without degrading performance. We validate our approach on a suite of more than 60 simulation and real-world manipulation tasks - most notably showing successful data selection from the Open X-Embodiment datasets-demonstrating consistent gains in success rates and superior performance over multiple baselines. Our results underscore the importance of end-to-end, performance-aware data selection for unlocking the potential of large prior datasets in robotics. More information at https://robin-lab.cs.utexas.edu/datamodels4imitation/"
  },
  {
    "title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware",
    "url": "http://arxiv.org/abs/2505.09601v1",
    "arxiv_id": "2505.09601v1",
    "authors": [
      "Justin Yu",
      "Letian Fu",
      "Huang Huang",
      "Karim El-Refai",
      "Rares Andrei Ambrus",
      "Richard Cheng",
      "Muhammad Zubair Irshad",
      "Ken Goldberg"
    ],
    "published": "2025-05-14T17:50:35+00:00",
    "summary": "Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com"
  },
  {
    "title": "Tropical Fermat-Weber Points over Bergman Fans",
    "url": "http://arxiv.org/abs/2505.09584v1",
    "arxiv_id": "2505.09584v1",
    "authors": [
      "Shelby Cox",
      "John Sabol",
      "Roan Talbut",
      "Ruriko Yoshida"
    ],
    "published": "2025-05-14T17:33:55+00:00",
    "summary": "Given a finite set $[p]:= \\{1, \\ldots , p\\}$, it is well known that the space of all ultrametrics on $[p]$ is the Bergman fan associated to the matroid underlying the complete graph of the vertex set $[p]$. Lin et al.~showed that the set of tropical Fermat-Weber points might not be contained in the space of ultrametrics on $[p]$ even if all input data points are in the space. Here we consider a more general set up and we focus on Fermat-Weber points with respect to the tropical metric over the Bergman fan associated with a matroid with the ground set of $q$ elements. We show that there always exists a Fermat-Weber point in the Bergman fan for data in the Bergman fan and we describe explicitly the set of all Fermat-Weber points in the Bergman fan. Then we introduce the natural extension of the safety radius introduced by Atteson to the set of Fermat-Weber points in the Bergman fan of a matroid."
  },
  {
    "title": "VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation",
    "url": "http://arxiv.org/abs/2505.09577v1",
    "arxiv_id": "2505.09577v1",
    "authors": [
      "Chaofan Zhang",
      "Peng Hao",
      "Xiaoge Cao",
      "Xiaoshuai Hao",
      "Shaowei Cui",
      "Shuo Wang"
    ],
    "published": "2025-05-14T17:29:35+00:00",
    "summary": "While vision-language models have advanced significantly, their application in language-conditioned robotic manipulation is still underexplored, especially for contact-rich tasks that extend beyond visually dominant pick-and-place scenarios. To bridge this gap, we introduce Vision-Tactile-Language-Action model, a novel framework that enables robust policy generation in contact-intensive scenarios by effectively integrating visual and tactile inputs through cross-modal language grounding. A low-cost, multi-modal dataset has been constructed in a simulation environment, containing vision-tactile-action-instruction pairs specifically designed for the fingertip insertion task. Furthermore, we introduce Direct Preference Optimization (DPO) to offer regression-like supervision for the VTLA model, effectively bridging the gap between classification-based next token prediction loss and continuous robotic tasks. Experimental results show that the VTLA model outperforms traditional imitation learning methods (e.g., diffusion policies) and existing multi-modal baselines (TLA/VLA), achieving over 90% success rates on unseen peg shapes. Finally, we conduct real-world peg-in-hole experiments to demonstrate the exceptional Sim2Real performance of the proposed VTLA model. For supplementary videos and results, please visit our project website: https://sites.google.com/view/vtla"
  },
  {
    "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach",
    "url": "http://arxiv.org/abs/2505.09576v1",
    "arxiv_id": "2505.09576v1",
    "authors": [
      "Shannon Lodoen",
      "Alexi Orchard"
    ],
    "published": "2025-05-14T17:29:19+00:00",
    "summary": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude have been trained using a specialized technique called Reinforcement Learning from Human Feedback (RLHF) to fine-tune language model output using feedback from human annotators. As a result, the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more \"human-like\" than those of previous versions using only supervised learning. The increasing convergence of human and machine-written text has potentially severe ethical, sociotechnical, and pedagogical implications relating to transparency, trust, bias, and interpersonal relations. To highlight these implications, this paper presents a rhetorical analysis of some of the central procedures and processes currently being reshaped by RLHF-enhanced generative AI chatbots: upholding language conventions, information seeking practices, and expectations for social relationships. Rhetorical investigations of generative AI and LLMs have, to this point, focused largely on the persuasiveness of the content generated. Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site of rhetorical investigation from content analysis to the underlying mechanisms of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical investigation opens a new direction for further inquiry in AI ethics that considers how procedures rerouted through AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships. It will therefore be of interest to educators, researchers, scholars, and the growing number of users of generative AI chatbots."
  },
  {
    "title": "UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations",
    "url": "http://arxiv.org/abs/2505.08787v1",
    "arxiv_id": "2505.08787v1",
    "authors": [
      "Hanjung Kim",
      "Jaehyun Kang",
      "Hyolim Kang",
      "Meedeum Cho",
      "Seon Joo Kim",
      "Youngwoon Lee"
    ],
    "published": "2025-05-13T17:59:22+00:00",
    "summary": "Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill."
  },
  {
    "title": "PCS-UQ: Uncertainty Quantification via the Predictability-Computability-Stability Framework",
    "url": "http://arxiv.org/abs/2505.08784v1",
    "arxiv_id": "2505.08784v1",
    "authors": [
      "Abhineet Agarwal",
      "Michael Xiao",
      "Rebecca Barter",
      "Omer Ronen",
      "Boyu Fan",
      "Bin Yu"
    ],
    "published": "2025-05-13T17:58:16+00:00",
    "summary": "As machine learning (ML) models are increasingly deployed in high-stakes domains, trustworthy uncertainty quantification (UQ) is critical for ensuring the safety and reliability of these models. Traditional UQ methods rely on specifying a true generative model and are not robust to misspecification. On the other hand, conformal inference allows for arbitrary ML models but does not consider model selection, which leads to large interval sizes. We tackle these drawbacks by proposing a UQ method based on the predictability, computability, and stability (PCS) framework for veridical data science proposed by Yu and Kumbier. Specifically, PCS-UQ addresses model selection by using a prediction check to screen out unsuitable models. PCS-UQ then fits these screened algorithms across multiple bootstraps to assess inter-sample variability and algorithmic instability, enabling more reliable uncertainty estimates. Further, we propose a novel calibration scheme that improves local adaptivity of our prediction sets. Experiments across $17$ regression and $6$ classification datasets show that PCS-UQ achieves the desired coverage and reduces width over conformal approaches by $\\approx 20\\%$. Further, our local analysis shows PCS-UQ often achieves target coverage across subgroups while conformal methods fail to do so. For large deep-learning models, we propose computationally efficient approximation schemes that avoid the expensive multiple bootstrap trainings of PCS-UQ. Across three computer vision benchmarks, PCS-UQ reduces prediction set size over conformal methods by $20\\%$. Theoretically, we show a modified PCS-UQ algorithm is a form of split conformal inference and achieves the desired coverage with exchangeable data."
  },
  {
    "title": "Full-volume aberration-space holography",
    "url": "http://arxiv.org/abs/2505.08777v1",
    "arxiv_id": "2505.08777v1",
    "authors": [
      "Ian Christen",
      "Christopher Panuski",
      "Thomas Propson",
      "Dirk Englund"
    ],
    "published": "2025-05-13T17:54:53+00:00",
    "summary": "Simultaneous, diffraction-limited control of multiple optical beams is crucial for applications ranging from lithography to optogenetics, deep tissue imaging, and tweezer-based manipulation of cells, particles, or atoms. Despite the desire to address wider fields of view, deeper volumes, and increasingly-disordered media, spatially-varying aberrations currently restrict parallelized steering to a limited \"isoplanatic\" region over which the point spread function is invariant. Here, we overcome this limitation by combining individual propagation kernels accounting for site-specific aberrations into a single spatial light modulator (SLM) hologram. This \"aberration-space holography\" unlocks precise, parallel holographic shaping over the SLM's entire Nyquist-limited volume, enabling us to realize full-field, anisoplanatic aberration compensation for the first time. By simultaneously correcting 50 isoplanatic patches with 8 principal aberration modes, we demonstrate a full-field optical tweezer array with 8x larger field of view than the best isoplanatic correction. Extending to 3D, we increase the volume of a multiphoton volumetric display by 12x. These performance enhancements are immediately accessible to a diverse range of applications through our open-source software implementation, which combines aberration-space holography with automated experimental feedback, wavefront calibration, and alignment."
  },
  {
    "title": "HealthBench: Evaluating Large Language Models Towards Improved Human Health",
    "url": "http://arxiv.org/abs/2505.08775v1",
    "arxiv_id": "2505.08775v1",
    "authors": [
      "Rahul K. Arora",
      "Jason Wei",
      "Rebecca Soskin Hicks",
      "Preston Bowman",
      "Joaquin Qui\u00f1onero-Candela",
      "Foivos Tsimpourlas",
      "Michael Sharman",
      "Meghan Shah",
      "Andrea Vallone",
      "Alex Beutel",
      "Johannes Heidecke",
      "Karan Singhal"
    ],
    "published": "2025-05-13T17:53:59+00:00",
    "summary": "We present HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare. HealthBench consists of 5,000 multi-turn conversations between a model and an individual user or healthcare professional. Responses are evaluated using conversation-specific rubrics created by 262 physicians. Unlike previous multiple-choice or short-answer benchmarks, HealthBench enables realistic, open-ended evaluation through 48,562 unique rubric criteria spanning several health contexts (e.g., emergencies, transforming clinical data, global health) and behavioral dimensions (e.g., accuracy, instruction following, communication). HealthBench performance over the last two years reflects steady initial progress (compare GPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3 scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms GPT-4o and is 25 times cheaper. We additionally release two HealthBench variations: HealthBench Consensus, which includes 34 particularly important dimensions of model behavior validated via physician consensus, and HealthBench Hard, where the current top score is 32%. We hope that HealthBench grounds progress towards model development and applications that benefit human health."
  },
  {
    "title": "Generative Molecular Design with Steerable and Granular Synthesizability Control",
    "url": "http://arxiv.org/abs/2505.08774v1",
    "arxiv_id": "2505.08774v1",
    "authors": [
      "Jeff Guo",
      "V\u00edctor Sabanza-Gil",
      "Zlatko Jon\u010dev",
      "Jeremy S. Luterbacher",
      "Philippe Schwaller"
    ],
    "published": "2025-05-13T17:53:54+00:00",
    "summary": "Synthesizability in small molecule generative design remains a bottleneck. Existing works that do consider synthesizability can output predicted synthesis routes for generated molecules. However, there has been minimal attention in addressing the ease of synthesis and enabling flexibility to incorporate desired reaction constraints. In this work, we propose a small molecule generative design framework that enables steerable and granular synthesizability control. Generated molecules satisfy arbitrary multi-parameter optimization objectives with predicted synthesis routes containing pre-defined allowed reactions, while optionally avoiding others. One can also enforce that all reactions belong to a pre-defined set. We show the capability to mix-and-match these reaction constraints across the most common medicinal chemistry transformations. Next, we show how our framework can be used to valorize industrial byproducts towards de novo optimized molecules. Going further, we demonstrate how granular control over synthesizability constraints can loosely mimic virtual screening of ultra-large make-on-demand libraries. Using only a single GPU, we generate and dock 15k molecules to identify promising candidates in Freedom 4.0 constituting 142B make-on-demand molecules (assessing only 0.00001% of the library). Generated molecules satisfying the reaction constraints have > 90% exact match rate. Lastly, we benchmark our framework against recent synthesizability-constrained generative models and demonstrate the highest sample efficiency even when imposing the additional constraint that all molecules must be synthesizable from a single reaction type. The main theme is demonstrating that a pre-trained generalist molecular generative model can be incentivized to generate property-optimized small molecules under challenging synthesizability constraints through reinforcement learning."
  },
  {
    "title": "H$^{\\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning",
    "url": "http://arxiv.org/abs/2505.07819v1",
    "arxiv_id": "2505.07819v1",
    "authors": [
      "Yiyang Lu",
      "Yufeng Tian",
      "Zhecheng Yuan",
      "Xianbang Wang",
      "Pu Hua",
      "Zhengrong Xue",
      "Huazhe Xu"
    ],
    "published": "2025-05-12T17:59:43+00:00",
    "summary": "Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce $\\textbf{Triply-Hierarchical Diffusion Policy}~(\\textbf{H$^{\\mathbf{3}}$DP})$, a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H$^{3}$DP contains $\\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H$^{3}$DP yields a $\\mathbf{+27.5\\%}$ average relative improvement over baselines across $\\mathbf{44}$ simulation tasks and achieves superior performance in $\\mathbf{4}$ challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/."
  },
  {
    "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
    "url": "http://arxiv.org/abs/2505.07818v1",
    "arxiv_id": "2505.07818v1",
    "authors": [
      "Zeyue Xue",
      "Jie Wu",
      "Yu Gao",
      "Fangyuan Kong",
      "Lingting Zhu",
      "Mengzhao Chen",
      "Zhiheng Liu",
      "Wei Liu",
      "Qiushan Guo",
      "Weilin Huang",
      "Ping Luo"
    ],
    "published": "2025-05-12T17:59:34+00:00",
    "summary": "Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released."
  },
  {
    "title": "Pixel Motion as Universal Representation for Robot Control",
    "url": "http://arxiv.org/abs/2505.07817v1",
    "arxiv_id": "2505.07817v1",
    "authors": [
      "Kanchana Ranasinghe",
      "Xiang Li",
      "Cristina Mata",
      "Jongwoo Park",
      "Michael S Ryoo"
    ],
    "published": "2025-05-12T17:59:32+00:00",
    "summary": "We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a self-supervised manner, enabling diffusion model training on web-scale video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo for visualizations."
  },
  {
    "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.07815v1",
    "arxiv_id": "2505.07815v1",
    "authors": [
      "Seungjae Lee",
      "Daniel Ekpo",
      "Haowen Liu",
      "Furong Huang",
      "Abhinav Shrivastava",
      "Jia-Bin Huang"
    ],
    "published": "2025-05-12T17:59:11+00:00",
    "summary": "Exploration is essential for general-purpose robotic learning, especially in open-ended environments where dense rewards, explicit goals, or task-specific supervision are scarce. Vision-language models (VLMs), with their semantic reasoning over objects, spatial relations, and potential outcomes, present a compelling foundation for generating high-level exploratory behaviors. However, their outputs are often ungrounded, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity. Human exploration is often driven by the desire to discover novel scene configurations and to deepen understanding of the environment. Similarly, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE enables more diverse and meaningful exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the entropy of visited states. Moreover, the collected experience supports downstream learning, producing policies that closely match or exceed the performance of those trained on human-collected demonstrations."
  },
  {
    "title": "DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies",
    "url": "http://arxiv.org/abs/2505.07813v1",
    "arxiv_id": "2505.07813v1",
    "authors": [
      "Tony Tao",
      "Mohan Kumar Srirama",
      "Jason Jingzhou Liu",
      "Kenneth Shaw",
      "Deepak Pathak"
    ],
    "published": "2025-05-12T17:59:05+00:00",
    "summary": "Large-scale, diverse robot datasets have emerged as a promising path toward enabling dexterous manipulation policies to generalize to novel environments, but acquiring such datasets presents many challenges. While teleoperation provides high-fidelity datasets, its high cost limits its scalability. Instead, what if people could use their own hands, just as they do in everyday life, to collect data? In DexWild, a diverse team of data collectors uses their hands to collect hours of interactions across a multitude of environments and objects. To record this data, we create DexWild-System, a low-cost, mobile, and easy-to-use device. The DexWild learning framework co-trains on both human and robot demonstrations, leading to improved performance compared to training on each dataset individually. This combination results in robust robot policies capable of generalizing to novel environments, tasks, and embodiments with minimal additional robot-specific data. Experimental results demonstrate that DexWild significantly improves performance, achieving a 68.5% success rate in unseen environments-nearly four times higher than policies trained with robot data only-and offering 5.8x better cross-embodiment generalization. Video results, codebases, and instructions at https://dexwild.github.io"
  },
  {
    "title": "Anomalous spin dynamics after dual optical excitation",
    "url": "http://arxiv.org/abs/2505.06225v1",
    "arxiv_id": "2505.06225v1",
    "authors": [
      "Sergii Parchenko",
      "Peter M. Oppeneer",
      "Andreas Scherz"
    ],
    "published": "2025-05-09T17:58:55+00:00",
    "summary": "Ultrashort optical pulses are a cornerstone for manipulating electronic and magnetic states in materials on a femtosecond timescale. Conventional models assume that optical excitation primarily modifies the occupation of the electron energy levels without long-lasting altering of the coupling of individual electrons in certain processes. Here, we demonstrate that optical excitation with two femtosecond pulses that come from different directions fundamentally transforms the electron dynamics in copper, affecting the efficiency of angular momentum transfer between electrons and the lattice. Using time-resolved magneto-optical Kerr effect measurements, we reveal a ~2.5. increase in spin imbalance decay time following inverse Faraday effect excitation under dual-pump conditions compared to single-pulse excitation. This observation challenges the prevailing paradigm of ultrafast light-matter interactions, showing that dual optical excitation can transiently modify electron dynamics beyond simple changes in the energy levels occupancy. Our findings open new avenues for controlling quantum states through a dual pump approach, with implications for ultrafast spintronics and the design of novel light-driven states."
  },
  {
    "title": "VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction",
    "url": "http://arxiv.org/abs/2505.06219v1",
    "arxiv_id": "2505.06219v1",
    "authors": [
      "Noah Frahm",
      "Dongxu Zhao",
      "Andrea Dunn Beltran",
      "Ron Alterovitz",
      "Jan-Michael Frahm",
      "Junier Oliva",
      "Roni Sengupta"
    ],
    "published": "2025-05-09T17:54:10+00:00",
    "summary": "Next Best View (NBV) algorithms aim to acquire an optimal set of images using minimal resources, time, or number of captures to enable efficient 3D reconstruction of a scene. Existing approaches often rely on prior scene knowledge or additional image captures and often develop policies that maximize coverage. Yet, for many real scenes with complex geometry and self-occlusions, coverage maximization does not lead to better reconstruction quality directly. In this paper, we propose the View Introspection Network (VIN), which is trained to predict the reconstruction quality improvement of views directly, and the VIN-NBV policy. A greedy sequential sampling-based policy, where at each acquisition step, we sample multiple query views and choose the one with the highest VIN predicted improvement score. We design the VIN to perform 3D-aware featurization of the reconstruction built from prior acquisitions, and for each query view create a feature that can be decoded into an improvement score. We then train the VIN using imitation learning to predict the reconstruction improvement score. We show that VIN-NBV improves reconstruction quality by ~30% over a coverage maximization baseline when operating with constraints on the number of acquisitions or the time in motion."
  },
  {
    "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails",
    "url": "http://arxiv.org/abs/2505.06218v1",
    "arxiv_id": "2505.06218v1",
    "authors": [
      "Kwan-Yee Lin",
      "Stella X. Yu"
    ],
    "published": "2025-05-09T17:53:02+00:00",
    "summary": "Hiking on complex trails demands balance, agility, and adaptive decision-making over unpredictable terrain. Current humanoid research remains fragmented and inadequate for hiking: locomotion focuses on motor skills without long-term goals or situational awareness, while semantic navigation overlooks real-world embodiment and local terrain variability. We propose training humanoids to hike on complex trails, driving integrative skill development across visual perception, decision making, and motor execution. We develop a learning framework, LEGO-H, that enables a vision-equipped humanoid robot to hike complex trails autonomously. We introduce two technical innovations: 1) A temporal vision transformer variant - tailored into Hierarchical Reinforcement Learning framework - anticipates future local goals to guide movement, seamlessly integrating locomotion with goal-directed navigation. 2) Latent representations of joint movement patterns, combined with hierarchical metric learning - enhance Privileged Learning scheme - enable smooth policy transfer from privileged training to onboard execution. These components allow LEGO-H to handle diverse physical and environmental challenges without relying on predefined motion patterns. Experiments across varied simulated trails and robot morphologies highlight LEGO-H's versatility and robustness, positioning hiking as a compelling testbed for embodied autonomy and LEGO-H as a baseline for future humanoid development."
  },
  {
    "title": "Neuro-Symbolic Concepts",
    "url": "http://arxiv.org/abs/2505.06191v1",
    "arxiv_id": "2505.06191v1",
    "authors": [
      "Jiayuan Mao",
      "Joshua B. Tenenbaum",
      "Jiajun Wu"
    ],
    "published": "2025-05-09T17:02:51+00:00",
    "summary": "This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented using a combination of symbolic programs and neural network representations. Leveraging such neuro-symbolic concepts, the agent can efficiently learn and recombine them to solve various tasks across different domains, ranging from 2D images, videos, 3D scenes, and robotic manipulation tasks. This concept-centric framework offers several advantages, including data efficiency, compositional generalization, continual learning, and zero-shot transfer."
  },
  {
    "title": "Active Perception for Tactile Sensing: A Task-Agnostic Attention-Based Approach",
    "url": "http://arxiv.org/abs/2505.06182v1",
    "arxiv_id": "2505.06182v1",
    "authors": [
      "Tim Schneider",
      "Cristiana de Farias",
      "Roberto Calandra",
      "Liming Chen",
      "Jan Peters"
    ],
    "published": "2025-05-09T16:49:26+00:00",
    "summary": "Humans make extensive use of haptic exploration to map and identify the properties of the objects that we touch. In robotics, active tactile perception has emerged as an important research domain that complements vision for tasks such as object classification, shape reconstruction, and manipulation. This work introduces TAP (Task-agnostic Active Perception) -- a novel framework that leverages reinforcement learning (RL) and transformer-based architectures to address the challenges posed by partially observable environments. TAP integrates Soft Actor-Critic (SAC) and CrossQ algorithms within a unified optimization objective, jointly training a perception module and decision-making policy. By design, TAP is completely task-agnostic and can, in principle, generalize to any active perception problem. We evaluate TAP across diverse tasks, including toy examples and realistic applications involving haptic exploration of 3D models from the Tactile MNIST benchmark. Experiments demonstrate the efficacy of TAP, achieving high accuracies on the Tactile MNIST haptic digit recognition task and a tactile pose estimation task. These findings underscore the potential of TAP as a versatile and generalizable framework for advancing active tactile perception in robotics."
  },
  {
    "title": "3D Scene Generation: A Survey",
    "url": "http://arxiv.org/abs/2505.05474v1",
    "arxiv_id": "2505.05474v1",
    "authors": [
      "Beichen Wen",
      "Haozhe Xie",
      "Zhaoxi Chen",
      "Fangzhou Hong",
      "Ziwei Liu"
    ],
    "published": "2025-05-08T17:59:54+00:00",
    "summary": "3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation."
  },
  {
    "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
    "url": "http://arxiv.org/abs/2505.05470v1",
    "arxiv_id": "2505.05470v1",
    "authors": [
      "Jie Liu",
      "Gongye Liu",
      "Jiajun Liang",
      "Yangguang Li",
      "Jiaheng Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Wanli Ouyang"
    ],
    "published": "2025-05-08T17:58:45+00:00",
    "summary": "We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy improves from $59\\%$ to $92\\%$, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments."
  },
  {
    "title": "Generating Physically Stable and Buildable LEGO Designs from Text",
    "url": "http://arxiv.org/abs/2505.05469v1",
    "arxiv_id": "2505.05469v1",
    "authors": [
      "Ava Pun",
      "Kangle Deng",
      "Ruixuan Liu",
      "Deva Ramanan",
      "Changliu Liu",
      "Jun-Yan Zhu"
    ],
    "published": "2025-05-08T17:58:18+00:00",
    "summary": "We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/."
  },
  {
    "title": "SITE: towards Spatial Intelligence Thorough Evaluation",
    "url": "http://arxiv.org/abs/2505.05456v1",
    "arxiv_id": "2505.05456v1",
    "authors": [
      "Wenqi Wang",
      "Reuben Tan",
      "Pengyue Zhu",
      "Jianwei Yang",
      "Zhengyuan Yang",
      "Lijuan Wang",
      "Andrey Kolobov",
      "Jianfeng Gao",
      "Boqing Gong"
    ],
    "published": "2025-05-08T17:45:44+00:00",
    "summary": "Spatial intelligence (SI) represents a cognitive ability encompassing the visualization, manipulation, and reasoning about spatial relationships, underpinning disciplines from neuroscience to robotics. We introduce SITE, a benchmark dataset towards SI Thorough Evaluation in a standardized format of multi-choice visual question-answering, designed to assess large vision-language models' spatial intelligence across diverse visual modalities (single-image, multi-image, and video) and SI factors (figural to environmental scales, spatial visualization and orientation, intrinsic and extrinsic, static and dynamic). Our approach to curating the benchmark combines a bottom-up survey about 31 existing datasets and a top-down strategy drawing upon three classification systems in cognitive science, which prompt us to design two novel types of tasks about view-taking and dynamic scenes. Extensive experiments reveal that leading models fall behind human experts especially in spatial orientation, a fundamental SI factor. Moreover, we demonstrate a positive correlation between a model's spatial reasoning proficiency and its performance on an embodied AI task."
  },
  {
    "title": "RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles",
    "url": "http://arxiv.org/abs/2505.05452v1",
    "arxiv_id": "2505.05452v1",
    "authors": [
      "Pouria Behnoudfar",
      "Nan Chen"
    ],
    "published": "2025-05-08T17:43:35+00:00",
    "summary": "Machine learning has become a powerful tool for enhancing data assimilation. While supervised learning remains the standard method, reinforcement learning (RL) offers unique advantages through its sequential decision-making framework, which naturally fits the iterative nature of data assimilation by dynamically balancing model forecasts with observations. We develop RL-DAUNCE, a new RL-based method that enhances data assimilation with physical constraints through three key aspects. First, RL-DAUNCE inherits the computational efficiency of machine learning while it uniquely structures its agents to mirror ensemble members in conventional data assimilation methods. Second, RL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble members, moving beyond simple mean-state optimization. Third, RL-DAUNCE's ensemble-as-agents design facilitates the enforcement of physical constraints during the assimilation process, which is crucial to improving the state estimation and subsequent forecasting. A primal-dual optimization strategy is developed to enforce constraints, which dynamically penalizes the reward function to ensure constraint satisfaction throughout the learning process. Also, state variable bounds are respected by constraining the RL action space. Together, these features ensure physical consistency without sacrificing efficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an intermittent atmospheric phenomenon characterized by strongly non-Gaussian features and multiple physical constraints. RL-DAUNCE outperforms the standard ensemble Kalman filter (EnKF), which fails catastrophically due to the violation of physical constraints. Notably, RL-DAUNCE matches the performance of constrained EnKF, particularly in recovering intermittent signals, capturing extreme events, and quantifying uncertainties, while requiring substantially less computational effort."
  },
  {
    "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.04623v1",
    "arxiv_id": "2505.04623v1",
    "authors": [
      "Zhenghao Xing",
      "Xiaowei Hu",
      "Chi-Wing Fu",
      "Wenhai Wang",
      "Jifeng Dai",
      "Pheng-Ann Heng"
    ],
    "published": "2025-05-07T17:59:49+00:00",
    "summary": "Multimodal large language models (MLLMs) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. We introduce EchoInk-R1, a reinforcement learning framework that enhances such reasoning in MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice question answering over synchronized audio-image pairs. To enable this, we curate AVQA-R1-6K, a dataset pairing such audio-image inputs with multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy, EchoInk-R1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. These results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. Code and data are publicly released to facilitate further research."
  },
  {
    "title": "Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation",
    "url": "http://arxiv.org/abs/2505.04619v1",
    "arxiv_id": "2505.04619v1",
    "authors": [
      "Abdulaziz Almuzairee",
      "Rohan Patil",
      "Dwait Bhatt",
      "Henrik I. Christensen"
    ],
    "published": "2025-05-07T17:59:28+00:00",
    "summary": "Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad"
  },
  {
    "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
    "url": "http://arxiv.org/abs/2505.04588v1",
    "arxiv_id": "2505.04588v1",
    "authors": [
      "Hao Sun",
      "Zile Qiao",
      "Jiayan Guo",
      "Xuanbo Fan",
      "Yingyan Hou",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Yan Zhang"
    ],
    "published": "2025-05-07T17:30:22+00:00",
    "summary": "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms."
  },
  {
    "title": "Active Sampling for MRI-based Sequential Decision Making",
    "url": "http://arxiv.org/abs/2505.04586v1",
    "arxiv_id": "2505.04586v1",
    "authors": [
      "Yuning Du",
      "Jingshuai Liu",
      "Rohan Dharmakumar",
      "Sotirios A. Tsaftaris"
    ],
    "published": "2025-05-07T17:27:51+00:00",
    "summary": "Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at https://github.com/vios-s/MRI_Sequential_Active_Sampling"
  },
  {
    "title": "Modeling Personalized Difficulty of Rehabilitation Exercises Using Causal Trees",
    "url": "http://arxiv.org/abs/2505.04583v1",
    "arxiv_id": "2505.04583v1",
    "authors": [
      "Nathaniel Dennler",
      "Zhonghao Shi",
      "Uksang Yoo",
      "Stefanos Nikolaidis",
      "Maja Matari\u0107"
    ],
    "published": "2025-05-07T17:21:45+00:00",
    "summary": "Rehabilitation robots are often used in game-like interactions for rehabilitation to increase a person's motivation to complete rehabilitation exercises. By adjusting exercise difficulty for a specific user throughout the exercise interaction, robots can maximize both the user's rehabilitation outcomes and the their motivation throughout the exercise. Previous approaches have assumed exercises have generic difficulty values that apply to all users equally, however, we identified that stroke survivors have varied and unique perceptions of exercise difficulty. For example, some stroke survivors found reaching vertically more difficult than reaching farther but lower while others found reaching farther more challenging than reaching vertically. In this paper, we formulate a causal tree-based method to calculate exercise difficulty based on the user's performance. We find that this approach accurately models exercise difficulty and provides a readily interpretable model of why that exercise is difficult for both users and caretakers."
  },
  {
    "title": "AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control",
    "url": "http://arxiv.org/abs/2505.03738v1",
    "arxiv_id": "2505.03738v1",
    "authors": [
      "Jialong Li",
      "Xuxin Cheng",
      "Tianshu Huang",
      "Shiqi Yang",
      "Ri-Zhao Qiu",
      "Xiaolong Wang"
    ],
    "published": "2025-05-06T17:59:51+00:00",
    "summary": "Humanoid robots derive much of their dexterity from hyper-dexterous whole-body movements, enabling tasks that require a large operational workspace: such as picking objects off the ground. However, achieving these capabilities on real humanoids remains challenging due to their high degrees of freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization (AMO), a framework that integrates sim-to-real reinforcement learning (RL) with trajectory optimization for real-time, adaptive whole-body control. To mitigate distribution bias in motion imitation RL, we construct a hybrid AMO dataset and train a network capable of robust, on-demand adaptation to potentially O.O.D. commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid robot, demonstrating superior stability and an expanded workspace compared to strong baselines. Finally, we show that AMO's consistent performance supports autonomous task execution via imitation learning, underscoring the system's versatility and robustness."
  },
  {
    "title": "Visual Imitation Enables Contextual Humanoid Control",
    "url": "http://arxiv.org/abs/2505.03729v1",
    "arxiv_id": "2505.03729v1",
    "authors": [
      "Arthur Allshire",
      "Hongsuk Choi",
      "Junyi Zhang",
      "David McAllister",
      "Anthony Zhang",
      "Chung Min Kim",
      "Trevor Darrell",
      "Pieter Abbeel",
      "Jitendra Malik",
      "Angjoo Kanazawa"
    ],
    "published": "2025-05-06T17:57:12+00:00",
    "summary": "How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments."
  },
  {
    "title": "PyRoki: A Modular Toolkit for Robot Kinematic Optimization",
    "url": "http://arxiv.org/abs/2505.03728v1",
    "arxiv_id": "2505.03728v1",
    "authors": [
      "Chung Min Kim",
      "Brent Yi",
      "Hongsuk Choi",
      "Yi Ma",
      "Ken Goldberg",
      "Angjoo Kanazawa"
    ],
    "published": "2025-05-06T17:56:40+00:00",
    "summary": "Robot motion can have many goals. Depending on the task, we might optimize for pose error, speed, collision, or similarity to a human demonstration. Motivated by this, we present PyRoki: a modular, extensible, and cross-platform toolkit for solving kinematic optimization problems. PyRoki couples an interface for specifying kinematic variables and costs with an efficient nonlinear least squares optimizer. Unlike existing tools, it is also cross-platform: optimization runs natively on CPU, GPU, and TPU. In this paper, we present (i) the design and implementation of PyRoki, (ii) motion retargeting and planning case studies that highlight the advantages of PyRoki's modularity, and (iii) optimization benchmarking, where PyRoki can be 1.4-1.7x faster and converges to lower errors than cuRobo, an existing GPU-accelerated inverse kinematics library."
  },
  {
    "title": "Meta-Optimization and Program Search using Language Models for Task and Motion Planning",
    "url": "http://arxiv.org/abs/2505.03725v1",
    "arxiv_id": "2505.03725v1",
    "authors": [
      "Denis Shcherba",
      "Eckart Cobo-Briesewitz",
      "Cornelius V. Braun",
      "Marc Toussaint"
    ],
    "published": "2025-05-06T17:53:14+00:00",
    "summary": "Intelligent interaction with the real world requires robotic agents to jointly reason over high-level plans and low-level controls. Task and motion planning (TAMP) addresses this by combining symbolic planning and continuous trajectory generation. Recently, foundation model approaches to TAMP have presented impressive results, including fast planning times and the execution of natural language instructions. Yet, the optimal interface between high-level planning and low-level motion generation remains an open question: prior approaches are limited by either too much abstraction (e.g., chaining simplified skill primitives) or a lack thereof (e.g., direct joint angle prediction). Our method introduces a novel technique employing a form of meta-optimization to address these issues by: (i) using program search over trajectory optimization problems as an interface between a foundation model and robot control, and (ii) leveraging a zero-order method to optimize numerical parameters in the foundation model output. Results on challenging object manipulation and drawing tasks confirm that our proposed method improves over prior TAMP approaches."
  },
  {
    "title": "Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.03721v1",
    "arxiv_id": "2505.03721v1",
    "authors": [
      "Dian Chen",
      "Zelin Wan",
      "Dong Sam Ha",
      "Jin-Hee Cho"
    ],
    "published": "2025-05-06T17:49:06+00:00",
    "summary": "Solar sensor-based monitoring systems have become a crucial agricultural innovation, advancing farm management and animal welfare through integrating sensor technology, Internet-of-Things, and edge and cloud computing. However, the resilience of these systems to cyber-attacks and their adaptability to dynamic and constrained energy supplies remain largely unexplored. To address these challenges, we propose a sustainable smart farm network designed to maintain high-quality animal monitoring under various cyber and adversarial threats, as well as fluctuating energy conditions. Our approach utilizes deep reinforcement learning (DRL) to devise optimal policies that maximize both monitoring effectiveness and energy efficiency. To overcome DRL's inherent challenge of slow convergence, we integrate transfer learning (TL) and decision theory (DT) to accelerate the learning process. By incorporating DT-guided strategies, we optimize monitoring quality and energy sustainability, significantly reducing training time while achieving comparable performance rewards. Our experimental results prove that DT-guided DRL outperforms TL-enhanced DRL models, improving system performance and reducing training runtime by 47.5%."
  },
  {
    "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.02835v1",
    "arxiv_id": "2505.02835v1",
    "authors": [
      "Yi-Fan Zhang",
      "Xingyu Lu",
      "Xiao Hu",
      "Chaoyou Fu",
      "Bin Wen",
      "Tianke Zhang",
      "Changyi Liu",
      "Kaiyu Jiang",
      "Kaibing Chen",
      "Kaiyu Tang",
      "Haojie Ding",
      "Jiankang Chen",
      "Fan Yang",
      "Zhang Zhang",
      "Tingting Gao",
      "Liang Wang"
    ],
    "published": "2025-05-05T17:59:50+00:00",
    "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs."
  },
  {
    "title": "TWIST: Teleoperated Whole-Body Imitation System",
    "url": "http://arxiv.org/abs/2505.02833v1",
    "arxiv_id": "2505.02833v1",
    "authors": [
      "Yanjie Ze",
      "Zixuan Chen",
      "Jo\u00e3o Pedro Ara\u00fajo",
      "Zi-ang Cao",
      "Xue Bin Peng",
      "Jiajun Wu",
      "C. Karen Liu"
    ],
    "published": "2025-05-05T17:59:03+00:00",
    "summary": "Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. Our project website: https://humanoid-teleop.github.io"
  },
  {
    "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing",
    "url": "http://arxiv.org/abs/2505.02811v1",
    "arxiv_id": "2505.02811v1",
    "authors": [
      "Diji Yang",
      "Linda Zeng",
      "Jinmeng Rao",
      "Yi Zhang"
    ],
    "published": "2025-05-05T17:39:35+00:00",
    "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.   This paper aims to address these limitations by introducing a new framework, \\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.   Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data."
  },
  {
    "title": "Stabilizing dark matter with quantum scale symmetry",
    "url": "http://arxiv.org/abs/2505.02803v1",
    "arxiv_id": "2505.02803v1",
    "authors": [
      "Abhishek Chikkaballi",
      "Kamila Kowalska",
      "Rafael R. Lino dos Santos",
      "Enrico Maria Sessolo"
    ],
    "published": "2025-05-05T17:27:04+00:00",
    "summary": "In the context of gauge-Yukawa theories with trans-Planckian asymptotic safety, quantum scale symmetry can prevent the appearance in the Lagrangian of couplings that would otherwise be allowed by the gauge symmetry. Such couplings correspond to irrelevant Gaussian fixed points of the renormalization group flow. Their absence in the theory implies that different sectors of the gauge-Yukawa theory are secluded from one another, in similar fashion to the effects of a global or a discrete symmetry. As an example, we impose the trans-Planckian scale symmetry on a model of Grand Unification based on the gauge group SU(6), showing that it leads to the emergence of several fermionic WIMP dark matter candidates whose coupling strengths are entirely predicted by the UV completion."
  },
  {
    "title": "Teaching the social media generation: rethinking learning without sacrificing quality",
    "url": "http://arxiv.org/abs/2505.02770v1",
    "arxiv_id": "2505.02770v1",
    "authors": [
      "Sepinoud Azimi"
    ],
    "published": "2025-05-05T16:31:10+00:00",
    "summary": "The rise of social media and AI tools has reshaped how students engage with learning, process information, and build trust in educational content. This generation prefers short, visual materials and fast feedback but often struggles with focus, critical thinking, and deep learning. Educators face the challenge of adapting teaching methods to these habits without lowering academic standards. This study presents a blended learning redesign of a first-year technical course at a Dutch university. Key features included short whiteboard videos before class, hands-on teamwork during class, narrative-style handouts to reinforce learning, in-class draft assignments without AI, and weekly anonymous feedback to adjust in real time. The results were promising: attendance increased by nearly 50%, and none of the regularly attending students failed the exam. Students found the videos useful but emphasized that in-person sessions were essential for understanding the material. While some resisted the shift in expectations, most appreciated the structure, clarity, and opportunities for active learning. This case suggests that combining digital familiarity with clear expectations and active support can help meet students where they are, while still challenging them to grow."
  },
  {
    "title": "GENMO: A GENeralist Model for Human MOtion",
    "url": "http://arxiv.org/abs/2505.01425v1",
    "arxiv_id": "2505.01425v1",
    "authors": [
      "Jiefeng Li",
      "Jinkun Cao",
      "Haotian Zhang",
      "Davis Rempe",
      "Jan Kautz",
      "Umar Iqbal",
      "Ye Yuan"
    ],
    "published": "2025-05-02T17:59:55+00:00",
    "summary": "Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model."
  },
  {
    "title": "Neutrino mass generation in asymptotically safe gravity",
    "url": "http://arxiv.org/abs/2505.01422v1",
    "arxiv_id": "2505.01422v1",
    "authors": [
      "Gustavo P. de Brito",
      "Astrid Eichhorn",
      "Antonio D. Pereira",
      "Masatoshi Yamada"
    ],
    "published": "2025-05-02T17:58:14+00:00",
    "summary": "There exist several distinct phenomenological models to generate neutrino masses. We explore, which of these models can consistently be embedded in a quantum theory of gravity and matter. We proceed by invoking a minimal number of degrees of freedom beyond the Standard Model. Thus, we first investigate whether the Weinberg operator, a dimension-five-operator that generates neutrino masses without requiring degrees of freedom beyond the Standard Model, can arise in asymptotically safe quantum gravity. We find a negative answer with far-reaching consequences: new degrees of freedom beyond gravity and the Standard Model are necessary to give neutrinos a mass in the asymptotic-safety paradigm. Second, we explore whether the type-I Seesaw mechanism is viable and discover an upper bound on the Seesaw scale. The bound depends on the mass of the visible neutrino. We find a numerical value of $10^{14}\\, \\rm GeV$ for this bound when neglecting neutrino mixing for a visible mass of $10^{-10}\\, \\rm GeV$. Conversely, for the most ``natural\" value of the Seesaw scale in a quantum-gravity setting, which is the Planck scale, we predict an upper bound for the neutrino mass of the visible neutrino of approximately $10^{-15}\\, \\rm GeV$. Third, we explore whether neutrinos could also be Pseudo-Dirac-neutrinos in asymptotic safety and find that this possibility can be accommodated."
  },
  {
    "title": "Evaluating Frontier Models for Stealth and Situational Awareness",
    "url": "http://arxiv.org/abs/2505.01420v1",
    "arxiv_id": "2505.01420v1",
    "authors": [
      "Mary Phuong",
      "Roland S. Zimmermann",
      "Ziyue Wang",
      "David Lindner",
      "Victoria Krakovna",
      "Sarah Cogan",
      "Allan Dafoe",
      "Lewis Ho",
      "Rohin Shah"
    ],
    "published": "2025-05-02T17:57:14+00:00",
    "summary": "Recent work has demonstrated the plausibility of frontier AI models scheming -- knowingly and covertly pursuing an objective misaligned with its developer's intentions. Such behavior could be very hard to detect, and if present in future advanced systems, could pose severe loss of control risk. It is therefore important for AI developers to rule out harm from scheming prior to model deployment. In this paper, we present a suite of scheming reasoning evaluations measuring two types of reasoning capabilities that we believe are prerequisites for successful scheming: First, we propose five evaluations of ability to reason about and circumvent oversight (stealth). Second, we present eleven evaluations for measuring a model's ability to instrumentally reason about itself, its environment and its deployment (situational awareness). We demonstrate how these evaluations can be used as part of a scheming inability safety case: a model that does not succeed on these evaluations is almost certainly incapable of causing severe harm via scheming in real deployment. We run our evaluations on current frontier models and find that none of them show concerning levels of either situational awareness or stealth."
  },
  {
    "title": "Propagation and circulating modes of reciprocal non-Hermitian skin effect",
    "url": "http://arxiv.org/abs/2505.01417v1",
    "arxiv_id": "2505.01417v1",
    "authors": [
      "Issei Takeda",
      "Taiki Yoda",
      "Yuto Moritake",
      "Kenta Takata",
      "Masaya Notomi"
    ],
    "published": "2025-05-02T17:52:36+00:00",
    "summary": "The non-Hermitian skin effect (NHSE) is a novel localization phenomenon, in which all bulk states in a non-Hermitian system under certain conditions are localized at the edge of the system. Conventionally, most studies of NHSE have dealt with discrete lattice systems with non-reciprocal couplings. However in recent years, NHSE in a reciprocal two-dimensional continuous medium, such as photonic crystal systems, has also been reported. In particular, we have previously shown that NHSE also occurs in two-dimensional uniform media. In such two-dimensional systems, skin modes propagate in a direction perpendicular to the localization direction, and especially, they have the property of propagating in only one direction. In this paper, we show numerically an intriguing scattering phenomenon: when a scatterer is placed in the path of a skin mode, the scattering causes the skin mode to hop between opposing edges. In addition, we propose a new method of generating circulating modes with orbital angular momentum using this scattering phenomenon. Our work paves the way for new applications of NHSE as micro-sized optical devices manipulating or generating OAM."
  },
  {
    "title": "Collection of fluorescence from an ion using trap-integrated photonics",
    "url": "http://arxiv.org/abs/2505.01412v1",
    "arxiv_id": "2505.01412v1",
    "authors": [
      "Felix W. Knollmann",
      "Sabrina M. Corsetti",
      "Ethan R. Clements",
      "Reuel Swint",
      "Aaron D. Leu",
      "May E. Kim",
      "Patrick T. Callahan",
      "Dave Kharas",
      "Thomas Mahony",
      "Cheryl Sorace-Agaskar",
      "Robert McConnell",
      "Colin D. Bruzewicz",
      "Isaac L. Chuang",
      "Jelena Notaros",
      "John Chiaverini"
    ],
    "published": "2025-05-02T17:42:20+00:00",
    "summary": "Spontaneously emitted photons are entangled with the electronic and nuclear degrees of freedom of the emitting atom, so interference and measurement of these photons can entangle separate matter-based quantum systems as a resource for quantum information processing. However, the isotropic nature of spontaneous emission hinders the single-mode photonic operations required to generate entanglement. Current demonstrations rely on bulk photon-collection and manipulation optics that suffer from environment-induced phase instability, mode matching challenges, and system-to-system variability, factors that impede scaling to the large numbers of entangled pairs needed for quantum information processing. To address these limitations, we demonstrate a collection method that enables passive phase stability, straightforward photonic manipulation, and intrinsic reproducibility. Specifically, we engineer a waveguide-integrated grating to couple photons emitted from a trapped ion into a single optical mode within a microfabricated ion-trap chip. Using the integrated collection optic, we characterize the collection efficiency, image the ion, and detect the ion's quantum state. This proof-of-principle demonstration lays the foundation for leveraging the inherent stability and reproducibility of integrated photonics to efficiently create, manipulate, and measure multipartite quantum states in arrays of quantum emitters."
  },
  {
    "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT",
    "url": "http://arxiv.org/abs/2505.00703v1",
    "arxiv_id": "2505.00703v1",
    "authors": [
      "Dongzhi Jiang",
      "Ziyu Guo",
      "Renrui Zhang",
      "Zhuofan Zong",
      "Hao Li",
      "Le Zhuo",
      "Shilin Yan",
      "Pheng-Ann Heng",
      "Hongsheng Li"
    ],
    "published": "2025-05-01T17:59:46+00:00",
    "summary": "Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1"
  },
  {
    "title": "Robotic Visual Instruction",
    "url": "http://arxiv.org/abs/2505.00693v1",
    "arxiv_id": "2505.00693v1",
    "authors": [
      "Yanbang Li",
      "Ziyang Gong",
      "Haoyang Li",
      "Haoyang Li",
      "Xiaoqi Huang",
      "Haolan Kang",
      "Guangping Bai",
      "Xianzheng Ma"
    ],
    "published": "2025-05-01T17:55:05+00:00",
    "summary": "Recently, natural language has been the primary medium for human-robot interaction. However, its inherent lack of spatial precision for robotic control introduces challenges such as ambiguity and verbosity. To address these limitations, we introduce the Robotic Visual Instruction (RoVI), a novel paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic representation. RoVI effectively encodes spatial-temporal information into human-interpretable visual instructions through 2D sketches, utilizing arrows, circles, colors, and numbers to direct 3D robotic manipulation. To enable robots to understand RoVI better and generate precise actions based on RoVI, we present Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for RoVI-conditioned policies. This approach leverages Vision-Language Models (VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from 2D pixel space via keypoint extraction, and then transform them into executable 3D action sequences. We additionally curate a specialized dataset of 15K instances to fine-tune small VLMs for edge deployment, enabling them to effectively learn RoVI capabilities. Our approach is rigorously validated across 11 novel tasks in both real and simulated environments, demonstrating significant generalization capability. Notably, VIEW achieves an 87.5% success rate in real-world scenarios involving unseen tasks that feature multi-step actions, with disturbances, and trajectory-following requirements. Code and Datasets in this paper will be released soon."
  },
  {
    "title": "Towards Autonomous Micromobility through Scalable Urban Simulation",
    "url": "http://arxiv.org/abs/2505.00690v1",
    "arxiv_id": "2505.00690v1",
    "authors": [
      "Wayne Wu",
      "Honglin He",
      "Chaoyuan Zhang",
      "Jack He",
      "Seth Z. Zhao",
      "Ran Gong",
      "Quanyi Li",
      "Bolei Zhou"
    ],
    "published": "2025-05-01T17:52:29+00:00",
    "summary": "Micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. Current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility. First, we build URBAN-SIM - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. URBAN-SIM contains three critical modules: Hierarchical Urban Generation pipeline, Interactive Dynamics Generation strategy, and Asynchronous Scene Sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. Then, we propose URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various capabilities of the AI agents in achieving autonomous micromobility. URBAN-BENCH includes eight tasks based on three core skills of the agents: Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. Experiments on diverse terrains and urban structures reveal each robot's strengths and limitations."
  },
  {
    "title": "Multi-Constraint Safe Reinforcement Learning via Closed-form Solution for Log-Sum-Exp Approximation of Control Barrier Functions",
    "url": "http://arxiv.org/abs/2505.00671v1",
    "arxiv_id": "2505.00671v1",
    "authors": [
      "Chenggang Wang",
      "Xinyi Wang",
      "Yutong Dong",
      "Lei Song",
      "Xinping Guan"
    ],
    "published": "2025-05-01T17:22:11+00:00",
    "summary": "The safety of training task policies and their subsequent application using reinforcement learning (RL) methods has become a focal point in the field of safe RL. A central challenge in this area remains the establishment of theoretical guarantees for safety during both the learning and deployment processes. Given the successful implementation of Control Barrier Function (CBF)-based safety strategies in a range of control-affine robotic systems, CBF-based safe RL demonstrates significant promise for practical applications in real-world scenarios. However, integrating these two approaches presents several challenges. First, embedding safety optimization within the RL training pipeline requires that the optimization outputs be differentiable with respect to the input parameters, a condition commonly referred to as differentiable optimization, which is non-trivial to solve. Second, the differentiable optimization framework confronts significant efficiency issues, especially when dealing with multi-constraint problems. To address these challenges, this paper presents a CBF-based safe RL architecture that effectively mitigates the issues outlined above. The proposed approach constructs a continuous AND logic approximation for the multiple constraints using a single composite CBF. By leveraging this approximation, a close-form solution of the quadratic programming is derived for the policy network in RL, thereby circumventing the need for differentiable optimization within the end-to-end safe RL pipeline. This strategy significantly reduces computational complexity because of the closed-form solution while maintaining safety guarantees. Simulation results demonstrate that, in comparison to existing approaches relying on differentiable optimization, the proposed method significantly reduces training computational costs while ensuring provable safety throughout the training process."
  },
  {
    "title": "Deep Reinforcement Learning for Urban Air Quality Management: Multi-Objective Optimization of Pollution Mitigation Booth Placement in Metropolitan Environments",
    "url": "http://arxiv.org/abs/2505.00668v1",
    "arxiv_id": "2505.00668v1",
    "authors": [
      "Kirtan Rajesh",
      "Suvidha Rupesh Kumar"
    ],
    "published": "2025-05-01T17:19:48+00:00",
    "summary": "Urban air pollution remains a pressing global concern, particularly in densely populated and traffic-intensive metropolitan areas like Delhi, where exposure to harmful pollutants severely impacts public health. Delhi, being one of the most polluted cities globally, experiences chronic air quality issues due to vehicular emissions, industrial activities, and construction dust, which exacerbate its already fragile atmospheric conditions. Traditional pollution mitigation strategies, such as static air purifying installations, often fail to maximize their impact due to suboptimal placement and limited adaptability to dynamic urban environments. This study presents a novel deep reinforcement learning (DRL) framework to optimize the placement of air purification booths to improve the air quality index (AQI) in the city of Delhi. We employ Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm, to iteratively learn and identify high-impact locations based on multiple spatial and environmental factors, including population density, traffic patterns, industrial influence, and green space constraints. Our approach is benchmarked against conventional placement strategies, including random and greedy AQI-based methods, using multi-dimensional performance evaluation metrics such as AQI improvement, spatial coverage, population and traffic impact, and spatial entropy. Experimental results demonstrate that the RL-based approach outperforms baseline methods by achieving a balanced and effective distribution of air purification infrastructure. Notably, the DRL framework achieves an optimal trade-off between AQI reduction and high-coverage deployment, ensuring equitable environmental benefits across urban regions. The findings underscore the potential of AI-driven spatial optimization in advancing smart city initiatives and data-driven urban air quality management."
  },
  {
    "title": "A Survey of Interactive Generative Video",
    "url": "http://arxiv.org/abs/2504.21853v1",
    "arxiv_id": "2504.21853v1",
    "authors": [
      "Jiwen Yu",
      "Yiran Qin",
      "Haoxuan Che",
      "Quande Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai",
      "Hao Chen",
      "Xihui Liu"
    ],
    "published": "2025-04-30T17:59:02+00:00",
    "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications."
  },
  {
    "title": "Active Light Modulation to Counter Manipulation of Speech Visual Content",
    "url": "http://arxiv.org/abs/2504.21846v1",
    "arxiv_id": "2504.21846v1",
    "authors": [
      "Hadleigh Schwartz",
      "Xiaofeng Yan",
      "Charles J. Carver",
      "Xia Zhou"
    ],
    "published": "2025-04-30T17:55:24+00:00",
    "summary": "High-profile speech videos are prime targets for falsification, owing to their accessibility and influence. This work proposes Spotlight, a low-overhead and unobtrusive system for protecting live speech videos from visual falsification of speaker identity and lip and facial motion. Unlike predominant falsification detection methods operating in the digital domain, Spotlight creates dynamic physical signatures at the event site and embeds them into all video recordings via imperceptible modulated light. These physical signatures encode semantically-meaningful features unique to the speech event, including the speaker's identity and facial motion, and are cryptographically-secured to prevent spoofing. The signatures can be extracted from any video downstream and validated against the portrayed speech content to check its integrity. Key elements of Spotlight include (1) a framework for generating extremely compact (i.e., 150-bit), pose-invariant speech video features, based on locality-sensitive hashing; and (2) an optical modulation scheme that embeds >200 bps into video while remaining imperceptible both in video and live. Prototype experiments on extensive video datasets show Spotlight achieves AUCs $\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified videos. Further, Spotlight is highly robust across recording conditions, video post-processing techniques, and white-box adversarial attacks on its video feature extraction methodologies."
  },
  {
    "title": "Neuro-Symbolic Generation of Explanations for Robot Policies with Weighted Signal Temporal Logic",
    "url": "http://arxiv.org/abs/2504.21841v1",
    "arxiv_id": "2504.21841v1",
    "authors": [
      "Mikihisa Yuasa",
      "Ramavarapu S. Sreenivas",
      "Huy T. Tran"
    ],
    "published": "2025-04-30T17:51:20+00:00",
    "summary": "Neural network-based policies have demonstrated success in many robotic applications, but often lack human-explanability, which poses challenges in safety-critical deployments. To address this, we propose a neuro-symbolic explanation framework that generates a weighted signal temporal logic (wSTL) specification to describe a robot policy in a interpretable form. Existing methods typically produce explanations that are verbose and inconsistent, which hinders explainability, and loose, which do not give meaningful insights into the underlying policy. We address these issues by introducing a simplification process consisting of predicate filtering, regularization, and iterative pruning. We also introduce three novel explainability evaluation metrics -- conciseness, consistency, and strictness -- to assess explanation quality beyond conventional classification metrics. Our method is validated in three simulated robotic environments, where it outperforms baselines in generating concise, consistent, and strict wSTL explanations without sacrificing classification accuracy. This work bridges policy learning with formal methods, contributing to safer and more transparent decision-making in robotics."
  },
  {
    "title": "An Underwater, Fault-Tolerant, Laser-Aided Robotic Multi-Modal Dense SLAM System for Continuous Underwater In-Situ Observation",
    "url": "http://arxiv.org/abs/2504.21826v1",
    "arxiv_id": "2504.21826v1",
    "authors": [
      "Yaming Ou",
      "Junfeng Fan",
      "Chao Zhou",
      "Pengju Zhang",
      "Zongyuan Shen",
      "Yichen Fu",
      "Xiaoyan Liu",
      "Zengguang Hou"
    ],
    "published": "2025-04-30T17:30:13+00:00",
    "summary": "Existing underwater SLAM systems are difficult to work effectively in texture-sparse and geometrically degraded underwater environments, resulting in intermittent tracking and sparse mapping. Therefore, we present Water-DSLAM, a novel laser-aided multi-sensor fusion system that can achieve uninterrupted, fault-tolerant dense SLAM capable of continuous in-situ observation in diverse complex underwater scenarios through three key innovations: Firstly, we develop Water-Scanner, a multi-sensor fusion robotic platform featuring a self-designed Underwater Binocular Structured Light (UBSL) module that enables high-precision 3D perception. Secondly, we propose a fault-tolerant triple-subsystem architecture combining: 1) DP-INS (DVL- and Pressure-aided Inertial Navigation System): fusing inertial measurement unit, doppler velocity log, and pressure sensor based Error-State Kalman Filter (ESKF) to provide high-frequency absolute odometry 2) Water-UBSL: a novel Iterated ESKF (IESKF)-based tight coupling between UBSL and DP-INS to mitigate UBSL's degeneration issues 3) Water-Stereo: a fusion of DP-INS and stereo camera for accurate initialization and tracking. Thirdly, we introduce a multi-modal factor graph back-end that dynamically fuses heterogeneous sensor data. The proposed multi-sensor factor graph maintenance strategy efficiently addresses issues caused by asynchronous sensor frequencies and partial data loss. Experimental results demonstrate Water-DSLAM achieves superior robustness (0.039 m trajectory RMSE and 100\\% continuity ratio during partial sensor dropout) and dense mapping (6922.4 points/m^3 in 750 m^3 water volume, approximately 10 times denser than existing methods) in various challenging environments, including pools, dark underwater scenes, 16-meter-deep sinkholes, and field rivers. Our project is available at https://water-scanner.github.io/."
  },
  {
    "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition",
    "url": "http://arxiv.org/abs/2504.21801v1",
    "arxiv_id": "2504.21801v1",
    "authors": [
      "Z. Z. Ren",
      "Zhihong Shao",
      "Junxiao Song",
      "Huajian Xin",
      "Haocheng Wang",
      "Wanjia Zhao",
      "Liyue Zhang",
      "Zhe Fu",
      "Qihao Zhu",
      "Dejian Yang",
      "Z. F. Wu",
      "Zhibin Gou",
      "Shirong Ma",
      "Hongxuan Tang",
      "Yuxuan Liu",
      "Wenjun Gao",
      "Daya Guo",
      "Chong Ruan"
    ],
    "published": "2025-04-30T16:57:48+00:00",
    "summary": "We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing."
  },
  {
    "title": "Toward Efficient Exploration by Large Language Model Agents",
    "url": "http://arxiv.org/abs/2504.20997v1",
    "arxiv_id": "2504.20997v1",
    "authors": [
      "Dilip Arumugam",
      "Thomas L. Griffiths"
    ],
    "published": "2025-04-29T17:59:48+00:00",
    "summary": "A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of data-efficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statistically-efficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, data-efficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration."
  },
  {
    "title": "TesserAct: Learning 4D Embodied World Models",
    "url": "http://arxiv.org/abs/2504.20995v1",
    "arxiv_id": "2504.20995v1",
    "authors": [
      "Haoyu Zhen",
      "Qiao Sun",
      "Hongxin Zhang",
      "Junyan Li",
      "Siyuan Zhou",
      "Yilun Du",
      "Chuang Gan"
    ],
    "published": "2025-04-29T17:59:30+00:00",
    "summary": "This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models."
  },
  {
    "title": "XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search",
    "url": "http://arxiv.org/abs/2504.20969v1",
    "arxiv_id": "2504.20969v1",
    "authors": [
      "Yiting Zhang",
      "Shichen Li",
      "Elena Shrestha"
    ],
    "published": "2025-04-29T17:37:45+00:00",
    "summary": "Mechanical search (MS) in cluttered environments remains a significant challenge for autonomous manipulators, requiring long-horizon planning and robust state estimation under occlusions and partial observability. In this work, we introduce XPG-RL, a reinforcement learning framework that enables agents to efficiently perform MS tasks through explainable, priority-guided decision-making based on raw sensory inputs. XPG-RL integrates a task-driven action prioritization mechanism with a learned context-aware switching strategy that dynamically selects from a discrete set of action primitives such as target grasping, occlusion removal, and viewpoint adjustment. Within this strategy, a policy is optimized to output adaptive threshold values that govern the discrete selection among action primitives. The perception module fuses RGB-D inputs with semantic and geometric features to produce a structured scene representation for downstream decision-making. Extensive experiments in both simulation and real-world settings demonstrate that XPG-RL consistently outperforms baseline methods in task success rates and motion efficiency, achieving up to 4.5$\\times$ higher efficiency in long-horizon tasks. These results underscore the benefits of integrating domain knowledge with learnable decision-making policies for robust and efficient robotic manipulation."
  },
  {
    "title": "Opinion-Driven Decision-Making for Multi-Robot Navigation through Narrow Corridors",
    "url": "http://arxiv.org/abs/2504.20947v1",
    "arxiv_id": "2504.20947v1",
    "authors": [
      "Norah K. Alghamdi",
      "Shinkyu Park"
    ],
    "published": "2025-04-29T17:14:55+00:00",
    "summary": "We propose an opinion-driven navigation framework for multi-robot traversal through a narrow corridor. Our approach leverages a multi-agent decision-making model known as the Nonlinear Opinion Dynamics (NOD) to address the narrow corridor passage problem, formulated as a multi-robot navigation game. By integrating the NOD model with a multi-robot path planning algorithm, we demonstrate that the framework effectively reduces the likelihood of deadlocks during corridor traversal. To ensure scalability with an increasing number of robots, we introduce a game reduction technique that enables efficient coordination in larger groups. Extensive simulation studies are conducted to validate the effectiveness of the proposed approach."
  },
  {
    "title": "Scenario-based Compositional Verification of Autonomous Systems with Neural Perception",
    "url": "http://arxiv.org/abs/2504.20942v1",
    "arxiv_id": "2504.20942v1",
    "authors": [
      "Christopher Watson",
      "Rajeev Alur",
      "Divya Gopinath",
      "Ravi Mangal",
      "Corina S. Pasareanu"
    ],
    "published": "2025-04-29T17:06:22+00:00",
    "summary": "Recent advances in deep learning have enabled the development of autonomous systems that use deep neural networks for perception. Formal verification of these systems is challenging due to the size and complexity of the perception DNNs as well as hard-to-quantify, changing environment conditions. To address these challenges, we propose a probabilistic verification framework for autonomous systems based on the following key concepts: (1) Scenario-based Modeling: We decompose the task (e.g., car navigation) into a composition of scenarios, each representing a different environment condition. (2) Probabilistic Abstractions: For each scenario, we build a compact abstraction of perception based on the DNN's performance on an offline dataset that represents the scenario's environment condition. (3) Symbolic Reasoning and Acceleration: The abstractions enable efficient compositional verification of the autonomous system via symbolic reasoning and a novel acceleration proof rule that bounds the error probability of the system under arbitrary variations of environment conditions. We illustrate our approach on two case studies: an experimental autonomous system that guides airplanes on taxiways using high-dimensional perception DNNs and a simulation model of an F1Tenth autonomous car using LiDAR observations."
  },
  {
    "title": "MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion",
    "url": "http://arxiv.org/abs/2504.20040v1",
    "arxiv_id": "2504.20040v1",
    "authors": [
      "Zador Pataki",
      "Paul-Edouard Sarlin",
      "Johannes L. Sch\u00f6nberger",
      "Marc Pollefeys"
    ],
    "published": "2025-04-28T17:59:52+00:00",
    "summary": "While Structure-from-Motion (SfM) has seen much progress over the years, state-of-the-art systems are prone to failure when facing extreme viewpoint changes in low-overlap, low-parallax or high-symmetry scenarios. Because capturing images that avoid these pitfalls is challenging, this severely limits the wider use of SfM, especially by non-expert users. We overcome these limitations by augmenting the classical SfM paradigm with monocular depth and normal priors inferred by deep neural networks. Thanks to a tight integration of monocular and multi-view constraints, our approach significantly outperforms existing ones under extreme viewpoint changes, while maintaining strong performance in standard conditions. We also show that monocular priors can help reject faulty associations due to symmetries, which is a long-standing problem for SfM. This makes our approach the first capable of reliably reconstructing challenging indoor environments from few images. Through principled uncertainty propagation, it is robust to errors in the priors, can handle priors inferred by different models with little tuning, and will thus easily benefit from future progress in monocular depth and normal estimation. Our code is publicly available at https://github.com/cvg/mpsfm."
  },
  {
    "title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning",
    "url": "http://arxiv.org/abs/2504.20024v1",
    "arxiv_id": "2504.20024v1",
    "authors": [
      "Wufei Ma",
      "Yu-Cheng Chou",
      "Qihao Liu",
      "Xingrui Wang",
      "Celso de Melo",
      "Jieneng Chen",
      "Jianwen Xie",
      "Alan Yuille"
    ],
    "published": "2025-04-28T17:48:43+00:00",
    "summary": "Recent studies in 3D spatial reasoning explore data-driven approaches and achieve enhanced spatial reasoning performance with reinforcement learning (RL). However, these methods typically perform spatial reasoning in an implicit manner, and it remains underexplored whether the acquired 3D knowledge generalizes to unseen question types at any stage of the training. In this work we introduce SpatialReasoner, a novel large vision-language model (LVLM) that address 3D spatial reasoning with explicit 3D representations shared between stages -- 3D perception, computation, and reasoning. Explicit 3D representations provide a coherent interface that supports advanced 3D spatial reasoning and enable us to study the factual errors made by LVLMs. Results show that our SpatialReasoner achieve improved performance on a variety of spatial reasoning benchmarks and generalizes better when evaluating on novel 3D spatial reasoning questions. Our study bridges the 3D parsing capabilities of prior visual foundation models with the powerful reasoning abilities of large language models, opening new directions for 3D spatial reasoning."
  },
  {
    "title": "Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models",
    "url": "http://arxiv.org/abs/2504.20020v1",
    "arxiv_id": "2504.20020v1",
    "authors": [
      "Xin Wang",
      "Haoyang Li",
      "Zeyang Zhang",
      "Haibo Chen",
      "Wenwu Zhu"
    ],
    "published": "2025-04-28T17:42:02+00:00",
    "summary": "Large language models (LLMs) have dramatically advanced machine learning research including natural language processing, computer vision, data mining, etc., yet they still exhibit critical limitations in reasoning, factual consistency, and interpretability. In this paper, we introduce a novel learning paradigm -- Modular Machine Learning (MML) -- as an essential approach toward new-generation LLMs. MML decomposes the complex structure of LLMs into three interdependent components: modular representation, modular model, and modular reasoning, aiming to enhance LLMs' capability of counterfactual reasoning, mitigating hallucinations, as well as promoting fairness, safety, and transparency. Specifically, the proposed MML paradigm can: i) clarify the internal working mechanism of LLMs through the disentanglement of semantic components; ii) allow for flexible and task-adaptive model design; iii) enable interpretable and logic-driven decision-making process. We present a feasible implementation of MML-based LLMs via leveraging advanced techniques such as disentangled representation learning, neural architecture search and neuro-symbolic learning. We critically identify key challenges, such as the integration of continuous neural and discrete symbolic processes, joint optimization, and computational scalability, present promising future research directions that deserve further exploration. Ultimately, the integration of the MML paradigm with LLMs has the potential to bridge the gap between statistical (deep) learning and formal (logical) reasoning, thereby paving the way for robust, adaptable, and trustworthy AI systems across a wide range of real-world applications."
  },
  {
    "title": "Modelling of Underwater Vehicles using Physics-Informed Neural Networks with Control",
    "url": "http://arxiv.org/abs/2504.20019v1",
    "arxiv_id": "2504.20019v1",
    "authors": [
      "Abdelhakim Amer",
      "David Felsager",
      "Yury Brodskiy",
      "Andriy Sarabakha"
    ],
    "published": "2025-04-28T17:38:57+00:00",
    "summary": "Physics-informed neural networks (PINNs) integrate physical laws with data-driven models to improve generalization and sample efficiency. This work introduces an open-source implementation of the Physics-Informed Neural Network with Control (PINC) framework, designed to model the dynamics of an underwater vehicle. Using initial states, control actions, and time inputs, PINC extends PINNs to enable physically consistent transitions beyond the training domain. Various PINC configurations are tested, including differing loss functions, gradient-weighting schemes, and hyperparameters. Validation on a simulated underwater vehicle demonstrates more accurate long-horizon predictions compared to a non-physics-informed baseline"
  },
  {
    "title": "Kinodynamic Trajectory Following with STELA: Simultaneous Trajectory Estimation & Local Adaptation",
    "url": "http://arxiv.org/abs/2504.20009v1",
    "arxiv_id": "2504.20009v1",
    "authors": [
      "Edgar Granados",
      "Sumanth Tangirala",
      "Kostas E. Bekris"
    ],
    "published": "2025-04-28T17:29:22+00:00",
    "summary": "State estimation and control are often addressed separately, leading to unsafe execution due to sensing noise, execution errors, and discrepancies between the planning model and reality. Simultaneous control and trajectory estimation using probabilistic graphical models has been proposed as a unified solution to these challenges. Previous work, however, relies heavily on appropriate Gaussian priors and is limited to holonomic robots with linear time-varying models. The current research extends graphical optimization methods to vehicles with arbitrary dynamical models via Simultaneous Trajectory Estimation and Local Adaptation (STELA). The overall approach initializes feasible trajectories using a kinodynamic, sampling-based motion planner. Then, it simultaneously: (i) estimates the past trajectory based on noisy observations, and (ii) adapts the controls to be executed to minimize deviations from the planned, feasible trajectory, while avoiding collisions. The proposed factor graph representation of trajectories in STELA can be applied for any dynamical system given access to first or second-order state update equations, and introduces the duration of execution between two states in the trajectory discretization as an optimization variable. These features provide both generalization and flexibility in trajectory following. In addition to targeting computational efficiency, the proposed strategy performs incremental updates of the factor graph using the iSAM algorithm and introduces a time-window mechanism. This mechanism allows the factor graph to be dynamically updated to operate over a limited history and forward horizon of the planned trajectory. This enables online updates of controls at a minimum of 10Hz. Experiments demonstrate that STELA achieves at least comparable performance to previous frameworks on idealized vehicles with linear dynamics.[...]"
  },
  {
    "title": "Generalization Capability for Imitation Learning",
    "url": "http://arxiv.org/abs/2504.18538v1",
    "arxiv_id": "2504.18538v1",
    "authors": [
      "Yixiao Wang"
    ],
    "published": "2025-04-25T17:59:59+00:00",
    "summary": "Imitation learning holds the promise of equipping robots with versatile skills by learning from expert demonstrations. However, policies trained on finite datasets often struggle to generalize beyond the training distribution. In this work, we present a unified perspective on the generalization capability of imitation learning, grounded in both information theorey and data distribution property. We first show that the generalization gap can be upper bounded by (i) the conditional information bottleneck on intermediate representations and (ii) the mutual information between the model parameters and the training dataset. This characterization provides theoretical guidance for designing effective training strategies in imitation learning, particularly in determining whether to freeze, fine-tune, or train large pretrained encoders (e.g., vision-language models or vision foundation models) from scratch to achieve better generalization. Furthermore, we demonstrate that high conditional entropy from input to output induces a flatter likelihood landscape, thereby reducing the upper bound on the generalization gap. In addition, it shortens the stochastic gradient descent (SGD) escape time from sharp local minima, which may increase the likelihood of reaching global optima under fixed optimization budgets. These insights explain why imitation learning often exhibits limited generalization and underscore the importance of not only scaling the diversity of input data but also enriching the variability of output labels conditioned on the same input."
  },
  {
    "title": "E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization",
    "url": "http://arxiv.org/abs/2504.18521v1",
    "arxiv_id": "2504.18521v1",
    "authors": [
      "Shintaro Shiba",
      "Quan Kong",
      "Norimasa Kobori"
    ],
    "published": "2025-04-25T17:43:20+00:00",
    "summary": "Optical communication using modulated LEDs (e.g., visible light communication) is an emerging application for event cameras, thanks to their high spatio-temporal resolutions. Event cameras can be used simply to decode the LED signals and also to localize the camera relative to the LED marker positions. However, there is no public dataset to benchmark the decoding and localization in various real-world settings. We present, to the best of our knowledge, the first public dataset that consists of an event camera, a frame camera, and ground-truth poses that are precisely synchronized with hardware triggers. It provides various camera motions with various sensitivities in different scene brightness settings, both indoor and outdoor. Furthermore, we propose a novel method of localization that leverages the Contrast Maximization framework for motion estimation and compensation. The detailed analysis and experimental results demonstrate the advantages of LED-based localization with events over the conventional AR-marker--based one with frames, as well as the efficacy of the proposed method in localization. We hope that the proposed dataset serves as a future benchmark for both motion-related classical computer vision tasks and LED marker decoding tasks simultaneously, paving the way to broadening applications of event cameras on mobile devices. https://woven-visionai.github.io/evlc-dataset"
  },
  {
    "title": "Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks",
    "url": "http://arxiv.org/abs/2504.18519v1",
    "arxiv_id": "2504.18519v1",
    "authors": [
      "Han Zhang",
      "Hao Zhou",
      "Medhat Elsayed",
      "Majid Bavand",
      "Raimundas Gaigalas",
      "Yigit Ozcan",
      "Melike Erol-Kantarci"
    ],
    "published": "2025-04-25T17:40:35+00:00",
    "summary": "Federated learning (FL) is a promising technique for learning-based functions in wireless networks, thanks to its distributed implementation capability. On the other hand, distributed learning may increase the risk of exposure to malicious attacks where attacks on a local model may spread to other models by parameter exchange. Meanwhile, such attacks can be hard to detect due to the dynamic wireless environment, especially considering local models can be heterogeneous with non-independent and identically distributed (non-IID) data. Therefore, it is critical to evaluate the effect of malicious attacks and develop advanced defense techniques for FL-enabled wireless networks. In this work, we introduce a federated deep reinforcement learning-based cell sleep control scenario that enhances the energy efficiency of the network. We propose multiple intelligent attacks targeting the learning-based approach and we propose defense methods to mitigate such attacks. In particular, we have designed two attack models, generative adversarial network (GAN)-enhanced model poisoning attack and regularization-based model poisoning attack. As a counteraction, we have proposed two defense schemes, autoencoder-based defense, and knowledge distillation (KD)-enabled defense. The autoencoder-based defense method leverages an autoencoder to identify the malicious participants and only aggregate the parameters of benign local models during the global aggregation, while KD-based defense protects the model from attacks by controlling the knowledge transferred between the global model and local models."
  },
  {
    "title": "Examining the Impact of Optical Aberrations to Image Classification and Object Detection Models",
    "url": "http://arxiv.org/abs/2504.18510v1",
    "arxiv_id": "2504.18510v1",
    "authors": [
      "Patrick M\u00fcller",
      "Alexander Braun",
      "Margret Keuper"
    ],
    "published": "2025-04-25T17:23:47+00:00",
    "summary": "Deep neural networks (DNNs) have proven to be successful in various computer vision applications such that models even infer in safety-critical situations. Therefore, vision models have to behave in a robust way to disturbances such as noise or blur. While seminal benchmarks exist to evaluate model robustness to diverse corruptions, blur is often approximated in an overly simplistic way to model defocus, while ignoring the different blur kernel shapes that result from optical systems. To study model robustness against realistic optical blur effects, this paper proposes two datasets of blur corruptions, which we denote OpticsBench and LensCorruptions. OpticsBench examines primary aberrations such as coma, defocus, and astigmatism, i.e. aberrations that can be represented by varying a single parameter of Zernike polynomials. To go beyond the principled but synthetic setting of primary aberrations, LensCorruptions samples linear combinations in the vector space spanned by Zernike polynomials, corresponding to 100 real lenses. Evaluations for image classification and object detection on ImageNet and MSCOCO show that for a variety of different pre-trained models, the performance on OpticsBench and LensCorruptions varies significantly, indicating the need to consider realistic image corruptions to evaluate a model's robustness against blur."
  },
  {
    "title": "Boxi: Design Decisions in the Context of Algorithmic Performance for Robotics",
    "url": "http://arxiv.org/abs/2504.18500v1",
    "arxiv_id": "2504.18500v1",
    "authors": [
      "Jonas Frey",
      "Turcan Tuna",
      "Lanke Frank Tarimo Fu",
      "Cedric Weibel",
      "Katharine Patterson",
      "Benjamin Krummenacher",
      "Matthias M\u00fcller",
      "Julian Nubert",
      "Maurice Fallon",
      "Cesar Cadena",
      "Marco Hutter"
    ],
    "published": "2025-04-25T17:11:48+00:00",
    "summary": "Achieving robust autonomy in mobile robots operating in complex and unstructured environments requires a multimodal sensor suite capable of capturing diverse and complementary information. However, designing such a sensor suite involves multiple critical design decisions, such as sensor selection, component placement, thermal and power limitations, compute requirements, networking, synchronization, and calibration. While the importance of these key aspects is widely recognized, they are often overlooked in academia or retained as proprietary knowledge within large corporations. To improve this situation, we present Boxi, a tightly integrated sensor payload that enables robust autonomy of robots in the wild. This paper discusses the impact of payload design decisions made to optimize algorithmic performance for downstream tasks, specifically focusing on state estimation and mapping. Boxi is equipped with a variety of sensors: two LiDARs, 10 RGB cameras including high-dynamic range, global shutter, and rolling shutter models, an RGB-D camera, 7 inertial measurement units (IMUs) of varying precision, and a dual antenna RTK GNSS system. Our analysis shows that time synchronization, calibration, and sensor modality have a crucial impact on the state estimation performance. We frame this analysis in the context of cost considerations and environment-specific challenges. We also present a mobile sensor suite `cookbook` to serve as a comprehensive guideline, highlighting generalizable key design considerations and lessons learned during the development of Boxi. Finally, we demonstrate the versatility of Boxi being used in a variety of applications in real-world scenarios, contributing to robust autonomy. More details and code: https://github.com/leggedrobotics/grand_tour_box"
  },
  {
    "title": "LiDPM: Rethinking Point Diffusion for Lidar Scene Completion",
    "url": "http://arxiv.org/abs/2504.17791v1",
    "arxiv_id": "2504.17791v1",
    "authors": [
      "Tetiana Martyniuk",
      "Gilles Puy",
      "Alexandre Boulch",
      "Renaud Marlet",
      "Raoul de Charette"
    ],
    "published": "2025-04-24T17:59:59+00:00",
    "summary": "Training diffusion models that work directly on lidar points at the scale of outdoor scenes is challenging due to the difficulty of generating fine-grained details from white noise over a broad field of view. The latest works addressing scene completion with diffusion models tackle this problem by reformulating the original DDPM as a local diffusion process. It contrasts with the common practice of operating at the level of objects, where vanilla DDPMs are currently used. In this work, we close the gap between these two lines of work. We identify approximations in the local diffusion formulation, show that they are not required to operate at the scene level, and that a vanilla DDPM with a well-chosen starting point is enough for completion. Finally, we demonstrate that our method, LiDPM, leads to better results in scene completion on SemanticKITTI. The project page is https://astra-vision.github.io/LiDPM ."
  },
  {
    "title": "Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation",
    "url": "http://arxiv.org/abs/2504.17784v1",
    "arxiv_id": "2504.17784v1",
    "authors": [
      "Yuyin Yang",
      "Zetao Cai",
      "Yang Tian",
      "Jia Zeng",
      "Jiangmiao Pang"
    ],
    "published": "2025-04-24T17:58:30+00:00",
    "summary": "Bimanual manipulation is a challenging yet crucial robotic capability, demanding precise spatial localization and versatile motion trajectories, which pose significant challenges to existing approaches. Existing approaches fall into two categories: keyframe-based strategies, which predict gripper poses in keyframes and execute them via motion planners, and continuous control methods, which estimate actions sequentially at each timestep. The keyframe-based method lacks inter-frame supervision, struggling to perform consistently or execute curved motions, while the continuous method suffers from weaker spatial perception. To address these issues, this paper introduces an end-to-end framework PPI (keyPose and Pointflow Interface), which integrates the prediction of target gripper poses and object pointflow with the continuous actions estimation. These interfaces enable the model to effectively attend to the target manipulation area, while the overall framework guides diverse and collision-free trajectories. By combining interface predictions with continuous actions estimation, PPI demonstrates superior performance in diverse bimanual manipulation tasks, providing enhanced spatial localization and satisfying flexibility in handling movement restrictions. In extensive evaluations, PPI significantly outperforms prior methods in both simulated and real-world experiments, achieving state-of-the-art performance with a +16.1% improvement on the RLBench2 simulation benchmark and an average of +27.5% gain across four challenging real-world tasks. Notably, PPI exhibits strong stability, high precision, and remarkable generalization capabilities in real-world scenarios. Project page: https://yuyinyang3y.github.io/PPI/"
  },
  {
    "title": "Josephson anomalous vortices",
    "url": "http://arxiv.org/abs/2504.17779v1",
    "arxiv_id": "2504.17779v1",
    "authors": [
      "Dan Crawford",
      "Stefan Ili\u0107",
      "Pauli Virtanen",
      "Tero T. Heikkil\u00e4"
    ],
    "published": "2025-04-24T17:56:00+00:00",
    "summary": "We show that vortices with circulating current, related with odd-frequency triplet pairing, appear in Josephson junctions where the barrier is a weak ferromagnet with strong spin-orbit coupling. By both symmetry analysis and microscopic methods we show that there is an additional term - a rotary invariant - in the superconducting free energy which allows for magnetoelectric effects even when the previously considered Lifshitz invariant vanishes. We show that the size, shape, and position of these vortices can be controlled by manipulating Rashba spin-orbit coupling in the weak link, via gates, and we suggest that these vortices could be detected via scanning magnetometry techniques. We also show that the transverse triplet components of the superconducting correlations can form a texture."
  },
  {
    "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
    "url": "http://arxiv.org/abs/2504.17771v1",
    "arxiv_id": "2504.17771v1",
    "authors": [
      "Haochen Wang",
      "Zhiwei Shi",
      "Chengxi Zhu",
      "Yafei Qiao",
      "Cheng Zhang",
      "Fan Yang",
      "Pengjie Ren",
      "Lan Lu",
      "Dong Xuan"
    ],
    "published": "2025-04-24T17:46:29+00:00",
    "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce \\ourmethod, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed ``IL+RL'' training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks such as agile catching and table tennis. Our project website: https://dreamstarring.github.io/HAMLET/."
  },
  {
    "title": "Zeptosecond free-electron compression through temporal lensing",
    "url": "http://arxiv.org/abs/2504.17770v1",
    "arxiv_id": "2504.17770v1",
    "authors": [
      "Xin Jin",
      "Cruz I. Velasco",
      "F. Javier Garc\u00eda de Abajo"
    ],
    "published": "2025-04-24T17:41:22+00:00",
    "summary": "The pursuit of ever-shorter time scales is a frontier in modern physics, exemplified by the synthesis of attosecond light pulses -- an achievement made possible by coherently superimposing a broad range of photon energies, as required by the uncertainty principle. However, extending this progress into the zeptosecond regime poses significant challenges, as it demands phase-correlated optical spectra spanning hundreds of electronvolts. In this context, electrons offer a compelling alternative to light because they can be coherently manipulated to form broad energy superpositions, as demonstrated by the generation of attosecond pulses in ultrafast electron microscopes. Here, we propose a practical scheme for compressing free electrons into the zeptosecond domain by modulating their wave functions using suitably tailored broadband light fields. Building on recent advances in {free-electron--light--matter} interactions, our method introduces the concept of temporal lensing -- an extension of conventional optical lensing to the time domain -- to produce electron pulses with arbitrarily short durations."
  },
  {
    "title": "Latent Diffusion Planning for Imitation Learning",
    "url": "http://arxiv.org/abs/2504.16925v1",
    "arxiv_id": "2504.16925v1",
    "authors": [
      "Amber Xie",
      "Oleh Rybkin",
      "Dorsa Sadigh",
      "Chelsea Finn"
    ],
    "published": "2025-04-23T17:53:34+00:00",
    "summary": "Recent progress in imitation learning has been enabled by policy architectures that scale to complex visuomotor tasks, multimodal distributions, and large datasets. However, these methods often rely on learning from large amount of expert demonstrations. To address these shortcomings, we propose Latent Diffusion Planning (LDP), a modular approach consisting of a planner which can leverage action-free demonstrations, and an inverse dynamics model which can leverage suboptimal data, that both operate over a learned latent space. First, we learn a compact latent space through a variational autoencoder, enabling effective forecasting of future states in image-based domains. Then, we train a planner and an inverse dynamics model with diffusion objectives. By separating planning from action prediction, LDP can benefit from the denser supervision signals of suboptimal and action-free data. On simulated visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation learning approaches, as they cannot leverage such additional data."
  },
  {
    "title": "Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.16923v1",
    "arxiv_id": "2504.16923v1",
    "authors": [
      "Jacob Levy",
      "Jason Gibson",
      "Bogdan Vlahov",
      "Erica Tevere",
      "Evangelos Theodorou",
      "David Fridovich-Keil",
      "Patrick Spieler"
    ],
    "published": "2025-04-23T17:51:36+00:00",
    "summary": "High-speed off-road autonomous driving presents unique challenges due to complex, evolving terrain characteristics and the difficulty of accurately modeling terrain-vehicle interactions. While dynamics models used in model-based control can be learned from real-world data, they often struggle to generalize to unseen terrain, making real-time adaptation essential. We propose a novel framework that combines a Kalman filter-based online adaptation scheme with meta-learned parameters to address these challenges. Offline meta-learning optimizes the basis functions along which adaptation occurs, as well as the adaptation parameters, while online adaptation dynamically adjusts the onboard dynamics model in real time for model-based control. We validate our approach through extensive experiments, including real-world testing on a full-scale autonomous off-road vehicle, demonstrating that our method outperforms baseline approaches in prediction accuracy, performance, and safety metrics, particularly in safety-critical scenarios. Our results underscore the effectiveness of meta-learned dynamics model adaptation, advancing the development of reliable autonomous systems capable of navigating diverse and unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA"
  },
  {
    "title": "Zero-shot Sim-to-Real Transfer for Reinforcement Learning-based Visual Servoing of Soft Continuum Arms",
    "url": "http://arxiv.org/abs/2504.16916v1",
    "arxiv_id": "2504.16916v1",
    "authors": [
      "Hsin-Jung Yang",
      "Mahsa Khosravi",
      "Benjamin Walt",
      "Girish Krishnan",
      "Soumik Sarkar"
    ],
    "published": "2025-04-23T17:41:55+00:00",
    "summary": "Soft continuum arms (SCAs) soft and deformable nature presents challenges in modeling and control due to their infinite degrees of freedom and non-linear behavior. This work introduces a reinforcement learning (RL)-based framework for visual servoing tasks on SCAs with zero-shot sim-to-real transfer capabilities, demonstrated on a single section pneumatic manipulator capable of bending and twisting. The framework decouples kinematics from mechanical properties using an RL kinematic controller for motion planning and a local controller for actuation refinement, leveraging minimal sensing with visual feedback. Trained entirely in simulation, the RL controller achieved a 99.8% success rate. When deployed on hardware, it achieved a 67% success rate in zero-shot sim-to-real transfer, demonstrating robustness and adaptability. This approach offers a scalable solution for SCAs in 3D visual servoing, with potential for further refinement and expanded applications."
  },
  {
    "title": "MorphoNavi: Aerial-Ground Robot Navigation with Object Oriented Mapping in Digital Twin",
    "url": "http://arxiv.org/abs/2504.16914v1",
    "arxiv_id": "2504.16914v1",
    "authors": [
      "Sausar Karaf",
      "Mikhail Martynov",
      "Oleg Sautenkov",
      "Zhanibek Darush",
      "Dzmitry Tsetserukou"
    ],
    "published": "2025-04-23T17:41:12+00:00",
    "summary": "This paper presents a novel mapping approach for a universal aerial-ground robotic system utilizing a single monocular camera. The proposed system is capable of detecting a diverse range of objects and estimating their positions without requiring fine-tuning for specific environments. The system's performance was evaluated through a simulated search-and-rescue scenario, where the MorphoGear robot successfully located a robotic dog while an operator monitored the process. This work contributes to the development of intelligent, multimodal robotic systems capable of operating in unstructured environments."
  },
  {
    "title": "Learning Verifiable Control Policies Using Relaxed Verification",
    "url": "http://arxiv.org/abs/2504.16879v1",
    "arxiv_id": "2504.16879v1",
    "authors": [
      "Puja Chaudhury",
      "Alexander Estornell",
      "Michael Everett"
    ],
    "published": "2025-04-23T16:54:35+00:00",
    "summary": "To provide safety guarantees for learning-based control systems, recent work has developed formal verification methods to apply after training ends. However, if the trained policy does not meet the specifications, or there is conservatism in the verification algorithm, establishing these guarantees may not be possible. Instead, this work proposes to perform verification throughout training to ultimately aim for policies whose properties can be evaluated throughout runtime with lightweight, relaxed verification algorithms. The approach is to use differentiable reachability analysis and incorporate new components into the loss function. Numerical experiments on a quadrotor model and unicycle model highlight the ability of this approach to lead to learned control policies that satisfy desired reach-avoid and invariance specifications."
  },
  {
    "title": "TTRL: Test-Time Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.16084v1",
    "arxiv_id": "2504.16084v1",
    "authors": [
      "Yuxin Zuo",
      "Kaiyan Zhang",
      "Shang Qu",
      "Li Sheng",
      "Xuekai Zhu",
      "Biqing Qi",
      "Youbang Sun",
      "Ganqu Cui",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "published": "2025-04-22T17:59:56+00:00",
    "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
  },
  {
    "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities",
    "url": "http://arxiv.org/abs/2504.16078v1",
    "arxiv_id": "2504.16078v1",
    "authors": [
      "Thomas Schmied",
      "J\u00f6rg Bornschein",
      "Jordi Grau-Moya",
      "Markus Wulfmeier",
      "Razvan Pascanu"
    ],
    "published": "2025-04-22T17:57:14+00:00",
    "summary": "The success of Large Language Models (LLMs) has sparked interest in various agentic applications. A key hypothesis is that LLMs, leveraging common sense and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently solve complex domains. However, LLM agents have been found to suffer from sub-optimal exploration and the knowing-doing gap, the inability to effectively act on knowledge present in the model. In this work, we systematically study why LLMs perform sub-optimally in decision-making scenarios. In particular, we closely examine three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. We propose mitigation of these shortcomings by fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales. Our experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making abilities of LLMs by increasing exploration and narrowing the knowing-doing gap. Finally, we study both classic exploration mechanisms, such as $\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and self-consistency, to enable more effective fine-tuning of LLMs for decision-making."
  },
  {
    "title": "ForesightNav: Learning Scene Imagination for Efficient Exploration",
    "url": "http://arxiv.org/abs/2504.16062v1",
    "arxiv_id": "2504.16062v1",
    "authors": [
      "Hardik Shah",
      "Jiaxu Xing",
      "Nico Messikommer",
      "Boyang Sun",
      "Marc Pollefeys",
      "Davide Scaramuzza"
    ],
    "published": "2025-04-22T17:38:38+00:00",
    "summary": "Understanding how humans leverage prior knowledge to navigate unseen environments while making exploratory decisions is essential for developing autonomous robots with similar abilities. In this work, we propose ForesightNav, a novel exploration strategy inspired by human imagination and reasoning. Our approach equips robotic agents with the capability to predict contextual information, such as occupancy and semantic details, for unexplored regions. These predictions enable the robot to efficiently select meaningful long-term navigation goals, significantly enhancing exploration in unseen environments. We validate our imagination-based approach using the Structured3D dataset, demonstrating accurate occupancy prediction and superior performance in anticipating unseen scene geometry. Our experiments show that the imagination module improves exploration efficiency in unseen environments, achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav on the Structured3D Validation split. These contributions demonstrate the power of imagination-driven reasoning for autonomous systems to enhance generalizable and efficient exploration."
  },
  {
    "title": "SAR4SLPs: An Asynchronous Survey of Speech-Language Pathologists' Perspectives on Socially Assistive Robots",
    "url": "http://arxiv.org/abs/2504.16055v1",
    "arxiv_id": "2504.16055v1",
    "authors": [
      "Denielle Oliva",
      "Abbie Olszewski",
      "David Feil-Seifer"
    ],
    "published": "2025-04-22T17:32:09+00:00",
    "summary": "Socially Assistive Robots (SARs) offer unique opportunities within speech language pathology (SLP) education and practice by supporting interactive interventions for children with communication disorders. This paper explores the implementation of SAR4SLPs (Socially Assistive Robots for Speech-Language Pathologists) to investigate aspects such as engagement, therapeutic strategy discipline, and consistent intervention support. We assessed the current application of technology to clinical and educational settings, especially with respect to how SLPs might use SAR in their therapeutic work. An asynchronous remote community (ARC) collaborated with a cohort of practicing SLPs to consider the feasibility, potential effectiveness, and anticipated challenges with implementing SARs in day-to-day interventions and as practice facilitators. We focus in particular on the expressive functionality of SARs, modeling a foundational strategy that SLPs employ across various intervention targets. This paper highlights clinician-driven insights and design implications for developing SARs that support specific treatment goals through collaborative and iterative design."
  },
  {
    "title": "$\u03c0_{0.5}$: a Vision-Language-Action Model with Open-World Generalization",
    "url": "http://arxiv.org/abs/2504.16054v1",
    "arxiv_id": "2504.16054v1",
    "authors": [
      "Physical Intelligence",
      "Kevin Black",
      "Noah Brown",
      "James Darpinian",
      "Karan Dhabalia",
      "Danny Driess",
      "Adnan Esmail",
      "Michael Equi",
      "Chelsea Finn",
      "Niccolo Fusai",
      "Manuel Y. Galliker",
      "Dibya Ghosh",
      "Lachy Groom",
      "Karol Hausman",
      "Brian Ichter",
      "Szymon Jakubczak",
      "Tim Jones",
      "Liyiming Ke",
      "Devin LeBlanc",
      "Sergey Levine",
      "Adrian Li-Bell",
      "Mohith Mothukuri",
      "Suraj Nair",
      "Karl Pertsch",
      "Allen Z. Ren",
      "Lucy Xiaoyang Shi",
      "Laura Smith",
      "Jost Tobias Springenberg",
      "Kyle Stachowicz",
      "James Tanner",
      "Quan Vuong",
      "Homer Walke",
      "Anna Walling",
      "Haohuan Wang",
      "Lili Yu",
      "Ury Zhilinsky"
    ],
    "published": "2025-04-22T17:31:29+00:00",
    "summary": "In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open question how far such models can generalize in the wild. We describe $\\pi_{0.5}$, a new model based on $\\pi_{0}$ that uses co-training on heterogeneous tasks to enable broad generalization. $\\pi_{0.5}$\\ uses data from multiple robots, high-level semantic prediction, web data, and other sources to enable broadly generalizable real-world robotic manipulation. Our system uses a combination of co-training and hybrid multi-modal examples that combine image observations, language commands, object detections, semantic subtask prediction, and low-level actions. Our experiments show that this kind of knowledge transfer is essential for effective generalization, and we demonstrate for the first time that an end-to-end learning-enabled robotic system can perform long-horizon and dexterous manipulation skills, such as cleaning a kitchen or bedroom, in entirely new homes."
  },
  {
    "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models",
    "url": "http://arxiv.org/abs/2504.15279v1",
    "arxiv_id": "2504.15279v1",
    "authors": [
      "Weiye Xu",
      "Jiahao Wang",
      "Weiyun Wang",
      "Zhe Chen",
      "Wengang Zhou",
      "Aijun Yang",
      "Lewei Lu",
      "Houqiang Li",
      "Xiaohua Wang",
      "Xizhou Zhu",
      "Wenhai Wang",
      "Jifeng Dai",
      "Jinguo Zhu"
    ],
    "published": "2025-04-21T17:59:53+00:00",
    "summary": "Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark of 1,000 human-verified problems across six categories (e.g., quantitative shifts, spatial relations, attribute comparisons). These various types of questions can be evaluated to assess the visual reasoning capabilities of MLLMs from multiple perspectives. We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes. Most models score below 30% accuracy-only slightly above the 25% random baseline and far below the 51.4% achieved by humans-revealing significant gaps in visual reasoning. Furthermore, we provide a supplementary training dataset and a reinforcement-learning baseline to support further progress."
  },
  {
    "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs",
    "url": "http://arxiv.org/abs/2504.15280v1",
    "arxiv_id": "2504.15280v1",
    "authors": [
      "Chun-Hsiao Yeh",
      "Chenyu Wang",
      "Shengbang Tong",
      "Ta-Ying Cheng",
      "Rouyu Wang",
      "Tianzhe Chu",
      "Yuexiang Zhai",
      "Yubei Chen",
      "Shenghua Gao",
      "Yi Ma"
    ],
    "published": "2025-04-21T17:59:53+00:00",
    "summary": "Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/."
  },
  {
    "title": "DRAWER: Digital Reconstruction and Articulation With Environment Realism",
    "url": "http://arxiv.org/abs/2504.15278v1",
    "arxiv_id": "2504.15278v1",
    "authors": [
      "Hongchi Xia",
      "Entong Su",
      "Marius Memmel",
      "Arhan Jain",
      "Raymond Yu",
      "Numfor Mbiziwo-Tiapo",
      "Ali Farhadi",
      "Abhishek Gupta",
      "Shenlong Wang",
      "Wei-Chiu Ma"
    ],
    "published": "2025-04-21T17:59:49+00:00",
    "summary": "Creating virtual digital replicas from real-world data unlocks significant potential across domains like gaming and robotics. In this paper, we present DRAWER, a novel framework that converts a video of a static indoor scene into a photorealistic and interactive digital environment. Our approach centers on two main contributions: (i) a reconstruction module based on a dual scene representation that reconstructs the scene with fine-grained geometric details, and (ii) an articulation module that identifies articulation types and hinge positions, reconstructs simulatable shapes and appearances and integrates them into the scene. The resulting virtual environment is photorealistic, interactive, and runs in real time, with compatibility for game engines and robotic simulation platforms. We demonstrate the potential of DRAWER by using it to automatically create an interactive game in Unreal Engine and to enable real-to-sim-to-real transfer for robotics applications."
  },
  {
    "title": "DRAWER: Digital Reconstruction and Articulation With Environment Realism",
    "url": "http://arxiv.org/abs/2504.15278v2",
    "arxiv_id": "2504.15278v2",
    "authors": [
      "Hongchi Xia",
      "Entong Su",
      "Marius Memmel",
      "Arhan Jain",
      "Raymond Yu",
      "Numfor Mbiziwo-Tiapo",
      "Ali Farhadi",
      "Abhishek Gupta",
      "Shenlong Wang",
      "Wei-Chiu Ma"
    ],
    "published": "2025-04-21T17:59:49+00:00",
    "summary": "Creating virtual digital replicas from real-world data unlocks significant potential across domains like gaming and robotics. In this paper, we present DRAWER, a novel framework that converts a video of a static indoor scene into a photorealistic and interactive digital environment. Our approach centers on two main contributions: (i) a reconstruction module based on a dual scene representation that reconstructs the scene with fine-grained geometric details, and (ii) an articulation module that identifies articulation types and hinge positions, reconstructs simulatable shapes and appearances and integrates them into the scene. The resulting virtual environment is photorealistic, interactive, and runs in real time, with compatibility for game engines and robotic simulation platforms. We demonstrate the potential of DRAWER by using it to automatically create an interactive game in Unreal Engine and to enable real-to-sim-to-real transfer for robotics applications."
  },
  {
    "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning",
    "url": "http://arxiv.org/abs/2504.15275v1",
    "arxiv_id": "2504.15275v1",
    "authors": [
      "Jie Cheng",
      "Ruixi Qiao",
      "Lijun Li",
      "Chao Guo",
      "Junle Wang",
      "Gang Xiong",
      "Yisheng Lv",
      "Fei-Yue Wang"
    ],
    "published": "2025-04-21T17:59:02+00:00",
    "summary": "Process reward models (PRMs) have proven effective for test-time scaling of Large Language Models (LLMs) on challenging reasoning tasks. However, reward hacking issues with PRMs limit their successful application in reinforcement fine-tuning. In this paper, we identify the main cause of PRM-induced reward hacking: the canonical summation-form credit assignment in reinforcement learning (RL), which defines the value as cumulative gamma-decayed future rewards, easily induces LLMs to hack steps with high rewards. To address this, we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation of PURE is a min-form credit assignment that formulates the value function as the minimum of future rewards. This method significantly alleviates reward hacking by limiting the value function range and distributing advantages more reasonably. Through extensive experiments on 3 base models, we show that PRM-based approaches enabling min-form credit assignment achieve comparable reasoning performance to verifiable reward-based methods within only 30% steps. In contrast, the canonical sum-form credit assignment collapses training even at the beginning! Additionally, when we supplement PRM-based fine-tuning with just 10% verifiable rewards, we further alleviate reward hacking and produce the best fine-tuned model based on Qwen2.5-Math-7B in our experiments, achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5 benchmarks. Moreover, we summarize the observed reward hacking cases and analyze the causes of training collapse. Code and models are available at https://github.com/CJReinforce/PURE."
  },
  {
    "title": "Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning",
    "url": "http://arxiv.org/abs/2504.15263v1",
    "arxiv_id": "2504.15263v1",
    "authors": [
      "Ehsan Ahmadi",
      "Chao Wang"
    ],
    "published": "2025-04-21T17:45:21+00:00",
    "summary": "Construction tasks are inherently unpredictable, with dynamic environments and safety-critical demands posing significant risks to workers. Exoskeletons offer potential assistance but falter without accurate intent recognition across diverse locomotion modes. This paper presents a locomotion prediction agent leveraging Large Language Models (LLMs) augmented with memory systems, aimed at improving exoskeleton assistance in such settings. Using multimodal inputs - spoken commands and visual data from smart glasses - the agent integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory (LTM), and Refinement Module to predict locomotion modes effectively. Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague and safety-critical commands. Calibration metrics, including a Brier Score drop from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability. This framework supports safer, high-level human-exoskeleton collaboration, with promise for adaptive assistive systems in dynamic industries."
  },
  {
    "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
    "url": "http://arxiv.org/abs/2504.13837v1",
    "arxiv_id": "2504.13837v1",
    "authors": [
      "Yang Yue",
      "Zhiqi Chen",
      "Rui Lu",
      "Andrew Zhao",
      "Zhaokai Wang",
      "Yang Yue",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2025-04-18T17:59:56+00:00",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io"
  },
  {
    "title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.13818v1",
    "arxiv_id": "2504.13818v1",
    "authors": [
      "Yixuan Even Xu",
      "Yash Savani",
      "Fei Fang",
      "Zico Kolter"
    ],
    "published": "2025-04-18T17:49:55+00:00",
    "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing reasoning capabilities in large language models, but faces a fundamental asymmetry in computation and memory requirements: inference is embarrassingly parallel with a minimal memory footprint, while policy updates require extensive synchronization and are memory-intensive. To address this asymmetry, we introduce PODS (Policy Optimization with Down-Sampling), a framework that strategically decouples these phases by generating numerous rollouts in parallel but updating only on an informative subset. Within this framework, we develop max-variance down-sampling, a theoretically motivated method that selects rollouts with maximally diverse reward signals. We prove that this approach has an efficient algorithmic solution, and empirically demonstrate that GRPO with PODS using max-variance down-sampling achieves superior performance over standard GRPO on the GSM8K benchmark."
  },
  {
    "title": "DiffOG: Differentiable Policy Trajectory Optimization with Generalizability",
    "url": "http://arxiv.org/abs/2504.13807v1",
    "arxiv_id": "2504.13807v1",
    "authors": [
      "Zhengtong Xu",
      "Zichen Miao",
      "Qiang Qiu",
      "Zhe Zhang",
      "Yu She"
    ],
    "published": "2025-04-18T17:20:27+00:00",
    "summary": "Imitation learning-based visuomotor policies excel at manipulation tasks but often produce suboptimal action trajectories compared to model-based methods. Directly mapping camera data to actions via neural networks can result in jerky motions and difficulties in meeting critical constraints, compromising safety and robustness in real-world deployment. For tasks that require high robustness or strict adherence to constraints, ensuring trajectory quality is crucial. However, the lack of interpretability in neural networks makes it challenging to generate constraint-compliant actions in a controlled manner. This paper introduces differentiable policy trajectory optimization with generalizability (DiffOG), a learning-based trajectory optimization framework designed to enhance visuomotor policies. By leveraging the proposed differentiable formulation of trajectory optimization with transformer, DiffOG seamlessly integrates policies with a generalizable optimization layer. Visuomotor policies enhanced by DiffOG generate smoother, constraint-compliant action trajectories in a more interpretable way. DiffOG exhibits strong generalization capabilities and high flexibility. We evaluated DiffOG across 11 simulated tasks and 2 real-world tasks. The results demonstrate that DiffOG significantly enhances the trajectory quality of visuomotor policies while having minimal impact on policy performance, outperforming trajectory processing baselines such as greedy constraint clipping and penalty-based trajectory optimization. Furthermore, DiffOG achieves superior performance compared to existing constrained visuomotor policy."
  },
  {
    "title": "Imitation Learning with Precisely Labeled Human Demonstrations",
    "url": "http://arxiv.org/abs/2504.13803v1",
    "arxiv_id": "2504.13803v1",
    "authors": [
      "Yilong Song"
    ],
    "published": "2025-04-18T17:12:00+00:00",
    "summary": "Within the imitation learning paradigm, training generalist robots requires large-scale datasets obtainable only through diverse curation. Due to the relative ease to collect, human demonstrations constitute a valuable addition when incorporated appropriately. However, existing methods utilizing human demonstrations face challenges in inferring precise actions, ameliorating embodiment gaps, and fusing with frontier generalist robot training pipelines. In this work, building on prior studies that demonstrate the viability of using hand-held grippers for efficient data collection, we leverage the user's control over the gripper's appearance--specifically by assigning it a unique, easily segmentable color--to enable simple and reliable application of the RANSAC and ICP registration method for precise end-effector pose estimation. We show in simulation that precisely labeled human demonstrations on their own allow policies to reach on average 88.1% of the performance of using robot demonstrations, and boost policy performance when combined with robot demonstrations, despite the inherent embodiment gap."
  },
  {
    "title": "Unified Manipulability and Compliance Analysis of Modular Soft-Rigid Hybrid Fingers",
    "url": "http://arxiv.org/abs/2504.13800v1",
    "arxiv_id": "2504.13800v1",
    "authors": [
      "Jianshu Zhou",
      "Boyuan Liang",
      "Junda Huang",
      "Masayoshi Tomizuka"
    ],
    "published": "2025-04-18T17:05:54+00:00",
    "summary": "This paper presents a unified framework to analyze the manipulability and compliance of modular soft-rigid hybrid robotic fingers. The approach applies to both hydraulic and pneumatic actuation systems. A Jacobian-based formulation maps actuator inputs to joint and task-space responses. Hydraulic actuators are modeled under incompressible assumptions, while pneumatic actuators are described using nonlinear pressure-volume relations. The framework enables consistent evaluation of manipulability ellipsoids and compliance matrices across actuation modes. We validate the analysis using two representative hands: DexCo (hydraulic) and Edgy-2 (pneumatic). Results highlight actuation-dependent trade-offs in dexterity and passive stiffness. These findings provide insights for structure-aware design and actuator selection in soft-rigid robotic fingers."
  },
  {
    "title": "ViTa-Zero: Zero-shot Visuotactile Object 6D Pose Estimation",
    "url": "http://arxiv.org/abs/2504.13179v1",
    "arxiv_id": "2504.13179v1",
    "authors": [
      "Hongyu Li",
      "James Akl",
      "Srinath Sridhar",
      "Tye Brady",
      "Taskin Padir"
    ],
    "published": "2025-04-17T17:59:56+00:00",
    "summary": "Object 6D pose estimation is a critical challenge in robotics, particularly for manipulation tasks. While prior research combining visual and tactile (visuotactile) information has shown promise, these approaches often struggle with generalization due to the limited availability of visuotactile data. In this paper, we introduce ViTa-Zero, a zero-shot visuotactile pose estimation framework. Our key innovation lies in leveraging a visual model as its backbone and performing feasibility checking and test-time optimization based on physical constraints derived from tactile and proprioceptive observations. Specifically, we model the gripper-object interaction as a spring-mass system, where tactile sensors induce attractive forces, and proprioception generates repulsive forces. We validate our framework through experiments on a real-world robot setup, demonstrating its effectiveness across representative visual backbones and manipulation scenarios, including grasping, object picking, and bimanual handover. Compared to the visual models, our approach overcomes some drastic failure modes while tracking the in-hand object pose. In our experiments, our approach shows an average increase of 55% in AUC of ADD-S and 60% in ADD, along with an 80% lower position error compared to FoundationPose."
  },
  {
    "title": "Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation",
    "url": "http://arxiv.org/abs/2504.13175v1",
    "arxiv_id": "2504.13175v1",
    "authors": [
      "Sizhe Yang",
      "Wenye Yu",
      "Jia Zeng",
      "Jun Lv",
      "Kerui Ren",
      "Cewu Lu",
      "Dahua Lin",
      "Jiangmiao Pang"
    ],
    "published": "2025-04-17T17:59:43+00:00",
    "summary": "Visuomotor policies learned from teleoperated demonstrations face challenges such as lengthy data collection, high costs, and limited data diversity. Existing approaches address these issues by augmenting image observations in RGB space or employing Real-to-Sim-to-Real pipelines based on physical simulators. However, the former is constrained to 2D data augmentation, while the latter suffers from imprecise physical simulation caused by inaccurate geometric reconstruction. This paper introduces RoboSplat, a novel method that generates diverse, visually realistic demonstrations by directly manipulating 3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian Splatting (3DGS), directly edit the reconstructed scene, and augment data across six types of generalization with five techniques: 3D Gaussian replacement for varying object types, scene appearance, and robot embodiments; equivariant transformations for different object poses; visual attribute editing for various lighting conditions; novel view synthesis for new camera perspectives; and 3D content generation for diverse object types. Comprehensive real-world experiments demonstrate that RoboSplat significantly enhances the generalization of visuomotor policies under diverse disturbances. Notably, while policies trained on hundreds of real-world demonstrations with additional 2D data augmentation achieve an average success rate of 57.2%, RoboSplat attains 87.8% in one-shot settings across six types of generalization in the real world."
  },
  {
    "title": "A New Semidefinite Relaxation for Linear and Piecewise-Affine Optimal Control with Time Scaling",
    "url": "http://arxiv.org/abs/2504.13170v1",
    "arxiv_id": "2504.13170v1",
    "authors": [
      "Lujie Yang",
      "Tobia Marcucci",
      "Pablo A. Parrilo",
      "Russ Tedrake"
    ],
    "published": "2025-04-17T17:59:23+00:00",
    "summary": "We introduce a semidefinite relaxation for optimal control of linear systems with time scaling. These problems are inherently nonconvex, since the system dynamics involves bilinear products between the discretization time step and the system state and controls. The proposed relaxation is closely related to the standard second-order semidefinite relaxation for quadratic constraints, but we carefully select a subset of the possible bilinear terms and apply a change of variables to achieve empirically tight relaxations while keeping the computational load light. We further extend our method to handle piecewise-affine (PWA) systems by formulating the PWA optimal-control problem as a shortest-path problem in a graph of convex sets (GCS). In this GCS, different paths represent different mode sequences for the PWA system, and the convex sets model the relaxed dynamics within each mode. By combining a tight convex relaxation of the GCS problem with our semidefinite relaxation with time scaling, we can solve PWA optimal-control problems through a single semidefinite program."
  },
  {
    "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling",
    "url": "http://arxiv.org/abs/2504.13169v1",
    "arxiv_id": "2504.13169v1",
    "authors": [
      "Tsung-Han Wu",
      "Heekyung Lee",
      "Jiaxin Ge",
      "Joseph E. Gonzalez",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "published": "2025-04-17T17:59:22+00:00",
    "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io."
  },
  {
    "title": "RUKA: Rethinking the Design of Humanoid Hands with Learning",
    "url": "http://arxiv.org/abs/2504.13165v1",
    "arxiv_id": "2504.13165v1",
    "authors": [
      "Anya Zorin",
      "Irmak Guzey",
      "Billy Yan",
      "Aadhithya Iyer",
      "Lisa Kondrich",
      "Nikhil X. Bhattasali",
      "Lerrel Pinto"
    ],
    "published": "2025-04-17T17:58:59+00:00",
    "summary": "Dexterous manipulation is a fundamental capability for robotic systems, yet progress has been limited by hardware trade-offs between precision, compactness, strength, and affordability. Existing control methods impose compromises on hand designs and applications. However, learning-based approaches present opportunities to rethink these trade-offs, particularly to address challenges with tendon-driven actuation and low-cost materials. This work presents RUKA, a tendon-driven humanoid hand that is compact, affordable, and capable. Made from 3D-printed parts and off-the-shelf components, RUKA has 5 fingers with 15 underactuated degrees of freedom enabling diverse human-like grasps. Its tendon-driven actuation allows powerful grasping in a compact, human-sized form factor. To address control challenges, we learn joint-to-actuator and fingertip-to-actuator models from motion-capture data collected by the MANUS glove, leveraging the hand's morphological accuracy. Extensive evaluations demonstrate RUKA's superior reachability, durability, and strength compared to other robotic hands. Teleoperation tasks further showcase RUKA's dexterous movements. The open-source design and assembly instructions of RUKA, code, and data are available at https://ruka-hand.github.io/."
  },
  {
    "title": "Adapting a World Model for Trajectory Following in a 3D Game",
    "url": "http://arxiv.org/abs/2504.12299v1",
    "arxiv_id": "2504.12299v1",
    "authors": [
      "Marko Tot",
      "Shu Ishida",
      "Abdelhak Lemkhenter",
      "David Bignell",
      "Pallavi Choudhury",
      "Chris Lovett",
      "Luis Fran\u00e7a",
      "Matheus Ribeiro Furtado de Mendon\u00e7a",
      "Tarun Gupta",
      "Darren Gehring",
      "Sam Devlin",
      "Sergio Valcarcel Macua",
      "Raluca Georgescu"
    ],
    "published": "2025-04-16T17:59:54+00:00",
    "summary": "Imitation learning is a powerful tool for training agents by leveraging expert knowledge, and being able to replicate a given trajectory is an integral part of it. In complex environments, like modern 3D video games, distribution shift and stochasticity necessitate robust approaches beyond simple action replay. In this study, we apply Inverse Dynamics Models (IDM) with different encoders and policy heads to trajectory following in a modern 3D video game -- Bleeding Edge. Additionally, we investigate several future alignment strategies that address the distribution shift caused by the aleatoric uncertainty and imperfections of the agent. We measure both the trajectory deviation distance and the first significant deviation point between the reference and the agent's trajectory and show that the optimal configuration depends on the chosen setting. Our results show that in a diverse data setting, a GPT-style policy head with an encoder trained from scratch performs the best, DINOv2 encoder with the GPT-style policy head gives the best results in the low data regime, and both GPT-style and MLP-style policy heads had comparable results when pre-trained on a diverse setting and fine-tuned for a specific behaviour setting."
  },
  {
    "title": "A complete theory of the Clifford commutant",
    "url": "http://arxiv.org/abs/2504.12263v1",
    "arxiv_id": "2504.12263v1",
    "authors": [
      "Lennart Bittel",
      "Jens Eisert",
      "Lorenzo Leone",
      "Antonio A. Mele",
      "Salvatore F. E. Oliviero"
    ],
    "published": "2025-04-16T17:21:34+00:00",
    "summary": "The Clifford group plays a central role in quantum information science. It is the building block for many error-correcting schemes and matches the first three moments of the Haar measure over the unitary group -a property that is essential for a broad range of quantum algorithms, with applications in pseudorandomness, learning theory, benchmarking, and entanglement distillation. At the heart of understanding many properties of the Clifford group lies the Clifford commutant: the set of operators that commute with $k$-fold tensor powers of Clifford unitaries. Previous understanding of this commutant has been limited to relatively small values of $k$, constrained by the number of qubits $n$. In this work, we develop a complete theory of the Clifford commutant. Our first result provides an explicit orthogonal basis for the commutant and computes its dimension for arbitrary $n$ and $k$. We also introduce an alternative and easy-to-manipulate basis formed by isotropic sums of Pauli operators. We show that this basis is generated by products of permutations -which generate the unitary group commutant- and at most three other operators. Additionally, we develop a graphical calculus allowing a diagrammatic manipulation of elements of this basis. These results enable a wealth of applications: among others, we characterize all measurable magic measures and identify optimal strategies for stabilizer property testing, whose success probability also offers an operational interpretation to stabilizer entropies. Finally, we show that these results also generalize to multi-qudit systems with prime local dimension."
  },
  {
    "title": "Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning",
    "url": "http://arxiv.org/abs/2504.12254v1",
    "arxiv_id": "2504.12254v1",
    "authors": [
      "Mahmoud Salhab",
      "Marwan Elghitany",
      "Shameed Sait",
      "Syed Sibghat Ullah",
      "Mohammad Abusheikh",
      "Hasan Abusheikh"
    ],
    "published": "2025-04-16T17:05:14+00:00",
    "summary": "Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach attains state-of-the-art (SOTA) performance, exceeding all previous efforts in the field of Arabic ASR on the standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings."
  },
  {
    "title": "Exotic Quantum States in Spin-1 Bose-Einstein Condensate with Spin-Orbit Coupling in Concentric Annular Traps",
    "url": "http://arxiv.org/abs/2504.12247v1",
    "arxiv_id": "2504.12247v1",
    "authors": [
      "Yun Liu",
      "Zu-Jian Ying"
    ],
    "published": "2025-04-16T16:54:14+00:00",
    "summary": "We explore the exotic quantum states emerging in the ground state (GS) of a strongly-correlated spin-1 Bose-Einstein condensate confined in two-dimensional concentric annular traps with a spin-orbit coupling (SOC). In the antiferromagnetic case, the GS density manifests various patterns of distributions, including facial-makeup states, petal states, topological fissure states, multiple-half-ring states and property-distinguished vertical and horizonal stripe states. We notice a peculiar phenomenon of density-phase separation in the sense that the variations of density and phase tend to be independent. In ferromagnetic case, the GS exhibits a semi-circular or half-disk status of density embedded with vortices and anti-vortices. The spin distribution can self-arrange into an array of half-skyrmions and we also find a half-antiskyrmion fence separating vortex-antivortex pairs. Our study indicates that one can manipulate the emergence of exotic quantum states via the interplay of the SOC, interaction and potential geometry and the abundant state variations might also provide potential resources for quantum metrology."
  },
  {
    "title": "Branching Bisimulation Learning",
    "url": "http://arxiv.org/abs/2504.12246v1",
    "arxiv_id": "2504.12246v1",
    "authors": [
      "Alessandro Abate",
      "Mirco Giacobbe",
      "Christian Micheletti",
      "Yannik Schnitzer"
    ],
    "published": "2025-04-16T16:52:07+00:00",
    "summary": "We introduce a bisimulation learning algorithm for non-deterministic transition systems. We generalise bisimulation learning to systems with bounded branching and extend its applicability to model checking branching-time temporal logic, while previously it was limited to deterministic systems and model checking linear-time properties. Our method computes a finite stutter-insensitive bisimulation quotient of the system under analysis, represented as a decision tree. We adapt the proof rule for well-founded bisimulations to an iterative procedure that trains candidate decision trees from sample transitions of the system, and checks their validity over the entire transition relation using SMT solving. This results in a new technology for model checking CTL* without the next-time operator. Our technique is sound, entirely automated, and yields abstractions that are succinct and effective for formal verification and system diagnostics. We demonstrate the efficacy of our method on diverse benchmarks comprising concurrent software, communication protocols and robotic scenarios. Our method performs comparably to mature tools in the special case of LTL model checking, and outperforms the state of the art in CTL and CTL* model checking for systems with very large and countably infinite state space."
  },
  {
    "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning",
    "url": "http://arxiv.org/abs/2504.11456v1",
    "arxiv_id": "2504.11456v1",
    "authors": [
      "Zhiwei He",
      "Tian Liang",
      "Jiahao Xu",
      "Qiuzhi Liu",
      "Xingyu Chen",
      "Yue Wang",
      "Linfeng Song",
      "Dian Yu",
      "Zhenwen Liang",
      "Wenxuan Wang",
      "Zhuosheng Zhang",
      "Rui Wang",
      "Zhaopeng Tu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "published": "2025-04-15T17:59:51+00:00",
    "summary": "The capacity for complex mathematical reasoning is a key benchmark for artificial intelligence. While reinforcement learning (RL) applied to LLMs shows promise, progress is significantly hindered by the lack of large-scale training data that is sufficiently challenging, possesses verifiable answer formats suitable for RL, and is free from contamination with evaluation benchmarks. To address these limitations, we introduce DeepMath-103K, a new, large-scale dataset comprising approximately 103K mathematical problems, specifically designed to train advanced reasoning models via RL. DeepMath-103K is curated through a rigorous pipeline involving source analysis, stringent decontamination against numerous benchmarks, and filtering for high difficulty (primarily Levels 5-9), significantly exceeding existing open resources in challenge. Each problem includes a verifiable final answer, enabling rule-based RL, and three distinct R1-generated solutions suitable for diverse training paradigms like supervised fine-tuning or distillation. Spanning a wide range of mathematical topics, DeepMath-103K promotes the development of generalizable reasoning. We demonstrate that models trained on DeepMath-103K achieve significant improvements on challenging mathematical benchmarks, validating its effectiveness. We release DeepMath-103K publicly to facilitate community progress in building more capable AI reasoning systems: https://github.com/zwhe99/DeepMath."
  },
  {
    "title": "A Clean Slate for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.11453v1",
    "arxiv_id": "2504.11453v1",
    "authors": [
      "Matthew Thomas Jackson",
      "Uljad Berdica",
      "Jarek Liesen",
      "Shimon Whiteson",
      "Jakob Nicolaus Foerster"
    ],
    "published": "2025-04-15T17:59:05+00:00",
    "summary": "Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches within a single, comprehensive hyperparameter space, enabling algorithm development in a shared hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https://github.com/EmptyJackson/unifloral."
  },
  {
    "title": "HeatSense: Intelligent Thermal Anomaly Detection for Securing NoC-Enabled MPSoCs",
    "url": "http://arxiv.org/abs/2504.11421v1",
    "arxiv_id": "2504.11421v1",
    "authors": [
      "Mahdi Hasanzadeh",
      "Kasem Khalil",
      "Cynthia Sturton",
      "Ahmad Patooghy"
    ],
    "published": "2025-04-15T17:36:53+00:00",
    "summary": "Multi-Processor System-on-Chips (MPSoCs) are highly vulnerable to thermal attacks that manipulate dynamic thermal management systems. To counter this, we propose an adaptive real-time monitoring mechanism that detects abnormal thermal patterns in chip tiles. Our design space exploration helped identify key thermal features for an efficient anomaly detection module to be implemented at routers of network-enabled MPSoCs. To minimize hardware overhead, we employ weighted moving average (WMA) calculations and bit-shift operations, ensuring a lightweight yet effective implementation. By defining a spectrum of abnormal behaviors, our system successfully detects and mitigates malicious temperature fluctuations, reducing severe cases from 3.00{\\deg}C to 1.9{\\deg}C. The anomaly detection module achieves up to 82% of accuracy in detecting thermal attacks, which is only 10-15% less than top-performing machine learning (ML) models like Random Forest. However, our approach reduces hardware usage by up to 75% for logic resources and 100% for specialized resources, making it significantly more efficient than ML-based solutions. This method provides a practical, low-cost solution for resource-constrained environments, ensuring resilience against thermal attacks while maintaining system performance."
  },
  {
    "title": "Embodied World Models Emerge from Navigational Task in Open-Ended Environments",
    "url": "http://arxiv.org/abs/2504.11419v1",
    "arxiv_id": "2504.11419v1",
    "authors": [
      "Li Jin",
      "Liu Jia"
    ],
    "published": "2025-04-15T17:35:13+00:00",
    "summary": "Understanding how artificial systems can develop spatial awareness and reasoning has long been a challenge in AI research. Traditional models often rely on passive observation, but embodied cognition theory suggests that deeper understanding emerges from active interaction with the environment. This study investigates whether neural networks can autonomously internalize spatial concepts through interaction, focusing on planar navigation tasks. Using Gated Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we show that agents can learn to encode spatial properties like direction, distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS) to model the agent-environment interaction as a closed dynamical system, revealing stable limit cycles that correspond to optimal navigation strategies. Ridge Representation allows us to map navigation paths into a fixed-dimensional behavioral space, enabling comparison with neural states. Canonical Correlation Analysis (CCA) confirms strong alignment between these representations, suggesting that the agent's neural states actively encode spatial knowledge. Intervention experiments further show that specific neural dimensions are causally linked to navigation performance. This work provides an approach to bridging the gap between action and perception in AI, offering new insights into building adaptive, interpretable models that can generalize across complex environments. The causal validation of neural representations also opens new avenues for understanding and controlling the internal mechanisms of AI systems, pushing the boundaries of how machines learn and reason in dynamic, real-world scenarios."
  },
  {
    "title": "A tutorial on simulating nonlinear behaviors of flexible structures with the discrete differential geometry (DDG) method",
    "url": "http://arxiv.org/abs/2504.11417v1",
    "arxiv_id": "2504.11417v1",
    "authors": [
      "Weicheng Huang",
      "Zhuonan Hao",
      "Jiahao Li",
      "Dezhong Tong",
      "Kexin Guo",
      "Yingchao Zhang",
      "Huajian Gao",
      "K. Jimmy Hsia",
      "Mingchao Liu"
    ],
    "published": "2025-04-15T17:33:06+00:00",
    "summary": "Flexible elastic structures, such as beams, rods, ribbons, plates, and shells, exhibit complex nonlinear dynamical behaviors that are central to a wide range of engineering and scientific applications, including soft robotics, deployable structures, and biomedical devices. While various numerical methods have been developed to simulate these behaviors, many conventional approaches struggle to simultaneously capture geometric and material nonlinearities, as well as nonlinear external interactions, particularly in highly deformable and dynamically evolving systems. The Discrete Differential Geometry (DDG) method has emerged as a robust and efficient numerical framework that intrinsically preserves geometric properties, accommodates material nonlinearity, and accurately models interactions with external environments and fields. By directly discretizing geometric and mechanical quantities, DDG provides an accurate, stable, and efficient approach to modeling flexible structures, addressing key limitations of traditional numerical methods. This tutorial provides a systematic introduction to the DDG method for simulating nonlinear behaviors in flexible structures. It covers DDG theory, simulation frameworks, and MATLAB implementation, with examples spanning dynamic systems, geometric and material nonlinearities, and external interactions like magnetics and fluids, culminating in practical insights and future directions. By offering a comprehensive and practical guide, together with open-source MATLAB code, this tutorial aims to facilitate the broader adoption of DDG-based numerical tools among researchers and engineers in computational mechanics, applied mathematics, and structural design."
  },
  {
    "title": "Decoupled Diffusion Sparks Adaptive Scene Generation",
    "url": "http://arxiv.org/abs/2504.10485v1",
    "arxiv_id": "2504.10485v1",
    "authors": [
      "Yunsong Zhou",
      "Naisheng Ye",
      "William Ljungbergh",
      "Tianyu Li",
      "Jiazhi Yang",
      "Zetong Yang",
      "Hongzi Zhu",
      "Christoffer Petersson",
      "Hongyang Li"
    ],
    "published": "2025-04-14T17:59:57+00:00",
    "summary": "Controllable scene generation could reduce the cost of diverse data collection substantially for autonomous driving. Prior works formulate the traffic layout generation as predictive progress, either by denoising entire sequences at once or by iteratively predicting the next frame. However, full sequence denoising hinders online reaction, while the latter's short-sighted next-frame prediction lacks precise goal-state guidance. Further, the learned model struggles to generate complex or challenging scenarios due to a large number of safe and ordinal driving behaviors from open datasets. To overcome these, we introduce Nexus, a decoupled scene generation framework that improves reactivity and goal conditioning by simulating both ordinal and challenging scenarios from fine-grained tokens with independent noise states. At the core of the decoupled pipeline is the integration of a partial noise-masking training strategy and a noise-aware schedule that ensures timely environmental updates throughout the denoising process. To complement challenging scenario generation, we collect a dataset consisting of complex corner cases. It covers 540 hours of simulated data, including high-risk interactions such as cut-in, sudden braking, and collision. Nexus achieves superior generation realism while preserving reactivity and goal orientation, with a 40% reduction in displacement error. We further demonstrate that Nexus improves closed-loop planning by 20% through data augmentation and showcase its capability in safety-critical data generation."
  },
  {
    "title": "Weight Ensembling Improves Reasoning in Language Models",
    "url": "http://arxiv.org/abs/2504.10478v1",
    "arxiv_id": "2504.10478v1",
    "authors": [
      "Xingyu Dang",
      "Christina Baek",
      "Kaiyue Wen",
      "Zico Kolter",
      "Aditi Raghunathan"
    ],
    "published": "2025-04-14T17:59:07+00:00",
    "summary": "We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades-off between bias and variance."
  },
  {
    "title": "Weight Ensembling Improves Reasoning in Language Models",
    "url": "http://arxiv.org/abs/2504.10478v2",
    "arxiv_id": "2504.10478v2",
    "authors": [
      "Xingyu Dang",
      "Christina Baek",
      "Kaiyue Wen",
      "Zico Kolter",
      "Aditi Raghunathan"
    ],
    "published": "2025-04-14T17:59:07+00:00",
    "summary": "We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades-off between bias and variance."
  },
  {
    "title": "Co-optimizing Physical Reconfiguration Parameters and Controllers for an Origami-inspired Reconfigurable Manipulator",
    "url": "http://arxiv.org/abs/2504.10474v1",
    "arxiv_id": "2504.10474v1",
    "authors": [
      "Zhe Chen",
      "Li Chen",
      "Hao Zhang",
      "Jianguo Zhao"
    ],
    "published": "2025-04-14T17:56:38+00:00",
    "summary": "Reconfigurable robots that can change their physical configuration post-fabrication have demonstrate their potential in adapting to different environments or tasks. However, it is challenging to determine how to optimally adjust reconfigurable parameters for a given task, especially when the controller depends on the robot's configuration. In this paper, we address this problem using a tendon-driven reconfigurable manipulator composed of multiple serially connected origami-inspired modules as an example. Under tendon actuation, these modules can achieve different shapes and motions, governed by joint stiffnesses (reconfiguration parameters) and the tendon displacements (control inputs). We leverage recent advances in co-optimization of design and control for robotic system to treat reconfiguration parameters as design variables and optimize them using reinforcement learning techniques. We first establish a forward model based on the minimum potential energy method to predict the shape of the manipulator under tendon actuations. Using the forward model as the environment dynamics, we then co-optimize the control policy (on the tendon displacements) and joint stiffnesses of the modules for goal reaching tasks while ensuring collision avoidance. Through co-optimization, we obtain optimized joint stiffness and the corresponding optimal control policy to enable the manipulator to accomplish the task that would be infeasible with fixed reconfiguration parameters (i.e., fixed joint stiffness). We envision the co-optimization framework can be extended to other reconfigurable robotic systems, enabling them to optimally adapt their configuration and behavior for diverse tasks and environments."
  },
  {
    "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
    "url": "http://arxiv.org/abs/2504.10458v1",
    "arxiv_id": "2504.10458v1",
    "authors": [
      "Xiaobo Xia",
      "Run Luo"
    ],
    "published": "2025-04-14T17:45:54+00:00",
    "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
  },
  {
    "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
    "url": "http://arxiv.org/abs/2504.10458v2",
    "arxiv_id": "2504.10458v2",
    "authors": [
      "Xiaobo Xia",
      "Run Luo"
    ],
    "published": "2025-04-14T17:45:54+00:00",
    "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
  },
  {
    "title": "Anchor Token Matching: Implicit Structure Locking for Training-free AR Image Editing",
    "url": "http://arxiv.org/abs/2504.10434v1",
    "arxiv_id": "2504.10434v1",
    "authors": [
      "Taihang Hu",
      "Linxuan Li",
      "Kai Wang",
      "Yaxing Wang",
      "Jian Yang",
      "Ming-Ming Cheng"
    ],
    "published": "2025-04-14T17:25:19+00:00",
    "summary": "Text-to-image generation has seen groundbreaking advancements with diffusion models, enabling high-fidelity synthesis and precise image editing through cross-attention manipulation. Recently, autoregressive (AR) models have re-emerged as powerful alternatives, leveraging next-token generation to match diffusion models. However, existing editing techniques designed for diffusion models fail to translate directly to AR models due to fundamental differences in structural control. Specifically, AR models suffer from spatial poverty of attention maps and sequential accumulation of structural errors during image editing, which disrupt object layouts and global consistency. In this work, we introduce Implicit Structure Locking (ISLock), the first training-free editing strategy for AR visual models. Rather than relying on explicit attention manipulation or fine-tuning, ISLock preserves structural blueprints by dynamically aligning self-attention patterns with reference images through the Anchor Token Matching (ATM) protocol. By implicitly enforcing structural consistency in latent space, our method ISLock enables structure-aware editing while maintaining generative autonomy. Extensive experiments demonstrate that ISLock achieves high-quality, structure-consistent edits without additional training and is superior or comparable to conventional editing techniques. Our findings pioneer the way for efficient and flexible AR-based image editing, further bridging the performance gap between diffusion and autoregressive generative models. The code will be publicly available at https://github.com/hutaiHang/ATM"
  },
  {
    "title": "BiFlex: A Passive Bimodal Stiffness Flexible Wrist for Manipulation in Unstructured Environments",
    "url": "http://arxiv.org/abs/2504.08706v1",
    "arxiv_id": "2504.08706v1",
    "authors": [
      "Gu-Cheol Jeong",
      "Stefano Dalla Gasperina",
      "Ashish D. Deshpande",
      "Lillian Chin",
      "Roberto Mart\u00edn-Mart\u00edn"
    ],
    "published": "2025-04-11T17:16:13+00:00",
    "summary": "Robotic manipulation in unstructured, humancentric environments poses a dual challenge: achieving the precision need for delicate free-space operation while ensuring safety during unexpected contact events. Traditional wrists struggle to balance these demands, often relying on complex control schemes or complicated mechanical designs to mitigate potential damage from force overload. In response, we present BiFlex, a flexible robotic wrist that uses a soft buckling honeycomb structure to provides a natural bimodal stiffness response. The higher stiffness mode enables precise household object manipulation, while the lower stiffness mode provides the compliance needed to adapt to external forces. We design BiFlex to maintain a fingertip deflection of less than 1 cm while supporting loads up to 500g and create a BiFlex wrist for many grippers, including Panda, Robotiq, and BaRiFlex. We validate BiFlex under several real-world experimental evaluations, including surface wiping, precise pick-and-place, and grasping under environmental constraints. We demonstrate that BiFlex simplifies control while maintaining precise object manipulation and enhanced safety in real-world applications."
  },
  {
    "title": "Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing",
    "url": "http://arxiv.org/abs/2504.08704v1",
    "arxiv_id": "2504.08704v1",
    "authors": [
      "Vinal Asodia",
      "Zhenhua Feng",
      "Saber Fallah"
    ],
    "published": "2025-04-11T17:11:21+00:00",
    "summary": "Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essential as it provides feedback for the learning algorithm to distinguish between desirable and undesirable behaviors, thereby improving policy performance. This paper presents a novel pipeline for generating human-aligned reward labels. The proposed approach addresses the challenge of absent reward signals in real-world datasets by generating labels that reflect human judgment and safety considerations. The pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios. The proposed pipeline is applied to an occluded pedestrian crossing scenario with varying levels of pedestrian traffic, using synthetic and simulation data. The results indicate that the generated reward labels closely match the simulation reward labels. When used to train the driving policy using Behavior Proximal Policy Optimisation, the results are competitive with other baselines. This demonstrates the effectiveness of our method in producing reliable and human-aligned reward signals, facilitating the training of autonomous driving systems through Reinforcement Learning outside of simulation environments and in alignment with human values."
  },
  {
    "title": "Preparation and coherent manipulation of toroidal moments in molecules",
    "url": "http://arxiv.org/abs/2504.08701v1",
    "arxiv_id": "2504.08701v1",
    "authors": [
      "Kieran Hymas",
      "Alessandro Soncini"
    ],
    "published": "2025-04-11T17:07:23+00:00",
    "summary": "Molecules with an odd number of electrons are expected to display paramagnetic behaviour in a uniform magnetic field. Instead, a vanishing magnetization is often measured in a family of lanthanide complexes known as Single Molecule Toroics. The anomaly can be explained in terms of degenerate quantum states in which electron spins and orbital currents give rise to time-odd and space-odd magnetic vortices known as toroidal moments, carrying a vanishing magnetic dipole. Resilient to stray magnetic fields and susceptible to electric manipulation, toroidal moments are sparking growing interest for applications in spintronics, magnonics, and photonics. While macroscopic toroidal moments have been observed in extended systems such as bulk low-dimensional non-collinear ferromagnets, theoretically predicted quantum toroidal states in molecules are yet to be observed, as it is currently unclear how to split degenerate states carrying counter-rotating vortices via existing experimental setups. Here we propose a realistic experimental protocol to polarize and observe molecular toroidal moments using pulsed microwave radiation. Modelling the spin-dynamics in a pulsed MW-field, we find that three resonant MW-pulses, delivered either sequentially or simultaneously, to a class of MDy$_6$ (M = Al$^{3+}$, Cr$^{3+}$) molecules consisting of coupled Dy$_3$ toroidal moieties, can selectively and coherently transfer population to a long-lived polarized toroidal state. The ensuing magneto-electric properties can then be used as a read-out mechanism. Our results provide a strategy to measure and coherently manipulate toroidal states in molecular systems, which is expected to trigger applications of molecular toroidal states to quantum technologies."
  },
  {
    "title": "Performance Evaluation of Trajectory Tracking Controllers for a Quadruped Robot Leg",
    "url": "http://arxiv.org/abs/2504.08698v1",
    "arxiv_id": "2504.08698v1",
    "authors": [
      "Hossein Shojaei",
      "Hamid Rahmanei",
      "Seyed Hossein Sadati"
    ],
    "published": "2025-04-11T17:04:53+00:00",
    "summary": "The complexities in the dynamic model of the legged robots make it necessary to utilize model-free controllers in the task of trajectory tracking. In This paper, an adaptive transpose Jacobian approach is proposed to deal with the dynamic model complexity, which utilizes an adaptive PI-algorithm to adjust the control gains. The performance of the proposed control algorithm is compared with the conventional transpose Jacobian and sliding mode control algorithms and evaluated by the root mean square of the errors and control input energy criteria. In order to appraise the effectiveness of the proposed control system, simulations are carried out in MATLAB/Simulink software for a quadruped robot leg for semi-elliptical path tracking. The obtained results show that the proposed adaptive transpose Jacobian reduces the overshoot and root mean square of the errors and at the same time, decreases the control input energy. Moreover, transpose Jacobin and adaptive transpose Jacobian are more robust to changes in initial conditions compared to the conventional sliding mode control. Furthermore, sliding mode control performs well up to 20% uncertainties in the parameters due to its model-based nature, whereas the transpose Jacobin and the proposed adaptive transpose Jacobian algorithms show promising results even in higher mass uncertainties."
  },
  {
    "title": "Pobogot -- An Open-Hardware Open-Source Low Cost Robot for Swarm Robotics",
    "url": "http://arxiv.org/abs/2504.08686v1",
    "arxiv_id": "2504.08686v1",
    "authors": [
      "Alessia Loi",
      "Loona Macabre",
      "J\u00e9r\u00e9my Fersula",
      "Keivan Amini",
      "Leo Cazenille",
      "Fabien Caura",
      "Alexandre Guerre",
      "St\u00e9phane Gourichon",
      "Olivier Dauchot",
      "Nicolas Bredeche"
    ],
    "published": "2025-04-11T16:47:59+00:00",
    "summary": "This paper describes the Pogobot, an open-source and open-hardware platform specifically designed for research involving swarm robotics. Pogobot features vibration-based locomotion, infrared communication, and an array of sensors in a cost-effective package (approx. 250~euros/unit). The platform's modular design, comprehensive API, and extensible architecture facilitate the implementation of swarm intelligence algorithms and distributed online reinforcement learning algorithms. Pogobots offer an accessible alternative to existing platforms while providing advanced capabilities including directional communication between units. More than 200 Pogobots are already being used on a daily basis at Sorbonne Universit\\'e and PSL to study self-organizing systems, programmable active matter, discrete reaction-diffusion-advection systems as well as models of social learning and evolution."
  },
  {
    "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.07954v1",
    "arxiv_id": "2504.07954v1",
    "authors": [
      "En Yu",
      "Kangheng Lin",
      "Liang Zhao",
      "Jisheng Yin",
      "Yana Wei",
      "Yuang Peng",
      "Haoran Wei",
      "Jianjian Sun",
      "Chunrui Han",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Daxin Jiang",
      "Jingyu Wang",
      "Wenbing Tao"
    ],
    "published": "2025-04-10T17:58:27+00:00",
    "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in MLLM post-training for perception policy learning. While promising, our initial experiments reveal that incorporating a thinking process through RL does not consistently lead to performance gains across all visual perception tasks. This leads us to delve into the essential role of RL in the context of visual perception. In this work, we return to the fundamentals and explore the effects of RL on different perception tasks. We observe that the perceptual complexity is a major factor in determining the effectiveness of RL. We also observe that reward design plays a crucial role in further approching the upper limit of model perception. To leverage these findings, we propose Perception-R1, a scalable RL framework using GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct, Perception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on PageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing a strong baseline for perception policy learning."
  },
  {
    "title": "Echo: An Open-Source, Low-Cost Teleoperation System with Force Feedback for Dataset Collection in Robot Learning",
    "url": "http://arxiv.org/abs/2504.07939v1",
    "arxiv_id": "2504.07939v1",
    "authors": [
      "Artem Bazhenov",
      "Sergei Satsevich",
      "Sergei Egorov",
      "Farit Khabibullin",
      "Dzmitry Tsetserukou"
    ],
    "published": "2025-04-10T17:51:37+00:00",
    "summary": "In this article, we propose Echo, a novel joint-matching teleoperation system designed to enhance the collection of datasets for manual and bimanual tasks. Our system is specifically tailored for controlling the UR manipulator and features a custom controller with force feedback and adjustable sensitivity modes, enabling precise and intuitive operation. Additionally, Echo integrates a user-friendly dataset recording interface, simplifying the process of collecting high-quality training data for imitation learning. The system is designed to be reliable, cost-effective, and easily reproducible, making it an accessible tool for researchers, laboratories, and startups passionate about advancing robotics through imitation learning. Although the current implementation focuses on the UR manipulator, Echo architecture is reconfigurable and can be adapted to other manipulators and humanoid systems. We demonstrate the effectiveness of Echo through a series of experiments, showcasing its ability to perform complex bimanual tasks and its potential to accelerate research in the field. We provide assembly instructions, a hardware description, and code at https://eterwait.github.io/Echo/."
  },
  {
    "title": "Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining",
    "url": "http://arxiv.org/abs/2504.07912v1",
    "arxiv_id": "2504.07912v1",
    "authors": [
      "Rosie Zhao",
      "Alexandru Meterez",
      "Sham Kakade",
      "Cengiz Pehlevan",
      "Samy Jelassi",
      "Eran Malach"
    ],
    "published": "2025-04-10T17:15:53+00:00",
    "summary": "Reinforcement learning (RL)-based fine-tuning has become a crucial step in post-training language models for advanced mathematical reasoning and coding. Following the success of frontier reasoning models, recent work has demonstrated that RL fine-tuning consistently improves performance, even in smaller-scale models; however, the underlying mechanisms driving these improvements are not well-understood. Understanding the effects of RL fine-tuning requires disentangling its interaction with pretraining data composition, hyperparameters, and model scale, but such problems are exacerbated by the lack of transparency regarding the training data used in many existing models. In this work, we present a systematic end-to-end study of RL fine-tuning for mathematical reasoning by training models entirely from scratch on different mixtures of fully open datasets. We investigate the effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration) across models of different scales. Our study reveals that RL algorithms consistently converge towards a dominant output distribution, amplifying patterns in the pretraining data. We also find that models of different scales trained on the same data mixture will converge to distinct output distributions, suggesting that there are scale-dependent biases in model generalization. Moreover, we find that RL post-training on simpler questions can lead to performance gains on harder ones, indicating that certain reasoning capabilities generalize across tasks. Our findings show that small-scale proxies in controlled settings can elicit interesting insights regarding the role of RL in shaping language model behavior."
  },
  {
    "title": "Fast Adaptation with Behavioral Foundation Models",
    "url": "http://arxiv.org/abs/2504.07896v1",
    "arxiv_id": "2504.07896v1",
    "authors": [
      "Harshit Sikchi",
      "Andrea Tirinzoni",
      "Ahmed Touati",
      "Yingchen Xu",
      "Anssi Kanervisto",
      "Scott Niekum",
      "Amy Zhang",
      "Alessandro Lazaric",
      "Matteo Pirotta"
    ],
    "published": "2025-04-10T16:14:17+00:00",
    "summary": "Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful paradigm for pretraining behavioral foundation models (BFMs), enabling agents to solve a wide range of downstream tasks specified via reward functions in a zero-shot fashion, i.e., without additional test-time learning or planning. This is achieved by learning self-supervised task embeddings alongside corresponding near-optimal behaviors and incorporating an inference procedure to directly retrieve the latent task embedding and associated policy for any given reward function. Despite promising results, zero-shot policies are often suboptimal due to errors induced by the unsupervised training process, the embedding, and the inference procedure. In this paper, we focus on devising fast adaptation strategies to improve the zero-shot performance of BFMs in a few steps of online interaction with the environment while avoiding any performance drop during the adaptation process. Notably, we demonstrate that existing BFMs learn a set of skills containing more performant policies than those identified by their inference procedure, making them well-suited for fast adaptation. Motivated by this observation, we propose both actor-critic and actor-only fast adaptation strategies that search in the low-dimensional task-embedding space of the pre-trained BFM to rapidly improve the performance of its zero-shot policies on any downstream task. Notably, our approach mitigates the initial \"unlearning\" phase commonly observed when fine-tuning pre-trained RL models. We evaluate our fast adaptation strategies on top of four state-of-the-art zero-shot RL methods in multiple navigation and locomotion domains. Our results show that they achieve 10-40% improvement over their zero-shot performance in a few tens of episodes, outperforming existing baselines."
  },
  {
    "title": "The Role of Buffer Gas in Shaping the D1 Line Spectrum of Potassium Vapour",
    "url": "http://arxiv.org/abs/2504.07888v1",
    "arxiv_id": "2504.07888v1",
    "authors": [
      "Sharaa A. Alqarni",
      "Danielle Pizzey",
      "Steven A Wrathmall",
      "Ifan G Hughes"
    ],
    "published": "2025-04-10T16:01:57+00:00",
    "summary": "In this study, we investigate the effect of buffer gas and magnetic field on the spectral line shapes of the potassium D1 transition using sealed vapour cells filled with varying amounts of neon as a buffer gas. Employing a dual-temperature control system, we independently manipulate the cell body and stem temperatures to explore Doppler and collisional effects on the spectrum. Our results show how the Voigt spectral profile changes from Gaussian- to Lorentzian-dominated forms due to pressure broadening and shifts caused by collisions between potassium atoms and neon. Our measurements are in excellent agreement with the literature values for potassium-neon collisions. For the first time we were able to incorporate the buffer-gas shift and broadening into the modified Voigt profile via the ElecSus code, and found excellent agreement between the predicted and measured line profiles. We also analyse the potassium D1 spectral lines in the hyperfine Paschen-Back regime using strong magnetic fields, demonstrating how Zeeman splitting modifies the pressure-broadened line shape. This work provides valuable insights into collision-induced broadening and shifts, enhancing our understanding of potassium spectroscopy and its application in the development of advanced magneto-optical filters for solar physics and other applications."
  },
  {
    "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
    "url": "http://arxiv.org/abs/2504.07097v1",
    "arxiv_id": "2504.07097v1",
    "authors": [
      "Nikhil Shivakumar Nayak",
      "Krishnateja Killamsetty",
      "Ligong Han",
      "Abhishek Bhandwaldar",
      "Prateek Chanda",
      "Kai Xu",
      "Hao Wang",
      "Aldo Pareja",
      "Oleg Silkin",
      "Mustafa Eyceoz",
      "Akash Srivastava"
    ],
    "published": "2025-04-09T17:59:42+00:00",
    "summary": "Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models."
  },
  {
    "title": "Neural Motion Simulator: Pushing the Limit of World Models in Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.07095v1",
    "arxiv_id": "2504.07095v1",
    "authors": [
      "Chenjie Hao",
      "Weyl Lu",
      "Yifan Xu",
      "Yubei Chen"
    ],
    "published": "2025-04-09T17:59:32+00:00",
    "summary": "An embodied system must not only model the patterns of the external world but also understand its own motion dynamics. A motion dynamic model is essential for efficient skill acquisition and effective planning. In this work, we introduce the neural motion simulator (MoSim), a world model that predicts the future physical state of an embodied system based on current observations and actions. MoSim achieves state-of-the-art performance in physical state prediction and provides competitive performance across a range of downstream tasks. This works shows that when a world model is accurate enough and performs precise long-horizon predictions, it can facilitate efficient skill acquisition in imagined worlds and even enable zero-shot reinforcement learning. Furthermore, MoSim can transform any model-free reinforcement learning (RL) algorithm into a model-based approach, effectively decoupling physical environment modeling from RL algorithm development. This separation allows for independent advancements in RL algorithms and world modeling, significantly improving sample efficiency and enhancing generalization capabilities. Our findings highlight that world models for motion dynamics is a promising direction for developing more versatile and capable embodied systems."
  },
  {
    "title": "FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution",
    "url": "http://arxiv.org/abs/2504.07093v1",
    "arxiv_id": "2504.07093v1",
    "authors": [
      "Gene Chou",
      "Wenqi Xian",
      "Guandao Yang",
      "Mohamed Abdelfattah",
      "Bharath Hariharan",
      "Noah Snavely",
      "Ning Yu",
      "Paul Debevec"
    ],
    "published": "2025-04-09T17:59:31+00:00",
    "summary": "A versatile video depth estimation model should (1) be accurate and consistent across frames, (2) produce high-resolution depth maps, and (3) support real-time streaming. We propose FlashDepth, a method that satisfies all three requirements, performing depth estimation on a 2044x1148 streaming video at 24 FPS. We show that, with careful modifications to pretrained single-image depth models, these capabilities are enabled with relatively little data and training. We evaluate our approach across multiple unseen datasets against state-of-the-art depth models, and find that ours outperforms them in terms of boundary sharpness and speed by a significant margin, while maintaining competitive accuracy. We hope our model will enable various applications that require high-resolution depth, such as video editing, and online decision-making, such as robotics."
  },
  {
    "title": "AssistanceZero: Scalably Solving Assistance Games",
    "url": "http://arxiv.org/abs/2504.07091v1",
    "arxiv_id": "2504.07091v1",
    "authors": [
      "Cassidy Laidlaw",
      "Eli Bronstein",
      "Timothy Guo",
      "Dylan Feng",
      "Lukas Berglund",
      "Justin Svegliato",
      "Stuart Russell",
      "Anca Dragan"
    ],
    "published": "2025-04-09T17:59:03+00:00",
    "summary": "Assistance games are a promising alternative to reinforcement learning from human feedback (RLHF) for training AI assistants. Assistance games resolve key drawbacks of RLHF, such as incentives for deceptive behavior, by explicitly modeling the interaction between assistant and user as a two-player game where the assistant cannot observe their shared goal. Despite their potential, assistance games have only been explored in simple settings. Scaling them to more complex environments is difficult because it requires both solving intractable decision-making problems under uncertainty and accurately modeling human users' behavior. We present the first scalable approach to solving assistance games and apply it to a new, challenging Minecraft-based assistance game with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends AlphaZero with a neural network that predicts human actions and rewards, enabling it to plan under uncertainty. We show that AssistanceZero outperforms model-free RL algorithms and imitation learning in the Minecraft-based assistance game. In a human study, our AssistanceZero-trained assistant significantly reduces the number of actions participants take to complete building tasks in Minecraft. Our results suggest that assistance games are a tractable framework for training effective AI assistants in complex environments. Our code and models are available at https://github.com/cassidylaidlaw/minecraft-building-assistance-game."
  },
  {
    "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility",
    "url": "http://arxiv.org/abs/2504.07086v1",
    "arxiv_id": "2504.07086v1",
    "authors": [
      "Andreas Hochlehnert",
      "Hardik Bhatnagar",
      "Vishaal Udandarao",
      "Samuel Albanie",
      "Ameya Prabhu",
      "Matthias Bethge"
    ],
    "published": "2025-04-09T17:58:17+00:00",
    "summary": "Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work."
  },
  {
    "title": "Underwater Robotic Simulators Review for Autonomous System Development",
    "url": "http://arxiv.org/abs/2504.06245v1",
    "arxiv_id": "2504.06245v1",
    "authors": [
      "Sara Aldhaheri",
      "Yang Hu",
      "Yongchang Xie",
      "Peng Wu",
      "Dimitrios Kanoulas",
      "Yuanchang Liu"
    ],
    "published": "2025-04-08T17:43:48+00:00",
    "summary": "The increasing complexity of underwater robotic systems has led to a surge in simulation platforms designed to support perception, planning, and control tasks in marine environments. However, selecting the most appropriate underwater robotic simulator (URS) remains a challenge due to wide variations in fidelity, extensibility, and task suitability. This paper presents a comprehensive review and comparative analysis of five state-of-the-art, ROS-compatible, open-source URSs: Stonefish, DAVE, HoloOcean, MARUS, and UNav-Sim. Each simulator is evaluated across multiple criteria including sensor fidelity, environmental realism, sim-to-real capabilities, and research impact. We evaluate them across architectural design, sensor and physics modeling, task capabilities, and research impact. Additionally, we discuss ongoing challenges in sim-to-real transfer and highlight the need for standardization and benchmarking in the field. Our findings aim to guide practitioners in selecting effective simulation environments and inform future development of more robust and transferable URSs."
  },
  {
    "title": "Addressing Relative Degree Issues in Control Barrier Function Synthesis with Physics-Informed Neural Networks",
    "url": "http://arxiv.org/abs/2504.06242v1",
    "arxiv_id": "2504.06242v1",
    "authors": [
      "Lukas Brunke",
      "Siqi Zhou",
      "Francesco D'Orazio",
      "Angela P. Schoellig"
    ],
    "published": "2025-04-08T17:41:43+00:00",
    "summary": "In robotics, control barrier function (CBF)-based safety filters are commonly used to enforce state constraints. A critical challenge arises when the relative degree of the CBF varies across the state space. This variability can create regions within the safe set where the control input becomes unconstrained. When implemented as a safety filter, this may result in chattering near the safety boundary and ultimately compromise system safety. To address this issue, we propose a novel approach for CBF synthesis by formulating it as solving a set of boundary value problems. The solutions to the boundary value problems are determined using physics-informed neural networks (PINNs). Our approach ensures that the synthesized CBFs maintain a constant relative degree across the set of admissible states, thereby preventing unconstrained control scenarios. We illustrate the approach in simulation and further verify it through real-world quadrotor experiments, demonstrating its effectiveness in preserving desired system safety properties."
  },
  {
    "title": "Dictionary-free Koopman Predictive Control for Autonomous Vehicles in Mixed Traffic",
    "url": "http://arxiv.org/abs/2504.06240v1",
    "arxiv_id": "2504.06240v1",
    "authors": [
      "Xu Shang",
      "Zhaojian Li",
      "Yang Zheng"
    ],
    "published": "2025-04-08T17:40:54+00:00",
    "summary": "Koopman Model Predictive Control (KMPC) and Data-EnablEd Predictive Control (DeePC) use linear models to approximate nonlinear systems and integrate them with predictive control. Both approaches have recently demonstrated promising performance in controlling Connected and Autonomous Vehicles (CAVs) in mixed traffic. However, selecting appropriate lifting functions for the Koopman operator in KMPC is challenging, while the data-driven representation from Willems' fundamental lemma in DeePC must be updated to approximate the local linearization when the equilibrium traffic state changes. In this paper, we propose a dictionary-free Koopman model predictive control (DF-KMPC) for CAV control. In particular, we first introduce a behavioral perspective to identify the optimal dictionary-free Koopman linear model. We then utilize an iterative algorithm to compute a data-driven approximation of the dictionary-free Koopman representation. Integrating this data-driven linear representation with predictive control leads to our DF-KMPC, which eliminates the need to select lifting functions and update the traffic equilibrium state. Nonlinear traffic simulations show that DF-KMPC effectively mitigates traffic waves and improves tracking performance."
  },
  {
    "title": "Accessible and Pedagogically-Grounded Explainability for Human-Robot Interaction: A Framework Based on UDL and Symbolic Interfaces",
    "url": "http://arxiv.org/abs/2504.06189v1",
    "arxiv_id": "2504.06189v1",
    "authors": [
      "Francisco J. Rodr\u00edguez Lera",
      "Raquel Fern\u00e1ndez Hern\u00e1ndez",
      "Sonia Lopez Gonz\u00e1lez",
      "Miguel Angel Gonz\u00e1lez-Santamarta",
      "Francisco Jes\u00fas Rodr\u00edguez Sedano",
      "Camino Fernandez Llamas"
    ],
    "published": "2025-04-08T16:33:52+00:00",
    "summary": "This paper presents a novel framework for accessible and pedagogically-grounded robot explainability, designed to support human-robot interaction (HRI) with users who have diverse cognitive, communicative, or learning needs. We combine principles from Universal Design for Learning (UDL) and Universal Design (UD) with symbolic communication strategies to facilitate the alignment of mental models between humans and robots. Our approach employs Asterics Grid and ARASAAC pictograms as a multimodal, interpretable front-end, integrated with a lightweight HTTP-to-ROS 2 bridge that enables real-time interaction and explanation triggering. We emphasize that explainability is not a one-way function but a bidirectional process, where human understanding and robot transparency must co-evolve. We further argue that in educational or assistive contexts, the role of a human mediator (e.g., a teacher) may be essential to support shared understanding. We validate our framework with examples of multimodal explanation boards and discuss how it can be extended to different scenarios in education, assistive robotics, and inclusive AI."
  },
  {
    "title": "Plug and Play Distributed Control of Clustered Energy Hub Networks",
    "url": "http://arxiv.org/abs/2504.06179v1",
    "arxiv_id": "2504.06179v1",
    "authors": [
      "Varsha Behrunani",
      "Cara Koepele",
      "Jared Miller",
      "Ahmed Aboudonia",
      "Philipp Heer",
      "Roy S. Smith",
      "John Lygeros"
    ],
    "published": "2025-04-08T16:21:23+00:00",
    "summary": "The transition to renewable energy is driving the rise of distributed multi-energy systems, in which individual energy hubs and prosumers (e.g., homes, industrial campuses) generate, store, and trade energy. Economic Model Predictive Control (MPC) schemes are widely used to optimize operation of energy hubs by efficiently dispatching resources and minimizing costs while ensuring operational constraints are met. Peer-to-peer (P2P) energy trading among hubs enhances network efficiency and reduces costs but also increases computational and privacy challenges, especially as the network scales. Additionally, current distributed control techniques require global recomputation whenever the network topology changes, limiting scalability. To address these challenges, we propose a clustering-based P2P trading framework that enables plug-and-play operation, allowing energy hubs to seamlessly join or leave without requiring network-wide controller updates. The impact is restricted to the hubs within the affected cluster. The energy trading problem is formulated as a bi-level bargaining game, where inter-cluster trading commitments are determined at the cluster level, while energy dispatch and cost-sharing among hubs within a cluster are refined at the hub level. Both levels are solved in a distributed manner using ADMM, ensuring computational feasibility and privacy preservation. Moreover, we develop plug-and-play procedures to handle dynamic topology changes at both the hub and cluster levels, minimizing disruptions across the network. Simulation results demonstrate that the proposed bi-level framework reduces operational costs, and enables scalable energy management under plug-and-play operation."
  },
  {
    "title": "Ionomeric extracellular matrices for dynamic soft robotic tissue engineering devices through protein sulfonation",
    "url": "http://arxiv.org/abs/2504.05302v1",
    "arxiv_id": "2504.05302v1",
    "authors": [
      "Matthew K Burgess",
      "Ryan T Murray",
      "Veronica M Lucian",
      "Zekun Liu",
      "Robin O Cleveland",
      "Callum J Beeston",
      "Malavika Nair"
    ],
    "published": "2025-04-07T17:59:11+00:00",
    "summary": "Conventional tissue engineering methodologies frequently depend on pharmacological strategies to induce or expedite tissue repair. However, bioengineered strategies incorporating biophysical stimulation have emerged as promising alternatives. Electroactive materials facilitate the provision of controlled electrical, mechanical, and electromechanical stimuli, which support cell proliferation and tissue remodelling. Despite their ability to supply external electrical and mechanical stimuli to the tissue microenvironment, the electroactive polymers in use today often lack critical biochemical signals essential for native-like cell-cell and cell-scaffold interactions, thereby constraining their regenerative capabilities. To address the demand for biomimetic materials that possess enhanced capabilities in promoting cell and tissue stimulation, we present the development of a novel class of polymers called ionomeric extracellular matrices (iECMs). By utilising the linker-mediated conjugation of sulfonic acid biomolecules (taurine) to the backbone of an extracellular matrix protein (collagen), we illustrate the potential of iECMs as the first electromechanical actuating material platform derived entirely from ECM materials, paving the way for dynamic and soft-robotic platforms for a wide range of tissue engineering applications."
  },
  {
    "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations",
    "url": "http://arxiv.org/abs/2504.05294v1",
    "arxiv_id": "2504.05294v1",
    "authors": [
      "Pedro Ferreira",
      "Wilker Aziz",
      "Ivan Titov"
    ],
    "published": "2025-04-07T17:49:23+00:00",
    "summary": "Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization - a key step in the alignment phase - can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model's internal decision process and the generated explanation. Consequently, the LLM may engage in \"reward hacking\" by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM's input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations."
  },
  {
    "title": "Using Physiological Measures, Gaze, and Facial Expressions to Model Human Trust in a Robot Partner",
    "url": "http://arxiv.org/abs/2504.05291v1",
    "arxiv_id": "2504.05291v1",
    "authors": [
      "Haley N. Green",
      "Tariq Iqbal"
    ],
    "published": "2025-04-07T17:45:17+00:00",
    "summary": "With robots becoming increasingly prevalent in various domains, it has become crucial to equip them with tools to achieve greater fluency in interactions with humans. One of the promising areas for further exploration lies in human trust. A real-time, objective model of human trust could be used to maximize productivity, preserve safety, and mitigate failure. In this work, we attempt to use physiological measures, gaze, and facial expressions to model human trust in a robot partner. We are the first to design an in-person, human-robot supervisory interaction study to create a dedicated trust dataset. Using this dataset, we train machine learning algorithms to identify the objective measures that are most indicative of trust in a robot partner, advancing trust prediction in human-robot interactions. Our findings indicate that a combination of sensor modalities (blood volume pulse, electrodermal activity, skin temperature, and gaze) can enhance the accuracy of detecting human trust in a robot partner. Furthermore, the Extra Trees, Random Forest, and Decision Trees classifiers exhibit consistently better performance in measuring the person's trust in the robot partner. These results lay the groundwork for constructing a real-time trust model for human-robot interaction, which could foster more efficient interactions between humans and robots."
  },
  {
    "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from Single-view Perception",
    "url": "http://arxiv.org/abs/2504.05287v1",
    "arxiv_id": "2504.05287v1",
    "authors": [
      "Hui Zhang",
      "Zijian Wu",
      "Linyi Huang",
      "Sammy Christen",
      "Jie Song"
    ],
    "published": "2025-04-07T17:38:19+00:00",
    "summary": "Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/"
  },
  {
    "title": "Manipulating phases in many-body interacting systems with subsystem resetting",
    "url": "http://arxiv.org/abs/2504.05260v1",
    "arxiv_id": "2504.05260v1",
    "authors": [
      "Anish Acharya",
      "Rupak Majumder",
      "Shamik Gupta"
    ],
    "published": "2025-04-07T16:53:44+00:00",
    "summary": "Stabilizing thermodynamically unstable phases in many-body systems -- such as suppressing pathological neuronal synchronization in Parkinson's disease or maintaining magnetic order across broad temperature ranges -- remains a persistent challenge. In traditional approaches, such phases are stabilized through intervening in the dynamics of all system constituents or introducing additional interactions. Here, we offer a hitherto-unexplored alternative -- subsystem resetting, whereby intervention in the dynamics of only a part of the system, and that too only occasionally in time, is implemented through resetting its state to a reset configuration. Just playing with a few parameters, e.g., the nature of the reset configuration and the size of the reset subsystem, one achieves a remarkable and robust control over the phase diagram of the bare dynamics. We demonstrate that these universal effects span a wide variety of scenarios, including equilibrium and non-equilibrium, mean-field and non-mean-field dynamics, with and without quenched disorder. Despite the challenges posed by memory effects, we obtain explicit analytical predictions, validated by simulations."
  },
  {
    "title": "SeGuE: Semantic Guided Exploration for Mobile Robots",
    "url": "http://arxiv.org/abs/2504.03629v1",
    "arxiv_id": "2504.03629v1",
    "authors": [
      "Cody Simons",
      "Aritra Samanta",
      "Amit K. Roy-Chowdhury",
      "Konstantinos Karydis"
    ],
    "published": "2025-04-04T17:46:45+00:00",
    "summary": "The rise of embodied AI applications has enabled robots to perform complex tasks which require a sophisticated understanding of their environment. To enable successful robot operation in such settings, maps must be constructed so that they include semantic information, in addition to geometric information. In this paper, we address the novel problem of semantic exploration, whereby a mobile robot must autonomously explore an environment to fully map both its structure and the semantic appearance of features. We develop a method based on next-best-view exploration, where potential poses are scored based on the semantic features visible from that pose. We explore two alternative methods for sampling potential views and demonstrate the effectiveness of our framework in both simulation and physical experiments. Automatic creation of high-quality semantic maps can enable robots to better understand and interact with their environments and enable future embodied AI applications to be more easily deployed."
  },
  {
    "title": "Align to Structure: Aligning Large Language Models with Structural Information",
    "url": "http://arxiv.org/abs/2504.03622v1",
    "arxiv_id": "2504.03622v1",
    "authors": [
      "Zae Myung Kim",
      "Anand Ramachandran",
      "Farideh Tavazoee",
      "Joo-Kyung Kim",
      "Oleg Rokhlenko",
      "Dongyeop Kang"
    ],
    "published": "2025-04-04T17:40:04+00:00",
    "summary": "Generating long, coherent text remains a challenge for large language models (LLMs), as they lack hierarchical planning and structured organization in discourse generation. We introduce Structural Alignment, a novel method that aligns LLMs with human-like discourse structures to enhance long-form text generation. By integrating linguistically grounded discourse frameworks into reinforcement learning, our approach guides models to produce coherent and well-organized outputs. We employ a dense reward scheme within a Proximal Policy Optimization framework, assigning fine-grained, token-level rewards based on the discourse distinctiveness relative to human writing. Two complementary reward models are evaluated: the first improves readability by scoring surface-level textual features to provide explicit structuring, while the second reinforces deeper coherence and rhetorical sophistication by analyzing global discourse patterns through hierarchical discourse motifs, outperforming both standard and RLHF-enhanced models in tasks such as essay generation and long-document summarization. All training data and code will be publicly shared at https://github.com/minnesotanlp/struct_align."
  },
  {
    "title": "Optimization of a Triangular Delaunay Mesh Generator using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.03610v1",
    "arxiv_id": "2504.03610v1",
    "authors": [
      "Will Thacher",
      "Per-Olof Persson",
      "Yulong Pan"
    ],
    "published": "2025-04-04T17:30:50+00:00",
    "summary": "In this work we introduce a triangular Delaunay mesh generator that can be trained using reinforcement learning to maximize a given mesh quality metric. Our mesh generator consists of a graph neural network that distributes and modifies vertices, and a standard Delaunay algorithm to triangulate the vertices. We explore various design choices and evaluate our mesh generator on various tasks including mesh generation, mesh improvement, and producing variable resolution meshes. The learned mesh generator outputs meshes that are comparable to those produced by Triangle and DistMesh, two popular Delaunay-based mesh generators."
  },
  {
    "title": "Robust Human Registration with Body Part Segmentation on Noisy Point Clouds",
    "url": "http://arxiv.org/abs/2504.03602v1",
    "arxiv_id": "2504.03602v1",
    "authors": [
      "Kai Lascheit",
      "Daniel Barath",
      "Marc Pollefeys",
      "Leonidas Guibas",
      "Francis Engelmann"
    ],
    "published": "2025-04-04T17:17:33+00:00",
    "summary": "Registering human meshes to 3D point clouds is essential for applications such as augmented reality and human-robot interaction but often yields imprecise results due to noise and background clutter in real-world data. We introduce a hybrid approach that incorporates body-part segmentation into the mesh fitting process, enhancing both human pose estimation and segmentation accuracy. Our method first assigns body part labels to individual points, which then guide a two-step SMPL-X fitting: initial pose and orientation estimation using body part centroids, followed by global refinement of the point cloud alignment. Additionally, we demonstrate that the fitted human mesh can refine body part labels, leading to improved segmentation. Evaluations on the cluttered and noisy real-world datasets InterCap, EgoBody, and BEHAVE show that our approach significantly outperforms prior methods in both pose estimation and segmentation accuracy. Code and results are available on our project website: https://segfit.github.io"
  },
  {
    "title": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin for Real-World Robot Policy Evaluation",
    "url": "http://arxiv.org/abs/2504.03597v1",
    "arxiv_id": "2504.03597v1",
    "authors": [
      "Jad Abou-Chakra",
      "Lingfeng Sun",
      "Krishan Rana",
      "Brandon May",
      "Karl Schmeckpeper",
      "Maria Vittoria Minniti",
      "Laura Herlant"
    ],
    "published": "2025-04-04T17:05:56+00:00",
    "summary": "Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com."
  },
  {
    "title": "Concept Lancet: Image Editing with Compositional Representation Transplant",
    "url": "http://arxiv.org/abs/2504.02828v1",
    "arxiv_id": "2504.02828v1",
    "authors": [
      "Jinqi Luo",
      "Tianjiao Ding",
      "Kwan Ho Ryan Chan",
      "Hancheng Min",
      "Chris Callison-Burch",
      "Ren\u00e9 Vidal"
    ],
    "published": "2025-04-03T17:59:58+00:00",
    "summary": "Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation."
  },
  {
    "title": "Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence",
    "url": "http://arxiv.org/abs/2504.02799v1",
    "arxiv_id": "2504.02799v1",
    "authors": [
      "Anita Rau",
      "Mark Endo",
      "Josiah Aklilu",
      "Jaewoo Heo",
      "Khaled Saab",
      "Alberto Paderno",
      "Jeffrey Jopling",
      "F. Christopher Holsinger",
      "Serena Yeung-Levy"
    ],
    "published": "2025-04-03T17:42:56+00:00",
    "summary": "Large Vision-Language Models offer a new paradigm for AI-driven image understanding, enabling models to perform tasks without task-specific training. This flexibility holds particular promise across medicine, where expert-annotated data is scarce. Yet, VLMs' practical utility in intervention-focused domains--especially surgery, where decision-making is subjective and clinical scenarios are variable--remains uncertain. Here, we present a comprehensive analysis of 11 state-of-the-art VLMs across 17 key visual understanding tasks in surgical AI--from anatomy recognition to skill assessment--using 13 datasets spanning laparoscopic, robotic, and open procedures. In our experiments, VLMs demonstrate promising generalizability, at times outperforming supervised models when deployed outside their training setting. In-context learning, incorporating examples during testing, boosted performance up to three-fold, suggesting adaptability as a key strength. Still, tasks requiring spatial or temporal reasoning remained difficult. Beyond surgery, our findings offer insights into VLMs' potential for tackling complex and dynamic scenarios in clinical and broader real-world applications."
  },
  {
    "title": "Spline-based Transformers",
    "url": "http://arxiv.org/abs/2504.02797v1",
    "arxiv_id": "2504.02797v1",
    "authors": [
      "Prashanth Chandran",
      "Agon Serifi",
      "Markus Gross",
      "Moritz B\u00e4cher"
    ],
    "published": "2025-04-03T17:42:07+00:00",
    "summary": "We introduce Spline-based Transformers, a novel class of Transformer models that eliminate the need for positional encoding. Inspired by workflows using splines in computer animation, our Spline-based Transformers embed an input sequence of elements as a smooth trajectory in latent space. Overcoming drawbacks of positional encoding such as sequence length extrapolation, Spline-based Transformers also provide a novel way for users to interact with transformer latent spaces by directly manipulating the latent control points to create new latent trajectories and sequences. We demonstrate the superior performance of our approach in comparison to conventional positional encoding on a variety of datasets, ranging from synthetic 2D to large-scale real-world datasets of images, 3D shapes, and animations."
  },
  {
    "title": "Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets",
    "url": "http://arxiv.org/abs/2504.02792v1",
    "arxiv_id": "2504.02792v1",
    "authors": [
      "Chuning Zhu",
      "Raymond Yu",
      "Siyuan Feng",
      "Benjamin Burchfiel",
      "Paarth Shah",
      "Abhishek Gupta"
    ],
    "published": "2025-04-03T17:38:59+00:00",
    "summary": "Imitation learning has emerged as a promising approach towards building generalist robots. However, scaling imitation learning for large robot foundation models remains challenging due to its reliance on high-quality expert demonstrations. Meanwhile, large amounts of video data depicting a wide range of environments and diverse behaviors are readily available. This data provides a rich source of information about real-world dynamics and agent-environment interactions. Leveraging this data directly for imitation learning, however, has proven difficult due to the lack of action annotation required for most contemporary methods. In this work, we present Unified World Models (UWM), a framework that allows for leveraging both video and action data for policy learning. Specifically, a UWM integrates an action diffusion process and a video diffusion process within a unified transformer architecture, where independent diffusion timesteps govern each modality. We show that by simply controlling each diffusion timestep, UWM can flexibly represent a policy, a forward dynamics, an inverse dynamics, and a video generator. Through simulated and real-world experiments, we show that: (1) UWM enables effective pretraining on large-scale multitask robot datasets with both dynamics and action predictions, resulting in more generalizable and robust policies than imitation learning, (2) UWM naturally facilitates learning from action-free video data through independent control of modality-specific diffusion timesteps, further improving the performance of finetuned policies. Our results suggest that UWM offers a promising step toward harnessing large, heterogeneous datasets for scalable robot learning, and provides a simple unification between the often disparate paradigms of imitation learning and world modeling. Videos and code are available at https://weirdlabuw.github.io/uwm/."
  },
  {
    "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation",
    "url": "http://arxiv.org/abs/2504.02782v1",
    "arxiv_id": "2504.02782v1",
    "authors": [
      "Zhiyuan Yan",
      "Junyan Ye",
      "Weijia Li",
      "Zilong Huang",
      "Shenghai Yuan",
      "Xiangyang He",
      "Kaiqing Lin",
      "Jun He",
      "Conghui He",
      "Li Yuan"
    ],
    "published": "2025-04-03T17:23:16+00:00",
    "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval."
  },
  {
    "title": "Slot-Level Robotic Placement via Visual Imitation from Single Human Video",
    "url": "http://arxiv.org/abs/2504.01959v1",
    "arxiv_id": "2504.01959v1",
    "authors": [
      "Dandan Shan",
      "Kaichun Mo",
      "Wei Yang",
      "Yu-Wei Chao",
      "David Fouhey",
      "Dieter Fox",
      "Arsalan Mousavian"
    ],
    "published": "2025-04-02T17:59:45+00:00",
    "summary": "The majority of modern robot learning methods focus on learning a set of pre-defined tasks with limited or no generalization to new tasks. Extending the robot skillset to novel tasks involves gathering an extensive amount of training data for additional tasks. In this paper, we address the problem of teaching new tasks to robots using human demonstration videos for repetitive tasks (e.g., packing). This task requires understanding the human video to identify which object is being manipulated (the pick object) and where it is being placed (the placement slot). In addition, it needs to re-identify the pick object and the placement slots during inference along with the relative poses to enable robot execution of the task. To tackle this, we propose SLeRP, a modular system that leverages several advanced visual foundation models and a novel slot-level placement detector Slot-Net, eliminating the need for expensive video demonstrations for training. We evaluate our system using a new benchmark of real-world videos. The evaluation results show that SLeRP outperforms several baselines and can be deployed on a real robot."
  },
  {
    "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
    "url": "http://arxiv.org/abs/2504.01943v1",
    "arxiv_id": "2504.01943v1",
    "authors": [
      "Wasi Uddin Ahmad",
      "Sean Narenthiran",
      "Somshubra Majumdar",
      "Aleksander Ficek",
      "Siddhartha Jain",
      "Jocelyn Huang",
      "Vahid Noroozi",
      "Boris Ginsburg"
    ],
    "published": "2025-04-02T17:50:31+00:00",
    "summary": "Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community."
  },
  {
    "title": "End-to-End Driving with Online Trajectory Evaluation via BEV World Model",
    "url": "http://arxiv.org/abs/2504.01941v1",
    "arxiv_id": "2504.01941v1",
    "authors": [
      "Yingyan Li",
      "Yuqi Wang",
      "Yang Liu",
      "Jiawei He",
      "Lue Fan",
      "Zhaoxiang Zhang"
    ],
    "published": "2025-04-02T17:47:23+00:00",
    "summary": "End-to-end autonomous driving has achieved remarkable progress by integrating perception, prediction, and planning into a fully differentiable framework. Yet, to fully realize its potential, an effective online trajectory evaluation is indispensable to ensure safety. By forecasting the future outcomes of a given trajectory, trajectory evaluation becomes much more effective. This goal can be achieved by employing a world model to capture environmental dynamics and predict future states. Therefore, we propose an end-to-end driving framework WoTE, which leverages a BEV World model to predict future BEV states for Trajectory Evaluation. The proposed BEV world model is latency-efficient compared to image-level world models and can be seamlessly supervised using off-the-shelf BEV-space traffic simulators. We validate our framework on both the NAVSIM benchmark and the closed-loop Bench2Drive benchmark based on the CARLA simulator, achieving state-of-the-art performance. Code is released at https://github.com/liyingyanUCAS/WoTE."
  },
  {
    "title": "Strengthening Multi-Robot Systems for SAR: Co-Designing Robotics and Communication Towards 6G",
    "url": "http://arxiv.org/abs/2504.01940v1",
    "arxiv_id": "2504.01940v1",
    "authors": [
      "Juan Bravo-Arrabal",
      "Ricardo V\u00e1zquez-Mart\u00edn",
      "J. J. Fern\u00e1ndez-Lozano",
      "Alfonso Garc\u00eda-Cerezo"
    ],
    "published": "2025-04-02T17:47:11+00:00",
    "summary": "This paper presents field-tested use cases from Search and Rescue (SAR) missions, highlighting the co-design of mobile robots and communication systems to support Edge-Cloud architectures based on 5G Standalone (SA). The main goal is to contribute to the effective cooperation of multiple robots and first responders. Our field experience includes the development of Hybrid Wireless Sensor Networks (H-WSNs) for risk and victim detection, smartphones integrated into the Robot Operating System (ROS) as Edge devices for mission requests and path planning, real-time Simultaneous Localization and Mapping (SLAM) via Multi-Access Edge Computing (MEC), and implementation of Uncrewed Ground Vehicles (UGVs) for victim evacuation in different navigation modes. These experiments, conducted in collaboration with actual first responders, underscore the need for intelligent network resource management, balancing low-latency and high-bandwidth demands. Network slicing is key to ensuring critical emergency services are performed despite challenging communication conditions. The paper identifies architectural needs, lessons learned, and challenges to be addressed by 6G technologies to enhance emergency response capabilities."
  },
  {
    "title": "Overcoming Deceptiveness in Fitness Optimization with Unsupervised Quality-Diversity",
    "url": "http://arxiv.org/abs/2504.01915v1",
    "arxiv_id": "2504.01915v1",
    "authors": [
      "Lisa Coiffard",
      "Paul Templier",
      "Antoine Cully"
    ],
    "published": "2025-04-02T17:18:21+00:00",
    "summary": "Policy optimization seeks the best solution to a control problem according to an objective or fitness function, serving as a fundamental field of engineering and research with applications in robotics. Traditional optimization methods like reinforcement learning and evolutionary algorithms struggle with deceptive fitness landscapes, where following immediate improvements leads to suboptimal solutions. Quality-diversity (QD) algorithms offer a promising approach by maintaining diverse intermediate solutions as stepping stones for escaping local optima. However, QD algorithms require domain expertise to define hand-crafted features, limiting their applicability where characterizing solution diversity remains unclear. In this paper, we show that unsupervised QD algorithms - specifically the AURORA framework, which learns features from sensory data - efficiently solve deceptive optimization problems without domain expertise. By enhancing AURORA with contrastive learning and periodic extinction events, we propose AURORA-XCon, which outperforms all traditional optimization baselines and matches, in some cases even improving by up to 34%, the best QD baseline with domain-specific hand-crafted features. This work establishes a novel application of unsupervised QD algorithms, shifting their focus from discovering novel solutions toward traditional optimization and expanding their potential to domains where defining feature spaces poses challenges."
  },
  {
    "title": "SU-YOLO: Spiking Neural Network for Efficient Underwater Object Detection",
    "url": "http://arxiv.org/abs/2503.24389v1",
    "arxiv_id": "2503.24389v1",
    "authors": [
      "Chenyang Li",
      "Wenxuan Liu",
      "Guoqiang Gong",
      "Xiaobo Ding",
      "Xian Zhong"
    ],
    "published": "2025-03-31T17:59:52+00:00",
    "summary": "Underwater object detection is critical for oceanic research and industrial safety inspections. However, the complex optical environment and the limited resources of underwater equipment pose significant challenges to achieving high accuracy and low power consumption. To address these issues, we propose Spiking Underwater YOLO (SU-YOLO), a Spiking Neural Network (SNN) model. Leveraging the lightweight and energy-efficient properties of SNNs, SU-YOLO incorporates a novel spike-based underwater image denoising method based solely on integer addition, which enhances the quality of feature maps with minimal computational overhead. In addition, we introduce Separated Batch Normalization (SeBN), a technique that normalizes feature maps independently across multiple time steps and is optimized for integration with residual structures to capture the temporal dynamics of SNNs more effectively. The redesigned spiking residual blocks integrate the Cross Stage Partial Network (CSPNet) with the YOLO architecture to mitigate spike degradation and enhance the model's feature extraction capabilities. Experimental results on URPC2019 underwater dataset demonstrate that SU-YOLO achieves mAP of 78.8% with 6.97M parameters and an energy consumption of 2.98 mJ, surpassing mainstream SNN models in both detection accuracy and computational efficiency. These results underscore the potential of SNNs for engineering applications. The code is available in https://github.com/lwxfight/snn-underwater."
  },
  {
    "title": "UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.24381v1",
    "arxiv_id": "2503.24381v1",
    "authors": [
      "Yuping Wang",
      "Xiangyu Huang",
      "Xiaokang Sun",
      "Mingxuan Yan",
      "Shuo Xing",
      "Zhengzhong Tu",
      "Jiachen Li"
    ],
    "published": "2025-03-31T17:59:24+00:00",
    "summary": "We introduce UniOcc, a comprehensive, unified benchmark for occupancy forecasting (i.e., predicting future occupancies based on historical information) and current-frame occupancy prediction from camera images. UniOcc unifies data from multiple real-world datasets (i.e., nuScenes, Waymo) and high-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3D occupancy labels with per-voxel flow annotations and support for cooperative autonomous driving. In terms of evaluation, unlike existing studies that rely on suboptimal pseudo labels for evaluation, UniOcc incorporates novel metrics that do not depend on ground-truth occupancy, enabling robust assessment of additional aspects of occupancy quality. Through extensive experiments on state-of-the-art models, we demonstrate that large-scale, diverse training data and explicit flow information significantly enhance occupancy prediction and forecasting performance."
  },
  {
    "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1",
    "url": "http://arxiv.org/abs/2503.24376v1",
    "arxiv_id": "2503.24376v1",
    "authors": [
      "Yi Chen",
      "Yuying Ge",
      "Rui Wang",
      "Yixiao Ge",
      "Lu Qiu",
      "Ying Shan",
      "Xihui Liu"
    ],
    "published": "2025-03-31T17:55:23+00:00",
    "summary": "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals."
  },
  {
    "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
    "url": "http://arxiv.org/abs/2503.24370v1",
    "arxiv_id": "2503.24370v1",
    "authors": [
      "Tong Wu",
      "Chong Xiang",
      "Jiachen T. Wang",
      "Prateek Mittal"
    ],
    "published": "2025-03-31T17:50:13+00:00",
    "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs."
  },
  {
    "title": "Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation",
    "url": "http://arxiv.org/abs/2503.24361v1",
    "arxiv_id": "2503.24361v1",
    "authors": [
      "Abhiram Maddukuri",
      "Zhenyu Jiang",
      "Lawrence Yunliang Chen",
      "Soroush Nasiriany",
      "Yuqi Xie",
      "Yu Fang",
      "Wenqi Huang",
      "Zu Wang",
      "Zhenjia Xu",
      "Nikita Chernyadev",
      "Scott Reed",
      "Ken Goldberg",
      "Ajay Mandlekar",
      "Linxi Fan",
      "Yuke Zhu"
    ],
    "published": "2025-03-31T17:39:38+00:00",
    "summary": "Large real-world robot datasets hold great potential to train generalist robot models, but scaling real-world human data collection is time-consuming and resource-intensive. Simulation has great potential in supplementing large-scale data, especially with recent advances in generative AI and automated data generation tools that enable scalable creation of robot behavior datasets. However, training a policy solely in simulation and transferring it to the real world often demands substantial human effort to bridge the reality gap. A compelling alternative is to co-train the policy on a mixture of simulation and real-world datasets. Preliminary studies have recently shown this strategy to substantially improve the performance of a policy over one trained on a limited amount of real-world data. Nonetheless, the community lacks a systematic understanding of sim-and-real co-training and what it takes to reap the benefits of simulation data for real-robot learning. This work presents a simple yet effective recipe for utilizing simulation data to solve vision-based robotic manipulation tasks. We derive this recipe from comprehensive experiments that validate the co-training strategy on various simulation and real-world datasets. Using two domains--a robot arm and a humanoid--across diverse tasks, we demonstrate that simulation data can enhance real-world task performance by an average of 38%, even with notable differences between the simulation and real-world data. Videos and additional results can be found at https://co-training.github.io/"
  },
  {
    "title": "Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation",
    "url": "http://arxiv.org/abs/2503.24361v2",
    "arxiv_id": "2503.24361v2",
    "authors": [
      "Abhiram Maddukuri",
      "Zhenyu Jiang",
      "Lawrence Yunliang Chen",
      "Soroush Nasiriany",
      "Yuqi Xie",
      "Yu Fang",
      "Wenqi Huang",
      "Zu Wang",
      "Zhenjia Xu",
      "Nikita Chernyadev",
      "Scott Reed",
      "Ken Goldberg",
      "Ajay Mandlekar",
      "Linxi Fan",
      "Yuke Zhu"
    ],
    "published": "2025-03-31T17:39:38+00:00",
    "summary": "Large real-world robot datasets hold great potential to train generalist robot models, but scaling real-world human data collection is time-consuming and resource-intensive. Simulation has great potential in supplementing large-scale data, especially with recent advances in generative AI and automated data generation tools that enable scalable creation of robot behavior datasets. However, training a policy solely in simulation and transferring it to the real world often demands substantial human effort to bridge the reality gap. A compelling alternative is to co-train the policy on a mixture of simulation and real-world datasets. Preliminary studies have recently shown this strategy to substantially improve the performance of a policy over one trained on a limited amount of real-world data. Nonetheless, the community lacks a systematic understanding of sim-and-real co-training and what it takes to reap the benefits of simulation data for real-robot learning. This work presents a simple yet effective recipe for utilizing simulation data to solve vision-based robotic manipulation tasks. We derive this recipe from comprehensive experiments that validate the co-training strategy on various simulation and real-world datasets. Using two domains--a robot arm and a humanoid--across diverse tasks, we demonstrate that simulation data can enhance real-world task performance by an average of 38%, even with notable differences between the simulation and real-world data. Videos and additional results can be found at https://co-training.github.io/"
  },
  {
    "title": "Q-Insight: Understanding Image Quality via Visual Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.22679v1",
    "arxiv_id": "2503.22679v1",
    "authors": [
      "Weiqi Li",
      "Xuanyu Zhang",
      "Shijie Zhao",
      "Yabin Zhang",
      "Junlin Li",
      "Li Zhang",
      "Jian Zhang"
    ],
    "published": "2025-03-28T17:59:54+00:00",
    "summary": "Image quality assessment (IQA) focuses on the perceptual visual quality of images, playing a crucial role in downstream tasks such as image reconstruction, compression, and generation. The rapid advancement of multi-modal large language models (MLLMs) has significantly broadened the scope of IQA, moving toward comprehensive image quality understanding that incorporates content analysis, degradation perception, and comparison reasoning beyond mere numerical scoring. Previous MLLM-based methods typically either generate numerical scores lacking interpretability or heavily rely on supervised fine-tuning (SFT) using large-scale annotated datasets to provide descriptive assessments, limiting their flexibility and applicability. In this paper, we propose Q-Insight, a reinforcement learning-based model built upon group relative policy optimization (GRPO), which demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels. By jointly optimizing score regression and degradation perception tasks with carefully designed reward functions, our approach effectively exploits their mutual benefits for enhanced performance. Extensive experiments demonstrate that Q-Insight substantially outperforms existing state-of-the-art methods in both score regression and degradation perception tasks, while exhibiting impressive zero-shot generalization to comparison reasoning tasks. Code will be available at https://github.com/lwq20020127/Q-Insight."
  },
  {
    "title": "Verifying Nonlinear Neural Feedback Systems using Polyhedral Enclosures",
    "url": "http://arxiv.org/abs/2503.22660v1",
    "arxiv_id": "2503.22660v1",
    "authors": [
      "Samuel I. Akinwande",
      "Chelsea Sidrane",
      "Mykel J. Kochenderfer",
      "Clark Barrett"
    ],
    "published": "2025-03-28T17:45:23+00:00",
    "summary": "As dynamical systems equipped with neural network controllers (neural feedback systems) become increasingly prevalent, it is critical to develop methods to ensure their safe operation. Verifying safety requires extending control theoretic analysis methods to these systems. Although existing techniques can efficiently handle linear neural feedback systems, relatively few scalable methods address the nonlinear case. We propose a novel algorithm for forward reachability analysis of nonlinear neural feedback systems. The approach leverages the structure of the nonlinear transition functions of the systems to compute tight polyhedral enclosures (i.e., abstractions). These enclosures, combined with the neural controller, are then encoded as a mixed-integer linear program (MILP). Optimizing this MILP yields a sound over-approximation of the forward-reachable set. We evaluate our algorithm on representative benchmarks and demonstrate an order of magnitude improvement over the current state of the art."
  },
  {
    "title": "Finding Unknown Unknowns using Cyber-Physical System Simulators (Extended Report)",
    "url": "http://arxiv.org/abs/2503.22646v1",
    "arxiv_id": "2503.22646v1",
    "authors": [
      "Semaan Douglas Wehbe",
      "Stanley Bak"
    ],
    "published": "2025-03-28T17:32:26+00:00",
    "summary": "Simulation-based approaches are among the most practical means to search for safety violations, bugs, and other unexpected events in cyber-physical systems (CPS). Where existing approaches search for simulations violating a formal specification or maximizing a notion of coverage, in this work we propose a new goal for testing: to discover unknown rare behaviors by examining discrete mode sequences. We assume a CPS simulator outputs mode information, and strive to explore the sequences of modes produced by varying the initial state or time-varying uncertainties. We hypothesize that rare mode sequences are often the most interesting to a designer, and we develop two accelerated sampling algorithms that speed up the process of finding such sequences. We evaluate our approach on several benchmarks, ranging from synthetic examples to Simulink diagrams of a CPS, demonstrating in some cases a speedup of over 100x compared with a random sampling strategy."
  },
  {
    "title": "Empirical Analysis of Sim-and-Real Cotraining Of Diffusion Policies For Planar Pushing from Pixels",
    "url": "http://arxiv.org/abs/2503.22634v1",
    "arxiv_id": "2503.22634v1",
    "authors": [
      "Adam Wei",
      "Abhinav Agarwal",
      "Boyuan Chen",
      "Rohan Bosworth",
      "Nicholas Pfaff",
      "Russ Tedrake"
    ],
    "published": "2025-03-28T17:25:57+00:00",
    "summary": "In imitation learning for robotics, cotraining with demonstration data generated both in simulation and on real hardware has emerged as a powerful recipe to overcome the sim2real gap. This work seeks to elucidate basic principles of this sim-and-real cotraining to help inform simulation design, sim-and-real dataset creation, and policy training. Focusing narrowly on the canonical task of planar pushing from camera inputs enabled us to be thorough in our study. These experiments confirm that cotraining with simulated data \\emph{can} dramatically improve performance in real, especially when real data is limited. Performance gains scale with simulated data, but eventually plateau; real-world data increases this performance ceiling. The results also suggest that reducing the domain gap in physics may be more important than visual fidelity for non-prehensile manipulation tasks. Perhaps surprisingly, having some visual domain gap actually helps the cotrained policy -- binary probes reveal that high-performing policies learn to distinguish simulated domains from real. We conclude by investigating this nuance and mechanisms that facilitate positive transfer between sim-and-real. In total, our experiments span over 40 real-world policies (evaluated on 800+ trials) and 200 simulated policies (evaluated on 40,000+ trials)."
  },
  {
    "title": "Reinforcement Learning for Machine Learning Model Deployment: Evaluating Multi-Armed Bandits in ML Ops Environments",
    "url": "http://arxiv.org/abs/2503.22595v1",
    "arxiv_id": "2503.22595v1",
    "authors": [
      "S. Aaron McClendon",
      "Vishaal Venkatesh",
      "Juan Morinelli"
    ],
    "published": "2025-03-28T16:42:21+00:00",
    "summary": "In modern ML Ops environments, model deployment is a critical process that traditionally relies on static heuristics such as validation error comparisons and A/B testing. However, these methods require human intervention to adapt to real-world deployment challenges, such as model drift or unexpected performance degradation. We investigate whether reinforcement learning, specifically multi-armed bandit (MAB) algorithms, can dynamically manage model deployment decisions more effectively. Our approach enables more adaptive production environments by continuously evaluating deployed models and rolling back underperforming ones in real-time. We test six model selection strategies across two real-world datasets and find that RL based approaches match or exceed traditional methods in performance. Our findings suggest that reinforcement learning (RL)-based model management can improve automation, reduce reliance on manual interventions, and mitigate risks associated with post-deployment model failures."
  },
  {
    "title": "HS-SLAM: Hybrid Representation with Structural Supervision for Improved Dense SLAM",
    "url": "http://arxiv.org/abs/2503.21778v1",
    "arxiv_id": "2503.21778v1",
    "authors": [
      "Ziren Gong",
      "Fabio Tosi",
      "Youmin Zhang",
      "Stefano Mattoccia",
      "Matteo Poggi"
    ],
    "published": "2025-03-27T17:59:54+00:00",
    "summary": "NeRF-based SLAM has recently achieved promising results in tracking and reconstruction. However, existing methods face challenges in providing sufficient scene representation, capturing structural information, and maintaining global consistency in scenes emerging significant movement or being forgotten. To this end, we present HS-SLAM to tackle these problems. To enhance scene representation capacity, we propose a hybrid encoding network that combines the complementary strengths of hash-grid, tri-planes, and one-blob, improving the completeness and smoothness of reconstruction. Additionally, we introduce structural supervision by sampling patches of non-local pixels rather than individual rays to better capture the scene structure. To ensure global consistency, we implement an active global bundle adjustment (BA) to eliminate camera drifts and mitigate accumulative errors. Experimental results demonstrate that HS-SLAM outperforms the baselines in tracking and reconstruction accuracy while maintaining the efficiency required for robotics."
  },
  {
    "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
    "url": "http://arxiv.org/abs/2503.21776v1",
    "arxiv_id": "2503.21776v1",
    "authors": [
      "Kaituo Feng",
      "Kaixiong Gong",
      "Bohao Li",
      "Zonghao Guo",
      "Yibing Wang",
      "Tianshuo Peng",
      "Benyou Wang",
      "Xiangyu Yue"
    ],
    "published": "2025-03-27T17:59:51+00:00",
    "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released."
  },
  {
    "title": "Optical control of orbital magnetism in magic angle twisted bilayer graphene",
    "url": "http://arxiv.org/abs/2503.21750v1",
    "arxiv_id": "2503.21750v1",
    "authors": [
      "Eylon Persky",
      "Minhao He",
      "Jiaqi Cai",
      "Takashi Taniguchi",
      "Kenji Watanabe",
      "Xiaodong Xu",
      "Aharon Kapitulnik"
    ],
    "published": "2025-03-27T17:56:23+00:00",
    "summary": "Flat bands in graphene-based moir\\'e structures host a wide range of emerging strongly correlated and topological phenomena. Optically probing and controlling them can reveal important information such as symmetry and dynamics, but have so far been challenging due to the small energy gap compared to optical wavelengths. Here, we report near infrared optical control of orbital magnetism and associated anomalous Hall effects (AHE) in a magic angle twisted bilayer graphene (MATBG) on monolayer WSe$_2$ device. We show that the properties of the AHE, such as hysteresis and amplitude, can be controlled by light near integer moir\\'e fillings, where spontaneous ferromagnetism exists. By modulating the light helicity, we observe periodic modulation of the transverse resistance in a wide range of fillings, indicating light induced orbital magnetization through a large inverse Faraday effect. At the transition between metallic and AHE regimes, we also reveal large and random switching of the Hall resistivity, which are attributed to optical control of percolating cluster of magnetic domains. Our results open the door to optical manipulation of correlation and topology in MATBG and related structures."
  },
  {
    "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics",
    "url": "http://arxiv.org/abs/2503.21735v1",
    "arxiv_id": "2503.21735v1",
    "authors": [
      "Arsham Gholamzadeh Khoee",
      "Shuai Wang",
      "Yinan Yu",
      "Robert Feldt",
      "Dhasarathy Parthasarathy"
    ],
    "published": "2025-03-27T17:48:32+00:00",
    "summary": "Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems. Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process. However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs. Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios. This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code. It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness. Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted. Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability. As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles. Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation. Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems."
  },
  {
    "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation",
    "url": "http://arxiv.org/abs/2503.21729v1",
    "arxiv_id": "2503.21729v1",
    "authors": [
      "Zhicheng Lee",
      "Shulin Cao",
      "Jinxin Liu",
      "Jiajie Zhang",
      "Weichuan Liu",
      "Xiaoyin Che",
      "Lei Hou",
      "Juanzi Li"
    ],
    "published": "2025-03-27T17:44:18+00:00",
    "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG)."
  }
]